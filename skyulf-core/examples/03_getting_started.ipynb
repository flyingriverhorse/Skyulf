{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e2054e",
   "metadata": {},
   "source": [
    "# Skyulf Core — Getting Started (Notebook 3)\n",
    "\n",
    "This notebook is a minimal, runnable smoke test for the Skyulf Core SDK in a notebook environment.\n",
    "\n",
    "It verifies the environment, checks imports, runs a tiny end-to-end preprocessing + training example, and (optionally) runs a quick pytest smoke check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8571054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "# Fail-fast import check\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skyulf.data.dataset import SplitDataset\n",
    "from skyulf.preprocessing.pipeline import FeatureEngineer\n",
    "from skyulf.modeling.base import StatefulEstimator\n",
    "from skyulf.modeling.classification import (\n",
    "    RandomForestClassifierApplier,\n",
    "    RandomForestClassifierCalculator,\n",
    ")\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87dd1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\Murat\\Desktop\\skyulf-mlflow\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook is in skyulf-core/examples -> project root is two levels up\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = (NOTEBOOK_DIR / \"..\" / \"..\").resolve()\n",
    "\n",
    "random.seed(42)\n",
    "try:\n",
    "    import numpy as np\n",
    "    np.random.seed(42)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s:%(name)s:%(message)s\")\n",
    "print(\"Project root:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d5fa0",
   "metadata": {},
   "source": [
    "End-to-end: load Iris → split → scale features → train RandomForest → predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e188eecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:skyulf.preprocessing.pipeline:Running step 0: scale_features (StandardScaler)\n",
      "INFO:skyulf.modeling.sklearn_wrapper:Initializing RandomForestClassifier with params: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'n_jobs': -1, 'random_state': 42}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "Name: target, Length: 150, dtype: int32\n",
      "Train shape: (120, 5)\n",
      "Test shape: (30, 5)\n",
      "Preprocessing metrics keys: ['mean', 'scale', 'var', 'columns']\n",
      "X_train_scaled: (120, 4)\n",
      "X_test_scaled: (30, 4)\n",
      "Predictions OK\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df[\"target\"] = iris.target\n",
    "\n",
    "df.columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"target\"]\n",
    "\n",
    "feature_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "target_col = \"target\"\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[target_col]\n",
    " )\n",
    "\n",
    "dataset = SplitDataset(\n",
    "    train=train_df.reset_index(drop=True),\n",
    "    test=test_df.reset_index(drop=True),\n",
    "    validation=None,\n",
    " )\n",
    "\n",
    "print(\"Train shape:\", dataset.train.shape)\n",
    "print(\"Test shape:\", dataset.test.shape)\n",
    "\n",
    "# Preprocess features\n",
    "steps = [\n",
    "    {\n",
    "        \"name\": \"scale_features\",\n",
    "        \"transformer\": \"StandardScaler\",\n",
    "        \"params\": {\"columns\": feature_cols},\n",
    "    }\n",
    "]\n",
    "\n",
    "engineer = FeatureEngineer(steps)\n",
    "X_train_scaled, metrics = engineer.fit_transform(dataset.train[feature_cols])\n",
    "X_test_scaled = engineer.transform(dataset.test[feature_cols])\n",
    "\n",
    "print(\"Preprocessing metrics keys:\", list(metrics.keys()))\n",
    "print(\"X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled:\", X_test_scaled.shape)\n",
    "\n",
    "# Train + predict\n",
    "estimator = StatefulEstimator(\n",
    "    calculator=RandomForestClassifierCalculator(),\n",
    "    applier=RandomForestClassifierApplier(),\n",
    "    node_id=\"iris_rf\",\n",
    " )\n",
    "\n",
    "train_model_df = X_train_scaled.copy()\n",
    "train_model_df[target_col] = dataset.train[target_col].reset_index(drop=True)\n",
    "\n",
    "test_model_df = X_test_scaled.copy()\n",
    "test_model_df[target_col] = dataset.test[target_col].reset_index(drop=True)\n",
    "\n",
    "predictions = estimator.fit_predict(\n",
    "    dataset=SplitDataset(train=train_model_df, test=test_model_df, validation=None),\n",
    "    target_column=target_col,\n",
    "    config={\"n_estimators\": 200, \"random_state\": 42},\n",
    " )\n",
    "\n",
    "assert \"train\" in predictions and \"test\" in predictions\n",
    "assert len(predictions[\"train\"]) == len(dataset.train)\n",
    "assert len(predictions[\"test\"]) == len(dataset.test)\n",
    "\n",
    "print(\"Predictions OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d56b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.90      0.95        10\n",
      "   virginica       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "test_acc = accuracy_score(dataset.test[target_col], predictions[\"test\"])\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        dataset.test[target_col],\n",
    "        predictions[\"test\"],\n",
    "        target_names=iris.target_names,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd4573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw preds: [0, 1]\n",
      "Species: ['setosa', 'versicolor']\n"
     ]
    }
   ],
   "source": [
    "# Predict on new data\n",
    "new_samples = pd.DataFrame(\n",
    "    {\n",
    "        \"sepal_length\": [5.1, 6.3],\n",
    "        \"sepal_width\": [3.5, 2.5],\n",
    "        \"petal_length\": [1.4, 4.9],\n",
    "        \"petal_width\": [0.2, 1.5],\n",
    "    }\n",
    ")\n",
    "\n",
    "new_scaled = engineer.transform(new_samples[feature_cols])\n",
    "\n",
    "# StatefulEstimator keeps the trained model in-memory as estimator.model\n",
    "new_preds = estimator.applier.predict(new_scaled, estimator.model)\n",
    "print(\"Raw preds:\", new_preds.tolist())\n",
    "print(\"Species:\", [iris.target_names[i] for i in new_preds.tolist()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapi_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
