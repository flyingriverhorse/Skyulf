{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Skyulf","text":"<p>Skyulf is a self-hosted MLOps platform with:</p> <ul> <li>a FastAPI backend</li> <li>a React frontend</li> <li>a standalone Python ML library: skyulf-core</li> </ul>"},{"location":"index.html#if-you-are-here-for-skyulf-core","title":"If you are here for skyulf-core","text":"<p>Start with:</p> <ul> <li>User Guide \u2192 Overview</li> <li>User Guide \u2192 Pipeline Quickstart</li> <li>Reference \u2192 Preprocessing Nodes / Modeling Nodes</li> </ul>"},{"location":"index.html#backend-quick-start","title":"Backend quick start","text":"<pre><code>pip install -r requirements-fastapi.txt\npython run_skyulf.py\n</code></pre> <p>Open:</p> <ul> <li>http://127.0.0.1:8000</li> </ul>"},{"location":"architecture.html","title":"Architecture","text":"<p>Skyulf is split into three pieces with strict boundaries:</p> <ol> <li>skyulf-core (this docs set focuses on this)</li> <li>A standalone Python ML library.</li> <li>Implements a strict Calculator \u2192 Applier pattern for every node.</li> <li>Depends on Pandas/Numpy/Scikit-Learn (and a few optional ML utilities).</li> <li>backend</li> <li>FastAPI + Celery orchestration layer.</li> <li>Handles ingestion, jobs, persistence, and exposes REST APIs.</li> <li>frontend</li> <li>React + TypeScript UI (ML canvas).</li> <li>Builds pipeline configs and talks to the backend.</li> </ol>"},{"location":"architecture.html#the-calculator-applier-pattern","title":"The Calculator \u2192 Applier Pattern","text":"<p>Skyulf-core separates learning from transformation:</p> <ul> <li>Calculator: <code>fit(data, config) -&gt; params</code></li> <li>Learns statistics / encoders / models.</li> <li>Returns a serializable <code>params</code> dictionary.</li> <li>Applier: <code>apply(data, params) -&gt; transformed_data</code></li> <li>Stateless transformer.</li> <li>Applies learned parameters.</li> </ul> <p>This makes pipelines easier to persist and safer to run in production:</p> <ul> <li>Learning happens on train.</li> <li>The learned state is explicit.</li> <li>Applying is pure and repeatable.</li> </ul>"},{"location":"architecture.html#pipeline-data-flow","title":"Pipeline Data Flow","text":"<p>At runtime, <code>SkyulfPipeline</code> orchestrates:</p> <ol> <li>Preprocessing: <code>FeatureEngineer</code></li> <li>Executes a list of steps (each step is a transformer).</li> <li>Some steps change the data structure (e.g., splitters) and are handled specially.</li> <li>Modeling: <code>StatefulEstimator</code></li> <li>Trains a model on the train split.</li> <li>Optionally evaluates on test/validation.</li> </ol> <p>High-level flow:</p> <pre><code>Raw DataFrame\n  \u2514\u2500 FeatureEngineer.fit_transform(...)  -&gt; DataFrame or SplitDataset\n        \u2514\u2500 (optionally) SplitDataset.train / test / validation\n              \u2514\u2500 StatefulEstimator.fit_predict(...) -&gt; predictions\n</code></pre>"},{"location":"architecture.html#avoiding-data-leakage","title":"Avoiding Data Leakage","text":"<p>If you split first (or provide a <code>SplitDataset</code>), calculators should learn only on the train split. This prevents leakage of statistics from test/validation.</p> <p>See the User Guide section \u201cSplitDataset &amp; Leakage\u201d for recommended patterns.</p>"},{"location":"contributing/writing_docs.html","title":"Writing Documentation","text":"<p>We use MkDocs with the Material for MkDocs theme to build our documentation.</p>"},{"location":"contributing/writing_docs.html#setup","title":"Setup","text":"<ol> <li> <p>Install Dependencies:     <code>bash     pip install mkdocs mkdocs-material mkdocstrings[python]</code></p> </li> <li> <p>Serve Locally:     <code>bash     mkdocs serve</code>     This will start a local server at <code>http://127.0.0.1:8000</code> that auto-reloads when you change files.</p> </li> </ol>"},{"location":"contributing/writing_docs.html#directory-structure","title":"Directory Structure","text":"<ul> <li><code>docs/</code>: Contains all markdown files.</li> <li><code>mkdocs.yml</code>: The main configuration file.</li> </ul>"},{"location":"contributing/writing_docs.html#adding-a-new-page","title":"Adding a New Page","text":"<ol> <li>Create a new <code>.md</code> file in <code>docs/</code>.</li> <li>Add the file to the <code>nav</code> section in <code>mkdocs.yml</code>.</li> </ol>"},{"location":"contributing/writing_docs.html#writing-api-documentation","title":"Writing API Documentation","text":"<p>We use <code>mkdocstrings</code> to auto-generate API docs from Python docstrings.</p> <p>To document a module, class, or function, use the <code>:::</code> directive:</p> <pre><code>::: core.my_module.MyClass\n</code></pre>"},{"location":"contributing/writing_docs.html#docstring-style","title":"Docstring Style","text":"<p>We follow the Google Style Guide for docstrings.</p> <pre><code>def my_function(arg1: int, arg2: str) -&gt; bool:\n    \"\"\"\n    Does something amazing.\n\n    Args:\n        arg1: The first argument.\n        arg2: The second argument.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"guides/getting_started.html","title":"Getting Started (skyulf-core)","text":"<p>This page is the fastest path to running <code>skyulf-core</code> locally.</p>"},{"location":"guides/getting_started.html#install","title":"Install","text":"<p>From the repository root:</p> <pre><code>pip install -e ./skyulf-core\n</code></pre>"},{"location":"guides/getting_started.html#minimal-end-to-end-example","title":"Minimal end-to-end example","text":"<p><code>SkyulfPipeline</code> expects a configuration with:</p> <ul> <li><code>preprocessing</code>: a list of steps</li> <li><code>modeling</code>: a single model config</li> </ul> <pre><code>import pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\ndf = pd.DataFrame(\n    {\n        \"age\": [10, 20, None, 40],\n        \"city\": [\"A\", \"B\", \"A\", \"C\"],\n        \"target\": [0, 1, 0, 1],\n    }\n)\n\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"impute_age\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"columns\": [\"age\"], \"strategy\": \"mean\"},\n        },\n        {\n            \"name\": \"encode_city\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": [\"city\"], \"drop_original\": True},\n        },\n    ],\n    \"modeling\": {\n        \"type\": \"logistic_regression\",\n        \"params\": {\"max_iter\": 1000},\n    },\n}\n\npipeline = SkyulfPipeline(config)\nmetrics = pipeline.fit(df, target_column=\"target\")\npreds = pipeline.predict(df.drop(columns=[\"target\"]))\n\nprint(metrics)\nprint(preds.head())\n</code></pre>"},{"location":"guides/getting_started.html#next-steps","title":"Next steps","text":"<ul> <li>Read the User Guide section \u201cPipeline Quickstart\u201d for train/test splits.</li> <li>Use the Reference section for supported preprocessing and modeling nodes.</li> </ul>"},{"location":"guides/recipes.html","title":"Recipes","text":"<p>This page contains practical patterns for common workflows.</p>"},{"location":"guides/recipes.html#recipe-split-preprocess-train","title":"Recipe: split, preprocess, train","text":"<p>If you want evaluation metrics, use a split step (or pass a <code>SplitDataset</code>).</p> <pre><code>from __future__ import annotations\n\nimport pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\n# In real usage you'd likely load a file:\n# df = pd.read_csv(\"your_data.csv\")\n\ndf = pd.DataFrame(\n    {\n        \"free_text\": [\n            \"  Hello   World  \",\n            \"Skyulf is GREAT! \",\n            \"  hello\\tworld \",\n            \"  ML pipelines   \",\n            \"Encode + scale\",\n            \" text cleaning  \",\n        ],\n        \"country\": [\"TR\", \"TR\", \"DE\", \"DE\", \"TR\", \"DE\"],\n        \"age\": [10, 20, 30, 40, 50, 60],\n        \"target\": [0, 1, 0, 1, 1, 0],\n    }\n)\n\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"split\",\n            \"transformer\": \"TrainTestSplitter\",\n            \"params\": {\n                \"test_size\": 0.34,\n                \"validation_size\": 0.0,\n                \"random_state\": 42,\n                \"shuffle\": True,\n                \"stratify\": True,\n                \"target_column\": \"target\",\n            },\n        },\n        {\n            \"name\": \"text_clean\",\n            \"transformer\": \"TextCleaning\",\n            \"params\": {\n                \"columns\": [\"free_text\"],\n                \"operations\": [\n                    {\"op\": \"trim\", \"mode\": \"both\"},\n                    {\"op\": \"case\", \"mode\": \"lower\"},\n                    {\"op\": \"regex\", \"mode\": \"collapse_whitespace\"},\n                ],\n            },\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\n                \"columns\": [\"country\", \"free_text\"],\n                \"drop_original\": True,\n                \"handle_unknown\": \"ignore\",\n            },\n        },\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n        },\n        {\n            \"name\": \"scale\",\n            \"transformer\": \"StandardScaler\",\n            \"params\": {\"auto_detect\": True},\n        },\n    ],\n    \"modeling\": {\"type\": \"random_forest_classifier\", \"params\": {\"n_estimators\": 200}},\n}\n\npipeline = SkyulfPipeline(config)\nreport = pipeline.fit(df, target_column=\"target\")\nprint(report.get(\"modeling\"))\n</code></pre>"},{"location":"guides/recipes.html#recipe-safe-inference","title":"Recipe: safe inference","text":"<p>At inference time you typically:</p> <ol> <li>load a persisted pipeline</li> <li>call <code>predict(df)</code> on a dataframe without the target column</li> </ol> <pre><code>from __future__ import annotations\n\nimport tempfile\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\ndf = pd.DataFrame(\n    {\n        \"age\": [10, 20, 30, 40, 50, 60],\n        \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\"],\n        \"target\": [0, 1, 0, 1, 1, 0],\n    }\n)\n\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"split\",\n            \"transformer\": \"TrainTestSplitter\",\n            \"params\": {\n                \"test_size\": 0.2,\n                \"validation_size\": 0.0,\n                \"random_state\": 42,\n                \"shuffle\": True,\n                \"stratify\": True,\n                \"target_column\": \"target\",\n            },\n        },\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": [\"city\"], \"drop_original\": True},\n        },\n    ],\n    \"modeling\": {\n        \"type\": \"random_forest_classifier\",\n        \"params\": {\"n_estimators\": 50, \"random_state\": 42},\n    },\n}\n\nwith tempfile.TemporaryDirectory() as tmp:\n    model_path = Path(tmp) / \"model.pkl\"\n\n    pipeline = SkyulfPipeline(config)\n    _ = pipeline.fit(df, target_column=\"target\")\n    pipeline.save(model_path)\n\n    loaded = SkyulfPipeline.load(model_path)\n\n    new_df = pd.DataFrame({\"age\": [25, 55], \"city\": [\"A\", \"B\"]})\n    preds = loaded.predict(new_df)\n\nprint(preds)\n</code></pre>"},{"location":"guides/recipes.html#recipe-use-a-single-component-debug","title":"Recipe: use a single component (debug)","text":"<p>For debugging, you can run one node directly.</p> <pre><code>import pandas as pd\n\nfrom skyulf.preprocessing.imputation import SimpleImputerApplier, SimpleImputerCalculator\n\ndf = pd.DataFrame({\"A\": [1, 2, None, 4]})\nconfig = {\"columns\": [\"A\"], \"strategy\": \"mean\"}\n\nparams = SimpleImputerCalculator().fit(df, config)\nout = SimpleImputerApplier().apply(df, params)\n\nprint(params)\nprint(out)\n</code></pre>"},{"location":"reference/modeling_nodes.html","title":"Modeling Nodes","text":"<p>This page documents modeling configuration for <code>SkyulfPipeline</code>.</p>"},{"location":"reference/modeling_nodes.html#common-config-shape","title":"Common config shape","text":"<p><code>SkyulfPipeline</code> expects a modeling block like:</p> <pre><code>{\n  \"type\": \"logistic_regression\",\n  \"node_id\": \"model_node\",  # optional\n  \"params\": { ... }          # optional; estimator hyperparameters\n}\n</code></pre> <p>The sklearn wrapper supports both:</p> <ul> <li>Nested params (preferred): <code>{ \"params\": {\"C\": 1.0} }</code></li> <li>Flat params (legacy): <code>{ \"C\": 1.0, \"type\": \"...\" }</code></li> </ul> <p>Example (RandomForestClassifier):</p> <pre><code>{\n  \"type\": \"random_forest_classifier\",\n  \"params\": {\"n_estimators\": 50, \"random_state\": 42}\n}\n</code></pre>"},{"location":"reference/modeling_nodes.html#classification","title":"Classification","text":""},{"location":"reference/modeling_nodes.html#logistic_regression","title":"logistic_regression","text":"<p>Backed by <code>sklearn.linear_model.LogisticRegression</code>.</p> <p>Defaults:</p> <ul> <li><code>max_iter=1000</code></li> <li><code>solver=lbfgs</code></li> <li><code>random_state=42</code></li> </ul> <p>Learned params:</p> <ul> <li>fitted sklearn estimator (stored in-memory and pickled when saving the pipeline)</li> </ul>"},{"location":"reference/modeling_nodes.html#random_forest_classifier","title":"random_forest_classifier","text":"<p>Backed by <code>sklearn.ensemble.RandomForestClassifier</code>.</p> <p>Defaults include:</p> <ul> <li><code>n_estimators=50</code>, <code>max_depth=10</code></li> <li><code>min_samples_split=5</code>, <code>min_samples_leaf=2</code></li> <li><code>n_jobs=-1</code>, <code>random_state=42</code></li> </ul> <p>Learned params:</p> <ul> <li>fitted sklearn estimator</li> </ul>"},{"location":"reference/modeling_nodes.html#regression","title":"Regression","text":""},{"location":"reference/modeling_nodes.html#ridge_regression","title":"ridge_regression","text":"<p>Backed by <code>sklearn.linear_model.Ridge</code>.</p> <p>Defaults:</p> <ul> <li><code>alpha=1.0</code>, <code>solver=auto</code>, <code>random_state=42</code></li> </ul>"},{"location":"reference/modeling_nodes.html#random_forest_regressor","title":"random_forest_regressor","text":"<p>Backed by <code>sklearn.ensemble.RandomForestRegressor</code>.</p> <p>Defaults include:</p> <ul> <li><code>n_estimators=50</code>, <code>max_depth=10</code></li> <li><code>min_samples_split=5</code>, <code>min_samples_leaf=2</code></li> <li><code>n_jobs=-1</code>, <code>random_state=42</code></li> </ul>"},{"location":"reference/modeling_nodes.html#hyperparameter-tuning","title":"Hyperparameter tuning","text":""},{"location":"reference/modeling_nodes.html#hyperparameter_tuner","title":"hyperparameter_tuner","text":"<p>This mode wraps a base model and performs search.</p> <p>Config:</p> <ul> <li><code>type</code>: <code>hyperparameter_tuner</code></li> <li><code>base_model</code>: dict with a supported base model type (e.g., logistic regression)</li> <li>tuning options such as:</li> <li><code>strategy</code>: <code>grid</code> | <code>random</code> | <code>halving_grid</code> | <code>halving_random</code> | <code>optuna</code> (availability depends on installed packages)</li> <li><code>search_space</code>: dict of parameter \u2192 list/range</li> <li><code>metric</code>: e.g., <code>accuracy</code>, <code>f1</code>, <code>roc_auc</code>, <code>rmse</code>, <code>r2</code></li> <li><code>cv_enabled</code>, <code>cv_type</code>, <code>cv_folds</code>, <code>random_state</code></li> </ul> <p>Learned params:</p> <ul> <li>a tuple <code>(best_model, tuning_result)</code> where <code>best_model</code> is a fitted estimator.</li> </ul>"},{"location":"reference/modeling_nodes.html#cross-validation","title":"Cross-validation","text":"<p><code>StatefulEstimator.cross_validate()</code> can perform CV on the train split and returns aggregated fold metrics.</p> <p>Note: <code>SkyulfPipeline</code> performs modeling through the same building blocks (a calculator + applier); <code>StatefulEstimator</code> is the lightweight wrapper exposed for low-level usage.</p>"},{"location":"reference/preprocessing_nodes.html","title":"Preprocessing Nodes","text":"<p>This page documents the preprocessing node types supported by <code>FeatureEngineer</code>.</p>"},{"location":"reference/preprocessing_nodes.html#step-schema","title":"Step schema","text":"<p>Every preprocessing step in the pipeline config uses:</p> <pre><code>{\n  \"name\": \"...\",\n  \"transformer\": \"TransformerType\",\n  \"params\": { ... }\n}\n</code></pre> <p>Where <code>params</code> is passed into the node\u2019s Calculator <code>fit()</code>.</p>"},{"location":"reference/preprocessing_nodes.html#splitters","title":"Splitters","text":"<p>Example step:</p> <pre><code>{\"name\": \"split\", \"transformer\": \"TrainTestSplitter\", \"params\": {\"test_size\": 0.2, \"random_state\": 42, \"target_column\": \"target\"}}\n</code></pre>"},{"location":"reference/preprocessing_nodes.html#traintestsplitter","title":"TrainTestSplitter","text":"<p>Splits a DataFrame (or <code>(X, y)</code> tuple) into <code>SplitDataset(train, test, validation)</code>.</p> <p>Config (<code>params</code>):</p> <ul> <li><code>test_size</code>: float (default 0.2)</li> <li><code>validation_size</code>: float (default 0.0)</li> <li><code>random_state</code>: int (default 42)</li> <li><code>shuffle</code>: bool (default True)</li> <li><code>stratify</code>: bool (default False)</li> <li><code>target_column</code>: str (required only when splitting a DataFrame and using stratify)</li> </ul> <p>Learned params: none (passes through config).</p>"},{"location":"reference/preprocessing_nodes.html#feature_target_split","title":"feature_target_split","text":"<p>Splits a DataFrame into <code>(X, y)</code> (or applies the split to each <code>SplitDataset</code> split).</p> <p>Config:</p> <ul> <li><code>target_column</code>: str (required)</li> </ul> <p>Learned params: none.</p>"},{"location":"reference/preprocessing_nodes.html#cleaning","title":"Cleaning","text":"<p>Example step:</p> <pre><code>{\n  \"name\": \"clean_text\",\n  \"transformer\": \"TextCleaning\",\n  \"params\": {\n    \"columns\": [\"free_text\"],\n    \"operations\": [\n      {\"op\": \"trim\", \"mode\": \"both\"},\n      {\"op\": \"case\", \"mode\": \"lower\"},\n      {\"op\": \"regex\", \"mode\": \"collapse_whitespace\"}\n    ]\n  }\n}\n</code></pre>"},{"location":"reference/preprocessing_nodes.html#textcleaning","title":"TextCleaning","text":"<p>Applies a list of string operations.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str] (optional; auto-detects text-like columns)</li> <li><code>operations</code>: list[dict]</li> <li><code>{ \"op\": \"trim\", \"mode\": \"both\"|\"leading\"|\"trailing\" }</code></li> <li><code>{ \"op\": \"case\", \"mode\": \"lower\"|\"upper\"|\"title\"|\"sentence\" }</code></li> <li><code>{ \"op\": \"remove_special\", \"mode\": \"keep_alphanumeric\"|\"keep_alphanumeric_space\"|\"letters_only\"|\"digits_only\", \"replacement\": \"\" }</code></li> <li><code>{ \"op\": \"regex\", \"mode\": \"collapse_whitespace\"|\"extract_digits\"|\"custom\", \"pattern\": \"...\", \"repl\": \"...\" }</code></li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>operations</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#valuereplacement","title":"ValueReplacement","text":"<p>Replaces values in selected columns.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li>Either:</li> <li><code>mapping</code>: dict (global mapping) or dict[col -&gt; mapping]</li> <li><code>to_replace</code> + <code>value</code></li> <li><code>replacements</code>: list of <code>{old, new}</code> (converted into a mapping)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>mapping</code>, <code>to_replace</code>, <code>value</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#aliasreplacement","title":"AliasReplacement","text":"<p>Normalizes common textual aliases (boolean/country/custom).</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str] (optional; auto-detects text-like columns)</li> <li><code>alias_type</code>: <code>boolean</code> | <code>country</code> | <code>custom</code> (also supports legacy <code>mode</code>)</li> <li><code>custom_map</code>: dict[str, str] (also supports legacy <code>custom_pairs</code>)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>alias_type</code>, <code>custom_map</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#invalidvaluereplacement","title":"InvalidValueReplacement","text":"<p>Replaces invalid numeric values.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>rule</code>: <code>negative</code> | <code>zero</code> | <code>negative_to_nan</code> | <code>custom_range</code> (also supports legacy <code>mode</code>)</li> <li><code>replacement</code>: any (default NaN)</li> <li><code>min_value</code> / <code>max_value</code>: used by <code>custom_range</code></li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>rule</code>, <code>replacement</code>, <code>min_value</code>, <code>max_value</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#drop-missing","title":"Drop &amp; Missing","text":""},{"location":"reference/preprocessing_nodes.html#deduplicate","title":"Deduplicate","text":"<p>Config:</p> <ul> <li><code>subset</code>: list[str] | None</li> <li><code>keep</code>: <code>first</code> | <code>last</code> | <code>none</code> (mapped to <code>False</code>)</li> </ul> <p>Learned params:</p> <ul> <li><code>subset</code>, <code>keep</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#dropmissingcolumns","title":"DropMissingColumns","text":"<p>Config:</p> <ul> <li><code>missing_threshold</code>: float (percentage; if &gt; 0, drops columns with missing% &gt;= threshold)</li> <li><code>columns</code>: list[str] (explicit columns to drop)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns_to_drop</code>, <code>threshold</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#dropmissingrows","title":"DropMissingRows","text":"<p>Config:</p> <ul> <li><code>subset</code>: list[str] | None</li> <li><code>how</code>: <code>any</code> | <code>all</code> (ignored if <code>threshold</code> provided)</li> <li><code>threshold</code>: int | None (min non-null values)</li> </ul> <p>Learned params:</p> <ul> <li><code>subset</code>, <code>how</code>, <code>threshold</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#missingindicator","title":"MissingIndicator","text":"<p>Adds <code>{col}_missing</code> indicator columns.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str] (optional; defaults to all columns with any missing values)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#imputation","title":"Imputation","text":"<p>Example step:</p> <pre><code>{\"name\": \"impute\", \"transformer\": \"SimpleImputer\", \"params\": {\"strategy\": \"median\", \"columns\": [\"age\"]}}\n</code></pre>"},{"location":"reference/preprocessing_nodes.html#simpleimputer","title":"SimpleImputer","text":"<p>Config:</p> <ul> <li><code>strategy</code>: <code>mean</code> | <code>median</code> | <code>most_frequent</code> | <code>constant</code> (also accepts <code>mode</code>)</li> <li><code>columns</code>: list[str] (optional; numeric auto-detection for mean/median)</li> <li><code>fill_value</code>: any (used for <code>constant</code>)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>strategy</code></li> <li><code>fill_values</code>: dict[col -&gt; value]</li> <li><code>missing_counts</code>: dict[col -&gt; count]</li> <li><code>total_missing</code>: int</li> </ul>"},{"location":"reference/preprocessing_nodes.html#knnimputer","title":"KNNImputer","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str] (numeric)</li> <li><code>n_neighbors</code>: int (default 5)</li> <li><code>weights</code>: <code>uniform</code> | <code>distance</code></li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>imputer_object</code> (sklearn object; pickled in pipeline)</li> <li><code>n_neighbors</code>, <code>weights</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#iterativeimputer","title":"IterativeImputer","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str] (numeric)</li> <li><code>max_iter</code>: int (default 10)</li> <li><code>estimator</code>: <code>BayesianRidge</code> | <code>DecisionTree</code> | <code>ExtraTrees</code> | <code>KNeighbors</code></li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>imputer_object</code> (sklearn object; pickled in pipeline)</li> <li><code>estimator</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#encoding","title":"Encoding","text":"<p>Example step:</p> <pre><code>{\"name\": \"encode\", \"transformer\": \"OneHotEncoder\", \"params\": {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"}}\n</code></pre>"},{"location":"reference/preprocessing_nodes.html#onehotencoder","title":"OneHotEncoder","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str] (optional; auto-detects categorical columns)</li> <li><code>drop_first</code>: bool (default False)</li> <li><code>max_categories</code>: int (default 20)</li> <li><code>handle_unknown</code>: <code>ignore</code> | <code>error</code> (default ignore)</li> <li><code>drop_original</code>: bool (default True)</li> <li><code>include_missing</code>: bool (default False)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>encoder_object</code> (sklearn OneHotEncoder)</li> <li><code>feature_names</code>: list[str]</li> <li><code>drop_original</code>, <code>include_missing</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#dummyencoder","title":"DummyEncoder","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>drop_first</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>categories</code>: dict[col -&gt; list[str]]</li> <li><code>drop_first</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#ordinalencoder","title":"OrdinalEncoder","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>handle_unknown</code>: str (default <code>use_encoded_value</code>)</li> <li><code>unknown_value</code>: int/float (default -1)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>encoder_object</code> (sklearn OrdinalEncoder)</li> <li><code>categories_count</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#labelencoder","title":"LabelEncoder","text":"<p>Encodes either target or selected feature columns.</p> <p>Config:</p> <ul> <li><code>columns</code>: optional list[str]</li> <li>if omitted, encodes the provided target <code>y</code></li> <li>if provided, encodes those feature columns (and also target if included)</li> </ul> <p>Learned params:</p> <ul> <li><code>encoders</code>: dict[col or \"target\" -&gt; sklearn LabelEncoder]</li> <li><code>classes_count</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#targetencoder","title":"TargetEncoder","text":"<p>Requires a target series (<code>y</code>).</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>smooth</code>: <code>auto</code> or numeric</li> <li><code>target_type</code>: <code>auto</code> or explicit type</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>encoder_object</code> (sklearn TargetEncoder)</li> </ul>"},{"location":"reference/preprocessing_nodes.html#hashencoder","title":"HashEncoder","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>n_features</code>: int (default 10)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>n_features</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#scaling","title":"Scaling","text":"<p>All scaling nodes accept <code>columns</code> (optional; numeric auto-detect) and return learned numeric arrays.</p>"},{"location":"reference/preprocessing_nodes.html#standardscaler","title":"StandardScaler","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>with_mean</code>: bool (default True)</li> <li><code>with_std</code>: bool (default True)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>mean</code>, <code>scale</code>, <code>var</code>, <code>with_mean</code>, <code>with_std</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#minmaxscaler","title":"MinMaxScaler","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>feature_range</code>: tuple (default (0, 1))</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>min</code>, <code>scale</code>, <code>data_min</code>, <code>data_max</code>, <code>feature_range</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#robustscaler","title":"RobustScaler","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>quantile_range</code>: tuple (default (25.0, 75.0))</li> <li><code>with_centering</code>: bool (default True)</li> <li><code>with_scaling</code>: bool (default True)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>center</code>, <code>scale</code>, <code>quantile_range</code>, <code>with_centering</code>, <code>with_scaling</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#maxabsscaler","title":"MaxAbsScaler","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>scale</code>, <code>max_abs</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#outliers","title":"Outliers","text":""},{"location":"reference/preprocessing_nodes.html#iqr","title":"IQR","text":"<p>Filters rows outside per-column IQR bounds.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>multiplier</code>: float (default 1.5)</li> </ul> <p>Learned params:</p> <ul> <li><code>bounds</code>: dict[col -&gt; {lower, upper}]</li> <li><code>warnings</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#zscore","title":"ZScore","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>threshold</code>: float (default 3.0)</li> </ul> <p>Learned params:</p> <ul> <li><code>stats</code>: dict[col -&gt; {mean, std}]</li> <li><code>threshold</code>, <code>warnings</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#winsorize","title":"Winsorize","text":"<p>Clips values into per-column percentile bounds.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>lower_percentile</code>: float (default 5.0)</li> <li><code>upper_percentile</code>: float (default 95.0)</li> </ul> <p>Learned params:</p> <ul> <li><code>bounds</code>: dict[col -&gt; {lower, upper}]</li> </ul>"},{"location":"reference/preprocessing_nodes.html#manualbounds","title":"ManualBounds","text":"<p>Filters rows outside user-provided bounds.</p> <p>Config:</p> <ul> <li><code>bounds</code>: dict[col -&gt; {lower, upper}]</li> </ul> <p>Learned params:</p> <ul> <li><code>bounds</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#ellipticenvelope","title":"EllipticEnvelope","text":"<p>Learns a per-column EllipticEnvelope model and filters outliers.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>contamination</code>: float (default 0.01)</li> </ul> <p>Learned params:</p> <ul> <li><code>models</code>: dict[col -&gt; sklearn model]</li> <li><code>contamination</code>, <code>warnings</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#transformations","title":"Transformations","text":""},{"location":"reference/preprocessing_nodes.html#powertransformer","title":"PowerTransformer","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>method</code>: <code>yeo-johnson</code> | <code>box-cox</code></li> <li><code>standardize</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>lambdas</code>, <code>method</code>, <code>standardize</code>, <code>scaler_params</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#simpletransformation","title":"SimpleTransformation","text":"<p>Config:</p> <ul> <li><code>transformations</code>: list of <code>{column, method, clip_threshold?}</code></li> <li>methods include <code>log</code>, <code>square_root</code>, <code>cube_root</code>, <code>reciprocal</code>, <code>square</code>, <code>exponential</code></li> </ul> <p>Learned params:</p> <ul> <li><code>transformations</code> (passes through)</li> </ul>"},{"location":"reference/preprocessing_nodes.html#generaltransformation","title":"GeneralTransformation","text":"<p>Config:</p> <ul> <li><code>transformations</code>: list of <code>{column, method, clip_threshold?}</code></li> <li>methods include power transforms (<code>box-cox</code>, <code>yeo-johnson</code>) and the simple methods</li> </ul> <p>Learned params:</p> <ul> <li><code>transformations</code> with fitted <code>lambdas</code>/<code>scaler_params</code> where applicable</li> </ul>"},{"location":"reference/preprocessing_nodes.html#bucketing-binning","title":"Bucketing (Binning)","text":""},{"location":"reference/preprocessing_nodes.html#generalbinning","title":"GeneralBinning","text":"<p>Creates binned features with configurable strategies.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str] (numeric)</li> <li><code>strategy</code>: <code>equal_width</code> | <code>equal_frequency</code> | <code>kmeans</code> | <code>custom</code> | <code>kbins</code></li> <li><code>n_bins</code> and strategy-specific keys:</li> <li><code>equal_width_bins</code>, <code>equal_frequency_bins</code>, <code>duplicates</code></li> <li><code>kbins_n_bins</code>, <code>kbins_strategy</code></li> <li><code>custom_bins</code>: dict[col -&gt; edges]</li> <li><code>custom_labels</code>: dict[col -&gt; labels]</li> <li>output formatting:</li> <li><code>output_suffix</code>, <code>drop_original</code>, <code>label_format</code>, <code>missing_strategy</code>, <code>missing_label</code>, <code>include_lowest</code>, <code>precision</code></li> </ul> <p>Learned params:</p> <ul> <li><code>bin_edges</code> (dict[col -&gt; edges])</li> <li>output formatting settings</li> </ul>"},{"location":"reference/preprocessing_nodes.html#custombinning","title":"CustomBinning","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>bins</code>: list[float] (shared edges)</li> <li>plus output formatting keys (same as GeneralBinning)</li> </ul> <p>Learned params:</p> <ul> <li><code>bin_edges</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#kbinsdiscretizer","title":"KBinsDiscretizer","text":"<p>Wrapper around <code>GeneralBinning</code> with a KBins-style interface.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>n_bins</code>: int</li> <li><code>strategy</code>: <code>uniform</code> | <code>quantile</code> | <code>kmeans</code></li> </ul> <p>Learned params:</p> <ul> <li><code>bin_edges</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#casting","title":"Casting","text":""},{"location":"reference/preprocessing_nodes.html#casting_1","title":"Casting","text":"<p>Config:</p> <ul> <li>Either:</li> <li><code>column_types</code>: dict[col -&gt; dtype]</li> <li>or <code>columns</code> + <code>target_type</code></li> <li><code>coerce_on_error</code>: bool (default True)</li> </ul> <p>Learned params:</p> <ul> <li><code>type_map</code>, <code>coerce_on_error</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#feature-generation","title":"Feature Generation","text":""},{"location":"reference/preprocessing_nodes.html#polynomialfeatures","title":"PolynomialFeatures","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>auto_detect</code>: bool</li> <li><code>degree</code>: int</li> <li><code>interaction_only</code>: bool</li> <li><code>include_bias</code>: bool</li> <li><code>include_input_features</code>: bool</li> <li><code>output_prefix</code>: str</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>degree</code>, <code>interaction_only</code>, <code>include_bias</code>, <code>include_input_features</code>, <code>output_prefix</code>, <code>feature_names</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#featuremath-featuregenerationnode","title":"FeatureMath / FeatureGenerationNode","text":"<p>Config:</p> <ul> <li><code>operations</code>: list[dict]</li> <li><code>epsilon</code>: float (default 1e-9)</li> <li><code>allow_overwrite</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>operations</code>, <code>epsilon</code>, <code>allow_overwrite</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#feature-selection","title":"Feature Selection","text":""},{"location":"reference/preprocessing_nodes.html#variancethreshold","title":"VarianceThreshold","text":"<p>Config:</p> <ul> <li><code>threshold</code>: float (default 0.0)</li> <li><code>columns</code>: list[str] (optional; numeric)</li> <li><code>drop_columns</code>: bool (default True)</li> </ul> <p>Learned params:</p> <ul> <li><code>candidate_columns</code>, <code>selected_columns</code>, <code>variances</code>, <code>threshold</code>, <code>drop_columns</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#correlationthreshold","title":"CorrelationThreshold","text":"<p>Config:</p> <ul> <li><code>threshold</code>: float (default 0.95)</li> <li><code>correlation_method</code>: <code>pearson</code> | <code>spearman</code> | <code>kendall</code> (default pearson)</li> <li><code>columns</code>: list[str] (numeric)</li> <li><code>drop_columns</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>columns_to_drop</code>, <code>threshold</code>, <code>method</code>, <code>drop_columns</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#univariateselection","title":"UnivariateSelection","text":"<p>Config:</p> <ul> <li><code>target_column</code>: str (if <code>y</code> not passed as tuple)</li> <li><code>problem_type</code>: <code>auto</code> | <code>classification</code> | <code>regression</code></li> <li><code>method</code>: <code>select_k_best</code> | <code>select_percentile</code> | <code>select_fpr</code> | <code>select_fdr</code> | <code>select_fwe</code> | <code>generic_univariate_select</code></li> <li>selector parameters (depending on method): <code>k</code>, <code>percentile</code>, <code>alpha</code>, <code>mode</code>, <code>param</code></li> <li>scoring: <code>score_func</code> (e.g., <code>f_classif</code>, <code>mutual_info_classif</code>, \u2026)</li> <li><code>drop_columns</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>selected_columns</code>, <code>candidate_columns</code>, <code>scores</code>, <code>pvalues</code> (when available), plus selector config</li> </ul>"},{"location":"reference/preprocessing_nodes.html#modelbasedselection","title":"ModelBasedSelection","text":"<p>Config:</p> <ul> <li><code>target_column</code>: str</li> <li><code>problem_type</code>: <code>auto</code> | <code>classification</code> | <code>regression</code></li> <li><code>method</code>: <code>select_from_model</code> | <code>rfe</code></li> <li><code>estimator</code>: <code>auto</code> | <code>logistic_regression</code> | <code>random_forest</code> | <code>linear_regression</code></li> <li>For select_from_model: <code>threshold</code>, <code>max_features</code></li> <li>For RFE: <code>n_features_to_select</code>, <code>step</code></li> <li><code>drop_columns</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>selected_columns</code>, <code>candidate_columns</code>, and method-specific metadata</li> </ul>"},{"location":"reference/preprocessing_nodes.html#feature_selection","title":"feature_selection","text":"<p>A higher-level facade node that dispatches to the selection implementations.</p>"},{"location":"reference/preprocessing_nodes.html#resampling","title":"Resampling","text":""},{"location":"reference/preprocessing_nodes.html#oversampling","title":"Oversampling","text":"<p>Config:</p> <ul> <li><code>method</code>: <code>smote</code> | <code>adasyn</code> | <code>borderline_smote</code> | <code>svm_smote</code> | <code>kmeans_smote</code> | <code>smote_tomek</code></li> <li><code>target_column</code>: required if <code>y</code> is not provided as tuple</li> <li><code>sampling_strategy</code>: <code>auto</code> or dict</li> <li><code>random_state</code>: int</li> <li>method-specific keys: <code>k_neighbors</code>, <code>m_neighbors</code>, <code>kind</code>, <code>out_step</code>, <code>cluster_balance_threshold</code>, <code>density_exponent</code>, <code>n_jobs</code></li> </ul> <p>Learned params: none (passes through config).</p>"},{"location":"reference/preprocessing_nodes.html#undersampling","title":"Undersampling","text":"<p>Config:</p> <ul> <li><code>method</code>: <code>random_under_sampling</code> | <code>nearmiss</code> | <code>tomek_links</code> | <code>edited_nearest_neighbours</code></li> <li><code>target_column</code>: required if <code>y</code> not provided as tuple</li> <li><code>sampling_strategy</code>, <code>random_state</code>, <code>replacement</code>, <code>version</code>, <code>n_neighbors</code>, <code>kind_sel</code>, <code>n_jobs</code></li> </ul> <p>Learned params: none.</p>"},{"location":"reference/preprocessing_nodes.html#inspection","title":"Inspection","text":""},{"location":"reference/preprocessing_nodes.html#datasetprofile","title":"DatasetProfile","text":"<p>Captures basic dataset stats without modifying data.</p> <p>Config: none.</p> <p>Learned params:</p> <ul> <li><code>profile</code>: rows/columns/dtypes/missing/numeric_stats</li> </ul>"},{"location":"reference/preprocessing_nodes.html#datasnapshot","title":"DataSnapshot","text":"<p>Captures the first N rows without modifying data.</p> <p>Config:</p> <ul> <li><code>n_rows</code>: int (default 5)</li> </ul> <p>Learned params:</p> <ul> <li><code>snapshot</code>: list[dict]</li> </ul>"},{"location":"reference/api/pipeline.html","title":"API: Pipeline","text":""},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline","title":"<code>skyulf.pipeline.SkyulfPipeline</code>","text":"<p>End-to-end ML Pipeline.</p> <p>Encapsulates: 1. Feature Engineering (Preprocessing) 2. Modeling (Training/Inference)</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>class SkyulfPipeline:\n    \"\"\"\n    End-to-end ML Pipeline.\n\n    Encapsulates:\n    1. Feature Engineering (Preprocessing)\n    2. Modeling (Training/Inference)\n    \"\"\"\n\n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        Initialize the pipeline.\n\n        Args:\n            config: Pipeline configuration dictionary.\n                    Must contain 'preprocessing' (list) and 'modeling' (dict).\n        \"\"\"\n        self.config = config\n        self.preprocessing_steps = config.get(\"preprocessing\", [])\n        self.modeling_config = config.get(\"modeling\", {})\n\n        self.feature_engineer = FeatureEngineer(self.preprocessing_steps)\n        self.model_estimator: Optional[StatefulEstimator] = None\n\n        # Initialize model estimator if config is present\n        if self.modeling_config:\n            self._init_model_estimator()\n\n    def _init_model_estimator(self):\n        \"\"\"Initialize the StatefulEstimator based on config.\"\"\"\n        model_type = self.modeling_config.get(\"type\")\n        node_id = self.modeling_config.get(\"node_id\", \"model_node\")\n\n        calculator: Optional[BaseModelCalculator] = None\n        applier: Optional[BaseModelApplier] = None\n\n        # Map model types to classes\n        # This mapping should ideally be dynamic or registered\n        if model_type == \"logistic_regression\":\n            calculator = LogisticRegressionCalculator()\n            applier = LogisticRegressionApplier()\n        elif model_type == \"random_forest_classifier\":\n            calculator = RandomForestClassifierCalculator()\n            applier = RandomForestClassifierApplier()\n        elif model_type == \"ridge_regression\":\n            calculator = RidgeRegressionCalculator()\n            applier = RidgeRegressionApplier()\n        elif model_type == \"random_forest_regressor\":\n            calculator = RandomForestRegressorCalculator()\n            applier = RandomForestRegressorApplier()\n        elif model_type == \"hyperparameter_tuner\":\n            # Tuner wraps another model\n            base_model_config = self.modeling_config.get(\"base_model\", {})\n            base_model_type = base_model_config.get(\"type\")\n\n            base_calc: Optional[BaseModelCalculator] = None\n            base_applier: Optional[BaseModelApplier] = None\n            if base_model_type == \"logistic_regression\":\n                base_calc = LogisticRegressionCalculator()\n                base_applier = LogisticRegressionApplier()\n            elif base_model_type == \"random_forest_classifier\":\n                base_calc = RandomForestClassifierCalculator()\n                base_applier = RandomForestClassifierApplier()\n            elif base_model_type == \"ridge_regression\":\n                base_calc = RidgeRegressionCalculator()\n                base_applier = RidgeRegressionApplier()\n            elif base_model_type == \"random_forest_regressor\":\n                base_calc = RandomForestRegressorCalculator()\n                base_applier = RandomForestRegressorApplier()\n\n            if base_calc and base_applier:\n                calculator = TunerCalculator(base_calc)\n                applier = TunerApplier(base_applier)\n            else:\n                raise ValueError(\n                    f\"Unknown base model type for tuner: {base_model_type}\"\n                )\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n        self.model_estimator = StatefulEstimator(\n            node_id=node_id, calculator=calculator, applier=applier\n        )\n\n    def fit(\n        self, data: Union[pd.DataFrame, SplitDataset], target_column: str\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Fit the pipeline.\n\n        Args:\n            data: Input data (DataFrame or SplitDataset).\n            target_column: Name of the target column.\n\n        Returns:\n            Dictionary containing execution metrics.\n        \"\"\"\n        metrics = {}\n\n        # 1. Feature Engineering\n        logger.info(\"Starting Feature Engineering...\")\n        transformed_data, fe_metrics = self.feature_engineer.fit_transform(data)\n        metrics[\"preprocessing\"] = fe_metrics\n\n        # 2. Modeling\n        if self.model_estimator:\n            logger.info(\"Starting Model Training...\")\n\n            # Ensure transformed_data is SplitDataset for modeling\n            if isinstance(transformed_data, pd.DataFrame):\n                # If we only have a DataFrame, we can't really evaluate properly without a split\n                # But we can fit on it.\n                # Ideally, the user should provide a SplitDataset or use a Splitter node in preprocessing.\n                # If preprocessing didn't split, we wrap it.\n                dataset = SplitDataset(\n                    train=transformed_data, test=pd.DataFrame(), validation=None\n                )\n            else:\n                dataset = transformed_data\n\n            # Fit the model\n            # Note: fit_predict updates self.model_estimator.model in-memory\n            _ = self.model_estimator.fit_predict(\n                dataset=dataset,\n                target_column=target_column,\n                config=self.modeling_config,\n            )\n\n            # Evaluate\n            # We can run evaluation if we have test/validation sets\n            try:\n                eval_report = self.model_estimator.evaluate(\n                    dataset=dataset, target_column=target_column\n                )\n                metrics[\"modeling\"] = eval_report\n            except Exception as e:\n                logger.warning(f\"Evaluation failed: {e}\")\n                metrics[\"modeling_error\"] = str(e)\n\n        return metrics\n\n    def predict(self, data: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"\n        Generate predictions.\n\n        Args:\n            data: Input DataFrame.\n\n        Returns:\n            Series of predictions.\n        \"\"\"\n        # 1. Feature Engineering (Transform only)\n        transformed_data = self.feature_engineer.transform(data)\n\n        # 2. Modeling\n        if self.model_estimator and self.model_estimator.model is not None:\n            return self.model_estimator.applier.predict(\n                transformed_data, self.model_estimator.model\n            )\n        else:\n            raise ValueError(\"Pipeline not fitted or no model configured.\")\n\n    def save(self, path: str):\n        \"\"\"Save the pipeline to a file.\"\"\"\n        # We can use pickle to save the whole object since we removed external dependencies\n        with open(path, \"wb\") as f:\n            pickle.dump(self, f)\n\n    @classmethod\n    def load(cls, path: str) -&gt; \"SkyulfPipeline\":\n        \"\"\"Load the pipeline from a file.\"\"\"\n        with open(path, \"rb\") as f:\n            return pickle.load(f)  # type: ignore\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>Pipeline configuration dictionary.     Must contain 'preprocessing' (list) and 'modeling' (dict).</p> required Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>def __init__(self, config: Dict[str, Any]):\n    \"\"\"\n    Initialize the pipeline.\n\n    Args:\n        config: Pipeline configuration dictionary.\n                Must contain 'preprocessing' (list) and 'modeling' (dict).\n    \"\"\"\n    self.config = config\n    self.preprocessing_steps = config.get(\"preprocessing\", [])\n    self.modeling_config = config.get(\"modeling\", {})\n\n    self.feature_engineer = FeatureEngineer(self.preprocessing_steps)\n    self.model_estimator: Optional[StatefulEstimator] = None\n\n    # Initialize model estimator if config is present\n    if self.modeling_config:\n        self._init_model_estimator()\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.fit","title":"<code>fit(data, target_column)</code>","text":"<p>Fit the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, SplitDataset]</code> <p>Input data (DataFrame or SplitDataset).</p> required <code>target_column</code> <code>str</code> <p>Name of the target column.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing execution metrics.</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>def fit(\n    self, data: Union[pd.DataFrame, SplitDataset], target_column: str\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Fit the pipeline.\n\n    Args:\n        data: Input data (DataFrame or SplitDataset).\n        target_column: Name of the target column.\n\n    Returns:\n        Dictionary containing execution metrics.\n    \"\"\"\n    metrics = {}\n\n    # 1. Feature Engineering\n    logger.info(\"Starting Feature Engineering...\")\n    transformed_data, fe_metrics = self.feature_engineer.fit_transform(data)\n    metrics[\"preprocessing\"] = fe_metrics\n\n    # 2. Modeling\n    if self.model_estimator:\n        logger.info(\"Starting Model Training...\")\n\n        # Ensure transformed_data is SplitDataset for modeling\n        if isinstance(transformed_data, pd.DataFrame):\n            # If we only have a DataFrame, we can't really evaluate properly without a split\n            # But we can fit on it.\n            # Ideally, the user should provide a SplitDataset or use a Splitter node in preprocessing.\n            # If preprocessing didn't split, we wrap it.\n            dataset = SplitDataset(\n                train=transformed_data, test=pd.DataFrame(), validation=None\n            )\n        else:\n            dataset = transformed_data\n\n        # Fit the model\n        # Note: fit_predict updates self.model_estimator.model in-memory\n        _ = self.model_estimator.fit_predict(\n            dataset=dataset,\n            target_column=target_column,\n            config=self.modeling_config,\n        )\n\n        # Evaluate\n        # We can run evaluation if we have test/validation sets\n        try:\n            eval_report = self.model_estimator.evaluate(\n                dataset=dataset, target_column=target_column\n            )\n            metrics[\"modeling\"] = eval_report\n        except Exception as e:\n            logger.warning(f\"Evaluation failed: {e}\")\n            metrics[\"modeling_error\"] = str(e)\n\n    return metrics\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load the pipeline from a file.</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; \"SkyulfPipeline\":\n    \"\"\"Load the pipeline from a file.\"\"\"\n    with open(path, \"rb\") as f:\n        return pickle.load(f)  # type: ignore\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.predict","title":"<code>predict(data)</code>","text":"<p>Generate predictions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series of predictions.</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>def predict(self, data: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"\n    Generate predictions.\n\n    Args:\n        data: Input DataFrame.\n\n    Returns:\n        Series of predictions.\n    \"\"\"\n    # 1. Feature Engineering (Transform only)\n    transformed_data = self.feature_engineer.transform(data)\n\n    # 2. Modeling\n    if self.model_estimator and self.model_estimator.model is not None:\n        return self.model_estimator.applier.predict(\n            transformed_data, self.model_estimator.model\n        )\n    else:\n        raise ValueError(\"Pipeline not fitted or no model configured.\")\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.save","title":"<code>save(path)</code>","text":"<p>Save the pipeline to a file.</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>def save(self, path: str):\n    \"\"\"Save the pipeline to a file.\"\"\"\n    # We can use pickle to save the whole object since we removed external dependencies\n    with open(path, \"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"reference/api/data/dataset.html","title":"API: SplitDataset","text":""},{"location":"reference/api/data/dataset.html#skyulf.data.dataset.SplitDataset","title":"<code>skyulf.data.dataset.SplitDataset</code>  <code>dataclass</code>","text":"Source code in <code>skyulf-core/skyulf/data/dataset.py</code> <pre><code>@dataclass\nclass SplitDataset:\n    train: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]]\n    test: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]]\n    validation: Optional[Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]]] = None\n\n    def copy(self) -&gt; \"SplitDataset\":\n        def copy_data(data):\n            if isinstance(data, tuple):\n                return (data[0].copy(), data[1].copy())\n            return data.copy()\n\n        return SplitDataset(\n            train=copy_data(self.train),\n            test=copy_data(self.test),\n            validation=(\n                copy_data(self.validation) if self.validation is not None else None\n            ),\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html","title":"API: modeling","text":""},{"location":"reference/api/modeling/index.html#skyulf.modeling","title":"<code>skyulf.modeling</code>","text":"<p>Modeling module for Skyulf.</p>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelApplier","title":"<code>BaseModelApplier</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class BaseModelApplier(ABC):\n    @abstractmethod\n    def predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n        \"\"\"\n        Generates predictions.\n        \"\"\"\n        pass\n\n    def predict_proba(\n        self, df: pd.DataFrame, model_artifact: Any\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"\n        Generates prediction probabilities if supported.\n        Returns DataFrame where columns are classes.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelApplier.predict","title":"<code>predict(df, model_artifact)</code>  <code>abstractmethod</code>","text":"<p>Generates predictions.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>@abstractmethod\ndef predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n    \"\"\"\n    Generates predictions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelApplier.predict_proba","title":"<code>predict_proba(df, model_artifact)</code>","text":"<p>Generates prediction probabilities if supported. Returns DataFrame where columns are classes.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def predict_proba(\n    self, df: pd.DataFrame, model_artifact: Any\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Generates prediction probabilities if supported.\n    Returns DataFrame where columns are classes.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelCalculator","title":"<code>BaseModelCalculator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class BaseModelCalculator(ABC):\n    @property\n    @abstractmethod\n    def problem_type(self) -&gt; str:\n        \"\"\"Returns 'classification' or 'regression'.\"\"\"\n        pass\n\n    @property\n    def default_params(self) -&gt; Dict[str, Any]:\n        \"\"\"Default hyperparameters for the model.\"\"\"\n        return {}\n\n    @abstractmethod\n    def fit(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        config: Dict[str, Any],\n        progress_callback: Optional[Callable[..., None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        validation_data: Optional[tuple[pd.DataFrame, pd.Series]] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Trains the model. Returns the model object (serializable).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelCalculator.default_params","title":"<code>default_params</code>  <code>property</code>","text":"<p>Default hyperparameters for the model.</p>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelCalculator.problem_type","title":"<code>problem_type</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns 'classification' or 'regression'.</p>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>  <code>abstractmethod</code>","text":"<p>Trains the model. Returns the model object (serializable).</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>@abstractmethod\ndef fit(\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: Dict[str, Any],\n    progress_callback: Optional[Callable[..., None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    validation_data: Optional[tuple[pd.DataFrame, pd.Series]] = None,\n) -&gt; Any:\n    \"\"\"\n    Trains the model. Returns the model object (serializable).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.HyperparameterField","title":"<code>HyperparameterField</code>  <code>dataclass</code>","text":"<p>Describe a single tunable hyperparameter.</p> Source code in <code>skyulf-core/skyulf/modeling/hyperparameters.py</code> <pre><code>@dataclass\nclass HyperparameterField:\n    \"\"\"Describe a single tunable hyperparameter.\"\"\"\n\n    name: str\n    label: str\n    type: str  # \"number\", \"select\", \"boolean\"\n    default: Any\n    description: str = \"\"\n    min: Optional[float] = None\n    max: Optional[float] = None\n    step: Optional[float] = None\n    options: Optional[List[Dict[str, Any]]] = (\n        None  # For 'select' type: [{\"label\": \"L1\", \"value\": \"l1\"}]\n    )\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return asdict(self)\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.LogisticRegressionApplier","title":"<code>LogisticRegressionApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Logistic Regression Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class LogisticRegressionApplier(SklearnApplier):\n    \"\"\"Logistic Regression Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.LogisticRegressionCalculator","title":"<code>LogisticRegressionCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Logistic Regression Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class LogisticRegressionCalculator(SklearnCalculator):\n    \"\"\"Logistic Regression Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=LogisticRegression,\n            default_params={\n                \"max_iter\": 1000,\n                \"solver\": \"lbfgs\",\n                \"random_state\": 42,\n            },\n            problem_type=\"classification\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RandomForestClassifierApplier","title":"<code>RandomForestClassifierApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Random Forest Classifier Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class RandomForestClassifierApplier(SklearnApplier):\n    \"\"\"Random Forest Classifier Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RandomForestClassifierCalculator","title":"<code>RandomForestClassifierCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Random Forest Classifier Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class RandomForestClassifierCalculator(SklearnCalculator):\n    \"\"\"Random Forest Classifier Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=RandomForestClassifier,\n            default_params={\n                \"n_estimators\": 50,\n                \"max_depth\": 10,\n                \"min_samples_split\": 5,\n                \"min_samples_leaf\": 2,\n                \"n_jobs\": -1,\n                \"random_state\": 42,\n            },\n            problem_type=\"classification\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RandomForestRegressorApplier","title":"<code>RandomForestRegressorApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Random Forest Regressor Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RandomForestRegressorApplier(SklearnApplier):\n    \"\"\"Random Forest Regressor Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RandomForestRegressorCalculator","title":"<code>RandomForestRegressorCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Random Forest Regressor Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RandomForestRegressorCalculator(SklearnCalculator):\n    \"\"\"Random Forest Regressor Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=RandomForestRegressor,\n            default_params={\n                \"n_estimators\": 50,\n                \"max_depth\": 10,\n                \"min_samples_split\": 5,\n                \"min_samples_leaf\": 2,\n                \"n_jobs\": -1,\n                \"random_state\": 42,\n            },\n            problem_type=\"regression\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RidgeRegressionApplier","title":"<code>RidgeRegressionApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Ridge Regression Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RidgeRegressionApplier(SklearnApplier):\n    \"\"\"Ridge Regression Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RidgeRegressionCalculator","title":"<code>RidgeRegressionCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Ridge Regression Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RidgeRegressionCalculator(SklearnCalculator):\n    \"\"\"Ridge Regression Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=Ridge,\n            default_params={\n                \"alpha\": 1.0,\n                \"solver\": \"auto\",\n                \"random_state\": 42,\n            },\n            problem_type=\"regression\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.SklearnApplier","title":"<code>SklearnApplier</code>","text":"<p>               Bases: <code>BaseModelApplier</code></p> <p>Base applier for Scikit-Learn models.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>class SklearnApplier(BaseModelApplier):\n    \"\"\"Base applier for Scikit-Learn models.\"\"\"\n\n    def predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n        \"\"\"Generate predictions.\"\"\"\n        # model_artifact is the fitted sklearn estimator\n        return pd.Series(model_artifact.predict(df), index=df.index)\n\n    def predict_proba(\n        self, df: pd.DataFrame, model_artifact: Any\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Generate prediction probabilities.\"\"\"\n        if hasattr(model_artifact, \"predict_proba\"):\n            try:\n                probas = model_artifact.predict_proba(df)\n                # Handle binary vs multiclass\n                # If binary, classes_ usually has 2 entries.\n                classes = getattr(model_artifact, \"classes_\", None)\n                if classes is None:\n                    # Fallback if classes_ is missing (unlikely for sklearn classifiers)\n                    return pd.DataFrame(probas, index=df.index)\n\n                return pd.DataFrame(probas, columns=classes, index=df.index)\n            except Exception:\n                return None\n        return None\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.SklearnApplier.predict","title":"<code>predict(df, model_artifact)</code>","text":"<p>Generate predictions.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>def predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n    \"\"\"Generate predictions.\"\"\"\n    # model_artifact is the fitted sklearn estimator\n    return pd.Series(model_artifact.predict(df), index=df.index)\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.SklearnApplier.predict_proba","title":"<code>predict_proba(df, model_artifact)</code>","text":"<p>Generate prediction probabilities.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>def predict_proba(\n    self, df: pd.DataFrame, model_artifact: Any\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Generate prediction probabilities.\"\"\"\n    if hasattr(model_artifact, \"predict_proba\"):\n        try:\n            probas = model_artifact.predict_proba(df)\n            # Handle binary vs multiclass\n            # If binary, classes_ usually has 2 entries.\n            classes = getattr(model_artifact, \"classes_\", None)\n            if classes is None:\n                # Fallback if classes_ is missing (unlikely for sklearn classifiers)\n                return pd.DataFrame(probas, index=df.index)\n\n            return pd.DataFrame(probas, columns=classes, index=df.index)\n        except Exception:\n            return None\n    return None\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.SklearnCalculator","title":"<code>SklearnCalculator</code>","text":"<p>               Bases: <code>BaseModelCalculator</code></p> <p>Base calculator for Scikit-Learn models.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>class SklearnCalculator(BaseModelCalculator):\n    \"\"\"Base calculator for Scikit-Learn models.\"\"\"\n\n    def __init__(\n        self,\n        model_class: Type[BaseEstimator],\n        default_params: Dict[str, Any],\n        problem_type: str,\n    ):\n        self.model_class = model_class\n        self._default_params = default_params\n        self._problem_type = problem_type\n\n    @property\n    def default_params(self) -&gt; Dict[str, Any]:\n        return self._default_params\n\n    @property\n    def problem_type(self) -&gt; str:\n        return self._problem_type\n\n    def fit(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        config: Dict[str, Any],\n        progress_callback=None,\n        log_callback=None,\n        validation_data=None,\n    ) -&gt; Any:\n        \"\"\"Fit the Scikit-Learn model.\"\"\"\n        # 1. Merge Config with Defaults\n        params = self.default_params.copy()\n        if config:\n            # We support two configuration structures:\n            # 1. Nested: {'params': {'C': 1.0, ...}} - Preferred\n            # 2. Flat: {'C': 1.0, 'type': '...', ...} - Legacy/Simple support\n\n            # Check for explicit 'params' dictionary first\n            overrides = config.get(\"params\", {})\n\n            # If 'params' key exists but is None or empty, check if there are other keys at top level\n            # that might be params. But be careful not to mix them.\n            # If config has 'params', we assume it's the source of truth.\n\n            if not overrides and \"params\" not in config:\n                # Fallback to flat config if 'params' key is completely missing\n                reserved_keys = {\n                    \"type\",\n                    \"target_column\",\n                    \"node_id\",\n                    \"step_type\",\n                    \"inputs\",\n                }\n                overrides = {\n                    k: v\n                    for k, v in config.items()\n                    if k not in reserved_keys and not isinstance(v, dict)\n                }\n\n            if overrides:\n                params.update(overrides)\n\n        msg = f\"Initializing {self.model_class.__name__} with params: {params}\"\n        logger.info(msg)\n        if log_callback:\n            log_callback(msg)\n\n        # 2. Instantiate Model\n        # Filter params to only those accepted by the model class\n        # This prevents errors if extra config is passed\n        # (Though sklearn usually ignores extra kwargs in __init__ if **kwargs is present,\n        # strict models might not)\n        # For now, we assume the user/config provides valid params.\n\n        # Handle special cases like 'random_state' if needed, but usually passed directly.\n\n        model = self.model_class(**params)\n\n        # 3. Fit\n        model.fit(X, y)\n\n        return model\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.SklearnCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>","text":"<p>Fit the Scikit-Learn model.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>def fit(\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: Dict[str, Any],\n    progress_callback=None,\n    log_callback=None,\n    validation_data=None,\n) -&gt; Any:\n    \"\"\"Fit the Scikit-Learn model.\"\"\"\n    # 1. Merge Config with Defaults\n    params = self.default_params.copy()\n    if config:\n        # We support two configuration structures:\n        # 1. Nested: {'params': {'C': 1.0, ...}} - Preferred\n        # 2. Flat: {'C': 1.0, 'type': '...', ...} - Legacy/Simple support\n\n        # Check for explicit 'params' dictionary first\n        overrides = config.get(\"params\", {})\n\n        # If 'params' key exists but is None or empty, check if there are other keys at top level\n        # that might be params. But be careful not to mix them.\n        # If config has 'params', we assume it's the source of truth.\n\n        if not overrides and \"params\" not in config:\n            # Fallback to flat config if 'params' key is completely missing\n            reserved_keys = {\n                \"type\",\n                \"target_column\",\n                \"node_id\",\n                \"step_type\",\n                \"inputs\",\n            }\n            overrides = {\n                k: v\n                for k, v in config.items()\n                if k not in reserved_keys and not isinstance(v, dict)\n            }\n\n        if overrides:\n            params.update(overrides)\n\n    msg = f\"Initializing {self.model_class.__name__} with params: {params}\"\n    logger.info(msg)\n    if log_callback:\n        log_callback(msg)\n\n    # 2. Instantiate Model\n    # Filter params to only those accepted by the model class\n    # This prevents errors if extra config is passed\n    # (Though sklearn usually ignores extra kwargs in __init__ if **kwargs is present,\n    # strict models might not)\n    # For now, we assume the user/config provides valid params.\n\n    # Handle special cases like 'random_state' if needed, but usually passed directly.\n\n    model = self.model_class(**params)\n\n    # 3. Fit\n    model.fit(X, y)\n\n    return model\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator","title":"<code>StatefulEstimator</code>","text":"Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class StatefulEstimator:\n    def __init__(\n        self, calculator: BaseModelCalculator, applier: BaseModelApplier, node_id: str\n    ):\n        self.calculator = calculator\n        self.applier = applier\n        self.node_id = node_id\n        self.model = None  # In-memory model storage\n\n    def _extract_xy(\n        self, data: Any, target_column: str\n    ) -&gt; tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Helper to extract X and y from DataFrame or Tuple.\"\"\"\n        if isinstance(data, tuple) and len(data) == 2:\n            return data[0], data[1]\n        elif isinstance(data, pd.DataFrame):\n            if target_column not in data.columns:\n                raise ValueError(f\"Target column '{target_column}' not found in data\")\n            return data.drop(columns=[target_column]), data[target_column]\n        else:\n            raise ValueError(f\"Unexpected data type: {type(data)}\")\n\n    def cross_validate(\n        self,\n        dataset: SplitDataset,\n        target_column: str,\n        config: Dict[str, Any],\n        n_folds: int = 5,\n        cv_type: str = \"k_fold\",\n        shuffle: bool = True,\n        random_state: int = 42,\n        progress_callback: Optional[Callable[[int, int], None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Performs cross-validation on the training split.\n        \"\"\"\n        # Import here to avoid circular dependency if any\n        from .cross_validation import perform_cross_validation\n\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n        return perform_cross_validation(\n            calculator=self.calculator,\n            applier=self.applier,\n            X=X_train,\n            y=y_train,\n            config=config,\n            n_folds=n_folds,\n            cv_type=cv_type,\n            shuffle=shuffle,\n            random_state=random_state,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n        )\n\n    def fit_predict(\n        self,\n        dataset: Union[SplitDataset, pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        target_column: str,\n        config: Dict[str, Any],\n        progress_callback: Optional[Callable[[int, int], None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        job_id: str = \"unknown\",\n    ) -&gt; Dict[str, pd.Series]:\n        \"\"\"\n        Fits the model on training data and returns predictions for all splits.\n        \"\"\"\n        # Handle raw DataFrame or Tuple input by wrapping it in a dummy SplitDataset\n        if isinstance(dataset, pd.DataFrame):\n            dataset = SplitDataset(train=dataset, test=pd.DataFrame(), validation=None)\n        elif isinstance(dataset, tuple):\n            # Check if it's (train_df, test_df) or (X, y)\n            elem0 = dataset[0]\n            if isinstance(elem0, pd.DataFrame) and target_column in elem0.columns:\n                # It's (train_df, test_df)\n                train_df, test_df = dataset\n                dataset = SplitDataset(train=train_df, test=test_df, validation=None)\n            else:\n                # It's (X, y) or something else, treat as train set\n                dataset = SplitDataset(\n                    train=dataset, test=pd.DataFrame(), validation=None\n                )\n\n        # 1. Prepare Data\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n        validation_data = None\n        if dataset.validation is not None:\n            X_val, y_val = self._extract_xy(dataset.validation, target_column)\n            validation_data = (X_val, y_val)\n\n        # 2. Train Model\n        self.model = self.calculator.fit(\n            X_train,\n            y_train,\n            config,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n            validation_data=validation_data,\n        )\n\n        # 3. Predict on all splits\n        predictions = {}\n\n        # Train Predictions\n        predictions[\"train\"] = self.applier.predict(X_train, self.model)\n\n        # Test Predictions\n        is_test_empty = False\n        if isinstance(dataset.test, tuple):\n            is_test_empty = dataset.test[0].empty\n        else:\n            is_test_empty = dataset.test.empty\n\n        if not is_test_empty:\n            if isinstance(dataset.test, tuple):\n                X_test, _ = dataset.test\n            else:\n                if target_column in dataset.test.columns:\n                    X_test = dataset.test.drop(columns=[target_column])\n                else:\n                    X_test = dataset.test\n            predictions[\"test\"] = self.applier.predict(X_test, self.model)\n\n        # Validation Predictions\n        if dataset.validation is not None:\n            if isinstance(dataset.validation, tuple):\n                X_val, _ = dataset.validation\n            else:\n                if target_column in dataset.validation.columns:\n                    X_val = dataset.validation.drop(columns=[target_column])\n                else:\n                    X_val = dataset.validation\n            predictions[\"validation\"] = self.applier.predict(X_val, self.model)\n\n        return predictions\n\n    def refit(\n        self,\n        dataset: SplitDataset,\n        target_column: str,\n        config: Dict[str, Any],\n        job_id: str = \"unknown\",\n    ) -&gt; None:\n        \"\"\"\n        Refits the model on Train + Validation data and updates the artifact.\n        \"\"\"\n        if dataset.validation is None:\n            # Fallback to normal fit if no validation set\n            self.fit_predict(dataset, target_column, config, job_id=job_id)\n            return\n\n        # 1. Prepare Combined Data\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n        X_val, y_val = self._extract_xy(dataset.validation, target_column)\n\n        X_combined = pd.concat([X_train, X_val], axis=0)\n        y_combined = pd.concat([y_train, y_val], axis=0)\n\n        # 2. Train Model\n        self.model = self.calculator.fit(X_combined, y_combined, config)\n\n    def evaluate(  # noqa: C901\n        self, dataset: SplitDataset, target_column: str, job_id: str = \"unknown\"\n    ) -&gt; Any:\n        \"\"\"\n        Evaluates the model on all splits and returns a detailed report.\n        \"\"\"\n        # Import here to avoid circular dependency\n        from .evaluation.classification import evaluate_classification_model\n        from .evaluation.regression import evaluate_regression_model\n\n        if self.model is None:\n            raise ValueError(\n                \"Model has not been trained yet. Call fit_predict() first.\"\n            )\n\n        problem_type = self.calculator.problem_type\n\n        splits_payload = {}\n\n        # Container for raw predictions\n        evaluation_data = {\n            \"job_id\": job_id,\n            \"node_id\": self.node_id,\n            \"problem_type\": problem_type,\n            \"splits\": {},\n        }\n\n        # Helper to evaluate a single split\n        def evaluate_split(split_name: str, data: Any):\n            if isinstance(data, tuple):\n                X, y = data\n            elif isinstance(data, pd.DataFrame):\n                if target_column not in data.columns:\n                    return None  # Cannot evaluate without target\n                X = data.drop(columns=[target_column])\n                y = data[target_column]\n            else:\n                return None\n\n            y_pred = self.applier.predict(X, self.model)\n\n            # Try to get probabilities for classification\n            y_proba = None\n            if problem_type == \"classification\":\n                y_proba_df = self.applier.predict_proba(X, self.model)\n                if y_proba_df is not None:\n                    y_proba = {\n                        \"classes\": y_proba_df.columns.tolist(),\n                        \"values\": y_proba_df.values.tolist(),\n                    }\n\n            split_data = {\n                \"y_true\": y.tolist() if hasattr(y, \"tolist\") else list(y),\n                \"y_pred\": (\n                    y_pred.tolist() if hasattr(y_pred, \"tolist\") else list(y_pred)\n                ),\n            }\n\n            if y_proba:\n                split_data[\"y_proba\"] = y_proba\n\n            evaluation_data[\"splits\"][split_name] = split_data\n\n            # Unpack model if it's a tuple (from Tuner)\n            model_to_evaluate = self.model\n            if isinstance(self.model, tuple) and len(self.model) == 2:\n                # Check if first element looks like a model (has fit/predict)\n                # or if it's just a convention from TunerCalculator\n                model_to_evaluate = self.model[0]\n\n            if problem_type == \"classification\":\n                return evaluate_classification_model(\n                    model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n                )\n            elif problem_type == \"regression\":\n                return evaluate_regression_model(\n                    model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n                )\n            else:\n                raise ValueError(f\"Unknown problem type: {problem_type}\")\n\n        # 2. Evaluate Train\n        splits_payload[\"train\"] = evaluate_split(\"train\", dataset.train)\n\n        # 3. Evaluate Test\n        has_test = False\n        if isinstance(dataset.test, pd.DataFrame):\n            has_test = not dataset.test.empty\n        elif isinstance(dataset.test, tuple):\n            has_test = len(dataset.test) == 2 and len(dataset.test[0]) &gt; 0\n\n        if has_test:\n            splits_payload[\"test\"] = evaluate_split(\"test\", dataset.test)\n\n        # 4. Evaluate Validation\n        if dataset.validation is not None:\n            has_val = False\n            if isinstance(dataset.validation, pd.DataFrame):\n                has_val = not dataset.validation.empty\n            elif isinstance(dataset.validation, tuple):\n                has_val = (\n                    len(dataset.validation) == 2 and len(dataset.validation[0]) &gt; 0\n                )\n\n            if has_val:\n                splits_payload[\"validation\"] = evaluate_split(\n                    \"validation\", dataset.validation\n                )\n\n        # Return report object (simplified for now, assuming schema matches)\n        return {\n            \"problem_type\": problem_type,\n            \"splits\": splits_payload,\n            \"raw_data\": evaluation_data,\n        }\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator.cross_validate","title":"<code>cross_validate(dataset, target_column, config, n_folds=5, cv_type='k_fold', shuffle=True, random_state=42, progress_callback=None, log_callback=None)</code>","text":"<p>Performs cross-validation on the training split.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def cross_validate(\n    self,\n    dataset: SplitDataset,\n    target_column: str,\n    config: Dict[str, Any],\n    n_folds: int = 5,\n    cv_type: str = \"k_fold\",\n    shuffle: bool = True,\n    random_state: int = 42,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Performs cross-validation on the training split.\n    \"\"\"\n    # Import here to avoid circular dependency if any\n    from .cross_validation import perform_cross_validation\n\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n    return perform_cross_validation(\n        calculator=self.calculator,\n        applier=self.applier,\n        X=X_train,\n        y=y_train,\n        config=config,\n        n_folds=n_folds,\n        cv_type=cv_type,\n        shuffle=shuffle,\n        random_state=random_state,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n    )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator.evaluate","title":"<code>evaluate(dataset, target_column, job_id='unknown')</code>","text":"<p>Evaluates the model on all splits and returns a detailed report.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def evaluate(  # noqa: C901\n    self, dataset: SplitDataset, target_column: str, job_id: str = \"unknown\"\n) -&gt; Any:\n    \"\"\"\n    Evaluates the model on all splits and returns a detailed report.\n    \"\"\"\n    # Import here to avoid circular dependency\n    from .evaluation.classification import evaluate_classification_model\n    from .evaluation.regression import evaluate_regression_model\n\n    if self.model is None:\n        raise ValueError(\n            \"Model has not been trained yet. Call fit_predict() first.\"\n        )\n\n    problem_type = self.calculator.problem_type\n\n    splits_payload = {}\n\n    # Container for raw predictions\n    evaluation_data = {\n        \"job_id\": job_id,\n        \"node_id\": self.node_id,\n        \"problem_type\": problem_type,\n        \"splits\": {},\n    }\n\n    # Helper to evaluate a single split\n    def evaluate_split(split_name: str, data: Any):\n        if isinstance(data, tuple):\n            X, y = data\n        elif isinstance(data, pd.DataFrame):\n            if target_column not in data.columns:\n                return None  # Cannot evaluate without target\n            X = data.drop(columns=[target_column])\n            y = data[target_column]\n        else:\n            return None\n\n        y_pred = self.applier.predict(X, self.model)\n\n        # Try to get probabilities for classification\n        y_proba = None\n        if problem_type == \"classification\":\n            y_proba_df = self.applier.predict_proba(X, self.model)\n            if y_proba_df is not None:\n                y_proba = {\n                    \"classes\": y_proba_df.columns.tolist(),\n                    \"values\": y_proba_df.values.tolist(),\n                }\n\n        split_data = {\n            \"y_true\": y.tolist() if hasattr(y, \"tolist\") else list(y),\n            \"y_pred\": (\n                y_pred.tolist() if hasattr(y_pred, \"tolist\") else list(y_pred)\n            ),\n        }\n\n        if y_proba:\n            split_data[\"y_proba\"] = y_proba\n\n        evaluation_data[\"splits\"][split_name] = split_data\n\n        # Unpack model if it's a tuple (from Tuner)\n        model_to_evaluate = self.model\n        if isinstance(self.model, tuple) and len(self.model) == 2:\n            # Check if first element looks like a model (has fit/predict)\n            # or if it's just a convention from TunerCalculator\n            model_to_evaluate = self.model[0]\n\n        if problem_type == \"classification\":\n            return evaluate_classification_model(\n                model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n            )\n        elif problem_type == \"regression\":\n            return evaluate_regression_model(\n                model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n            )\n        else:\n            raise ValueError(f\"Unknown problem type: {problem_type}\")\n\n    # 2. Evaluate Train\n    splits_payload[\"train\"] = evaluate_split(\"train\", dataset.train)\n\n    # 3. Evaluate Test\n    has_test = False\n    if isinstance(dataset.test, pd.DataFrame):\n        has_test = not dataset.test.empty\n    elif isinstance(dataset.test, tuple):\n        has_test = len(dataset.test) == 2 and len(dataset.test[0]) &gt; 0\n\n    if has_test:\n        splits_payload[\"test\"] = evaluate_split(\"test\", dataset.test)\n\n    # 4. Evaluate Validation\n    if dataset.validation is not None:\n        has_val = False\n        if isinstance(dataset.validation, pd.DataFrame):\n            has_val = not dataset.validation.empty\n        elif isinstance(dataset.validation, tuple):\n            has_val = (\n                len(dataset.validation) == 2 and len(dataset.validation[0]) &gt; 0\n            )\n\n        if has_val:\n            splits_payload[\"validation\"] = evaluate_split(\n                \"validation\", dataset.validation\n            )\n\n    # Return report object (simplified for now, assuming schema matches)\n    return {\n        \"problem_type\": problem_type,\n        \"splits\": splits_payload,\n        \"raw_data\": evaluation_data,\n    }\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator.fit_predict","title":"<code>fit_predict(dataset, target_column, config, progress_callback=None, log_callback=None, job_id='unknown')</code>","text":"<p>Fits the model on training data and returns predictions for all splits.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def fit_predict(\n    self,\n    dataset: Union[SplitDataset, pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n    target_column: str,\n    config: Dict[str, Any],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    job_id: str = \"unknown\",\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"\n    Fits the model on training data and returns predictions for all splits.\n    \"\"\"\n    # Handle raw DataFrame or Tuple input by wrapping it in a dummy SplitDataset\n    if isinstance(dataset, pd.DataFrame):\n        dataset = SplitDataset(train=dataset, test=pd.DataFrame(), validation=None)\n    elif isinstance(dataset, tuple):\n        # Check if it's (train_df, test_df) or (X, y)\n        elem0 = dataset[0]\n        if isinstance(elem0, pd.DataFrame) and target_column in elem0.columns:\n            # It's (train_df, test_df)\n            train_df, test_df = dataset\n            dataset = SplitDataset(train=train_df, test=test_df, validation=None)\n        else:\n            # It's (X, y) or something else, treat as train set\n            dataset = SplitDataset(\n                train=dataset, test=pd.DataFrame(), validation=None\n            )\n\n    # 1. Prepare Data\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n    validation_data = None\n    if dataset.validation is not None:\n        X_val, y_val = self._extract_xy(dataset.validation, target_column)\n        validation_data = (X_val, y_val)\n\n    # 2. Train Model\n    self.model = self.calculator.fit(\n        X_train,\n        y_train,\n        config,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n        validation_data=validation_data,\n    )\n\n    # 3. Predict on all splits\n    predictions = {}\n\n    # Train Predictions\n    predictions[\"train\"] = self.applier.predict(X_train, self.model)\n\n    # Test Predictions\n    is_test_empty = False\n    if isinstance(dataset.test, tuple):\n        is_test_empty = dataset.test[0].empty\n    else:\n        is_test_empty = dataset.test.empty\n\n    if not is_test_empty:\n        if isinstance(dataset.test, tuple):\n            X_test, _ = dataset.test\n        else:\n            if target_column in dataset.test.columns:\n                X_test = dataset.test.drop(columns=[target_column])\n            else:\n                X_test = dataset.test\n        predictions[\"test\"] = self.applier.predict(X_test, self.model)\n\n    # Validation Predictions\n    if dataset.validation is not None:\n        if isinstance(dataset.validation, tuple):\n            X_val, _ = dataset.validation\n        else:\n            if target_column in dataset.validation.columns:\n                X_val = dataset.validation.drop(columns=[target_column])\n            else:\n                X_val = dataset.validation\n        predictions[\"validation\"] = self.applier.predict(X_val, self.model)\n\n    return predictions\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator.refit","title":"<code>refit(dataset, target_column, config, job_id='unknown')</code>","text":"<p>Refits the model on Train + Validation data and updates the artifact.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def refit(\n    self,\n    dataset: SplitDataset,\n    target_column: str,\n    config: Dict[str, Any],\n    job_id: str = \"unknown\",\n) -&gt; None:\n    \"\"\"\n    Refits the model on Train + Validation data and updates the artifact.\n    \"\"\"\n    if dataset.validation is None:\n        # Fallback to normal fit if no validation set\n        self.fit_predict(dataset, target_column, config, job_id=job_id)\n        return\n\n    # 1. Prepare Combined Data\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n    X_val, y_val = self._extract_xy(dataset.validation, target_column)\n\n    X_combined = pd.concat([X_train, X_val], axis=0)\n    y_combined = pd.concat([y_train, y_val], axis=0)\n\n    # 2. Train Model\n    self.model = self.calculator.fit(X_combined, y_combined, config)\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.perform_cross_validation","title":"<code>perform_cross_validation(calculator, applier, X, y, config, n_folds=5, cv_type='k_fold', shuffle=True, random_state=42, progress_callback=None, log_callback=None)</code>","text":"<p>Performs K-Fold cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>calculator</code> <code>BaseModelCalculator</code> <p>The model calculator (fit logic).</p> required <code>applier</code> <code>BaseModelApplier</code> <p>The model applier (predict logic).</p> required <code>X</code> <code>DataFrame</code> <p>Features.</p> required <code>y</code> <code>Series</code> <p>Target.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>Model configuration.</p> required <code>n_folds</code> <code>int</code> <p>Number of folds.</p> <code>5</code> <code>cv_type</code> <code>str</code> <p>Type of CV.</p> <code>'k_fold'</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle data before splitting (for KFold/Stratified).</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Random seed for shuffling.</p> <code>42</code> <code>progress_callback</code> <code>Optional[Callable[[int, int], None]]</code> <p>Optional callback(current_fold, total_folds).</p> <code>None</code> <code>log_callback</code> <code>Optional[Callable[[str], None]]</code> <p>Optional callback for logging messages.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing aggregated metrics and per-fold details.</p> Source code in <code>skyulf-core/skyulf/modeling/cross_validation.py</code> <pre><code>def perform_cross_validation(\n    calculator: BaseModelCalculator,\n    applier: BaseModelApplier,\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: Dict[str, Any],\n    n_folds: int = 5,\n    cv_type: str = \"k_fold\",  # k_fold, stratified_k_fold, time_series_split, shuffle_split\n    shuffle: bool = True,\n    random_state: int = 42,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Performs K-Fold cross-validation.\n\n    Args:\n        calculator: The model calculator (fit logic).\n        applier: The model applier (predict logic).\n        X: Features.\n        y: Target.\n        config: Model configuration.\n        n_folds: Number of folds.\n        cv_type: Type of CV.\n        shuffle: Whether to shuffle data before splitting (for KFold/Stratified).\n        random_state: Random seed for shuffling.\n        progress_callback: Optional callback(current_fold, total_folds).\n        log_callback: Optional callback for logging messages.\n\n    Returns:\n        Dict containing aggregated metrics and per-fold details.\n    \"\"\"\n\n    problem_type = calculator.problem_type\n\n    if log_callback:\n        log_callback(f\"Starting Cross-Validation (Folds: {n_folds}, Type: {cv_type})\")\n\n    # 1. Setup Splitter\n    if cv_type == \"time_series_split\":\n        splitter = TimeSeriesSplit(n_splits=n_folds)\n    elif cv_type == \"shuffle_split\":\n        splitter = ShuffleSplit(\n            n_splits=n_folds, test_size=0.2, random_state=random_state\n        )\n    elif cv_type == \"stratified_k_fold\" and problem_type == \"classification\":\n        splitter = StratifiedKFold(\n            n_splits=n_folds,\n            shuffle=shuffle,\n            random_state=random_state if shuffle else None,\n        )\n    else:\n        # Default to KFold\n        splitter = KFold(\n            n_splits=n_folds,\n            shuffle=shuffle,\n            random_state=random_state if shuffle else None,\n        )\n\n    fold_results = []\n\n    # Ensure numpy for splitting\n    X_arr = X.to_numpy() if hasattr(X, \"to_numpy\") else X\n    y_arr = y.to_numpy() if hasattr(y, \"to_numpy\") else y\n\n    # 2. Iterate Folds\n    for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X_arr, y_arr)):\n        if progress_callback:\n            progress_callback(fold_idx + 1, n_folds)\n\n        if log_callback:\n            log_callback(f\"Processing Fold {fold_idx + 1}/{n_folds}...\")\n\n        # Split Data\n        X_train_fold = X.iloc[train_idx] if hasattr(X, \"iloc\") else X[train_idx]\n        y_train_fold = y.iloc[train_idx] if hasattr(y, \"iloc\") else y[train_idx]\n        X_val_fold = X.iloc[val_idx] if hasattr(X, \"iloc\") else X[val_idx]\n        y_val_fold = y.iloc[val_idx] if hasattr(y, \"iloc\") else y[val_idx]\n\n        # Fit\n        model_artifact = calculator.fit(X_train_fold, y_train_fold, config)\n\n        # Evaluate\n        if problem_type == \"classification\":\n            metrics = calculate_classification_metrics(\n                model_artifact, X_val_fold, y_val_fold\n            )\n        else:\n            metrics = calculate_regression_metrics(\n                model_artifact, X_val_fold, y_val_fold\n            )\n\n        if log_callback:\n            # Log a key metric for the fold\n            key_metric = \"accuracy\" if problem_type == \"classification\" else \"r2\"\n            score = metrics.get(key_metric, 0.0)\n            log_callback(f\"Fold {fold_idx + 1} completed. {key_metric}: {score:.4f}\")\n\n        fold_results.append(\n            {\n                \"fold\": fold_idx + 1,\n                \"metrics\": sanitize_metrics(metrics),\n                # We could store predictions here if needed, but might be too heavy\n            }\n        )\n\n    # 3. Aggregate\n    fold_metrics = [cast(Dict[str, float], r[\"metrics\"]) for r in fold_results]\n    aggregated = _aggregate_metrics(fold_metrics)\n\n    if log_callback:\n        log_callback(f\"Cross-Validation Completed. Aggregated Metrics: {aggregated}\")\n\n    return {\n        \"aggregated_metrics\": aggregated,\n        \"folds\": fold_results,\n        \"cv_config\": {\n            \"n_folds\": n_folds,\n            \"cv_type\": cv_type,\n            \"shuffle\": shuffle,\n            \"random_state\": random_state,\n        },\n    }\n</code></pre>"},{"location":"reference/api/modeling/base.html","title":"API: modeling.base","text":""},{"location":"reference/api/modeling/base.html#skyulf.modeling.base","title":"<code>skyulf.modeling.base</code>","text":""},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelApplier","title":"<code>BaseModelApplier</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class BaseModelApplier(ABC):\n    @abstractmethod\n    def predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n        \"\"\"\n        Generates predictions.\n        \"\"\"\n        pass\n\n    def predict_proba(\n        self, df: pd.DataFrame, model_artifact: Any\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"\n        Generates prediction probabilities if supported.\n        Returns DataFrame where columns are classes.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelApplier.predict","title":"<code>predict(df, model_artifact)</code>  <code>abstractmethod</code>","text":"<p>Generates predictions.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>@abstractmethod\ndef predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n    \"\"\"\n    Generates predictions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelApplier.predict_proba","title":"<code>predict_proba(df, model_artifact)</code>","text":"<p>Generates prediction probabilities if supported. Returns DataFrame where columns are classes.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def predict_proba(\n    self, df: pd.DataFrame, model_artifact: Any\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Generates prediction probabilities if supported.\n    Returns DataFrame where columns are classes.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelCalculator","title":"<code>BaseModelCalculator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class BaseModelCalculator(ABC):\n    @property\n    @abstractmethod\n    def problem_type(self) -&gt; str:\n        \"\"\"Returns 'classification' or 'regression'.\"\"\"\n        pass\n\n    @property\n    def default_params(self) -&gt; Dict[str, Any]:\n        \"\"\"Default hyperparameters for the model.\"\"\"\n        return {}\n\n    @abstractmethod\n    def fit(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        config: Dict[str, Any],\n        progress_callback: Optional[Callable[..., None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        validation_data: Optional[tuple[pd.DataFrame, pd.Series]] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Trains the model. Returns the model object (serializable).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelCalculator.default_params","title":"<code>default_params</code>  <code>property</code>","text":"<p>Default hyperparameters for the model.</p>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelCalculator.problem_type","title":"<code>problem_type</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns 'classification' or 'regression'.</p>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>  <code>abstractmethod</code>","text":"<p>Trains the model. Returns the model object (serializable).</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>@abstractmethod\ndef fit(\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: Dict[str, Any],\n    progress_callback: Optional[Callable[..., None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    validation_data: Optional[tuple[pd.DataFrame, pd.Series]] = None,\n) -&gt; Any:\n    \"\"\"\n    Trains the model. Returns the model object (serializable).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator","title":"<code>StatefulEstimator</code>","text":"Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class StatefulEstimator:\n    def __init__(\n        self, calculator: BaseModelCalculator, applier: BaseModelApplier, node_id: str\n    ):\n        self.calculator = calculator\n        self.applier = applier\n        self.node_id = node_id\n        self.model = None  # In-memory model storage\n\n    def _extract_xy(\n        self, data: Any, target_column: str\n    ) -&gt; tuple[pd.DataFrame, pd.Series]:\n        \"\"\"Helper to extract X and y from DataFrame or Tuple.\"\"\"\n        if isinstance(data, tuple) and len(data) == 2:\n            return data[0], data[1]\n        elif isinstance(data, pd.DataFrame):\n            if target_column not in data.columns:\n                raise ValueError(f\"Target column '{target_column}' not found in data\")\n            return data.drop(columns=[target_column]), data[target_column]\n        else:\n            raise ValueError(f\"Unexpected data type: {type(data)}\")\n\n    def cross_validate(\n        self,\n        dataset: SplitDataset,\n        target_column: str,\n        config: Dict[str, Any],\n        n_folds: int = 5,\n        cv_type: str = \"k_fold\",\n        shuffle: bool = True,\n        random_state: int = 42,\n        progress_callback: Optional[Callable[[int, int], None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Performs cross-validation on the training split.\n        \"\"\"\n        # Import here to avoid circular dependency if any\n        from .cross_validation import perform_cross_validation\n\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n        return perform_cross_validation(\n            calculator=self.calculator,\n            applier=self.applier,\n            X=X_train,\n            y=y_train,\n            config=config,\n            n_folds=n_folds,\n            cv_type=cv_type,\n            shuffle=shuffle,\n            random_state=random_state,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n        )\n\n    def fit_predict(\n        self,\n        dataset: Union[SplitDataset, pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        target_column: str,\n        config: Dict[str, Any],\n        progress_callback: Optional[Callable[[int, int], None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        job_id: str = \"unknown\",\n    ) -&gt; Dict[str, pd.Series]:\n        \"\"\"\n        Fits the model on training data and returns predictions for all splits.\n        \"\"\"\n        # Handle raw DataFrame or Tuple input by wrapping it in a dummy SplitDataset\n        if isinstance(dataset, pd.DataFrame):\n            dataset = SplitDataset(train=dataset, test=pd.DataFrame(), validation=None)\n        elif isinstance(dataset, tuple):\n            # Check if it's (train_df, test_df) or (X, y)\n            elem0 = dataset[0]\n            if isinstance(elem0, pd.DataFrame) and target_column in elem0.columns:\n                # It's (train_df, test_df)\n                train_df, test_df = dataset\n                dataset = SplitDataset(train=train_df, test=test_df, validation=None)\n            else:\n                # It's (X, y) or something else, treat as train set\n                dataset = SplitDataset(\n                    train=dataset, test=pd.DataFrame(), validation=None\n                )\n\n        # 1. Prepare Data\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n        validation_data = None\n        if dataset.validation is not None:\n            X_val, y_val = self._extract_xy(dataset.validation, target_column)\n            validation_data = (X_val, y_val)\n\n        # 2. Train Model\n        self.model = self.calculator.fit(\n            X_train,\n            y_train,\n            config,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n            validation_data=validation_data,\n        )\n\n        # 3. Predict on all splits\n        predictions = {}\n\n        # Train Predictions\n        predictions[\"train\"] = self.applier.predict(X_train, self.model)\n\n        # Test Predictions\n        is_test_empty = False\n        if isinstance(dataset.test, tuple):\n            is_test_empty = dataset.test[0].empty\n        else:\n            is_test_empty = dataset.test.empty\n\n        if not is_test_empty:\n            if isinstance(dataset.test, tuple):\n                X_test, _ = dataset.test\n            else:\n                if target_column in dataset.test.columns:\n                    X_test = dataset.test.drop(columns=[target_column])\n                else:\n                    X_test = dataset.test\n            predictions[\"test\"] = self.applier.predict(X_test, self.model)\n\n        # Validation Predictions\n        if dataset.validation is not None:\n            if isinstance(dataset.validation, tuple):\n                X_val, _ = dataset.validation\n            else:\n                if target_column in dataset.validation.columns:\n                    X_val = dataset.validation.drop(columns=[target_column])\n                else:\n                    X_val = dataset.validation\n            predictions[\"validation\"] = self.applier.predict(X_val, self.model)\n\n        return predictions\n\n    def refit(\n        self,\n        dataset: SplitDataset,\n        target_column: str,\n        config: Dict[str, Any],\n        job_id: str = \"unknown\",\n    ) -&gt; None:\n        \"\"\"\n        Refits the model on Train + Validation data and updates the artifact.\n        \"\"\"\n        if dataset.validation is None:\n            # Fallback to normal fit if no validation set\n            self.fit_predict(dataset, target_column, config, job_id=job_id)\n            return\n\n        # 1. Prepare Combined Data\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n        X_val, y_val = self._extract_xy(dataset.validation, target_column)\n\n        X_combined = pd.concat([X_train, X_val], axis=0)\n        y_combined = pd.concat([y_train, y_val], axis=0)\n\n        # 2. Train Model\n        self.model = self.calculator.fit(X_combined, y_combined, config)\n\n    def evaluate(  # noqa: C901\n        self, dataset: SplitDataset, target_column: str, job_id: str = \"unknown\"\n    ) -&gt; Any:\n        \"\"\"\n        Evaluates the model on all splits and returns a detailed report.\n        \"\"\"\n        # Import here to avoid circular dependency\n        from .evaluation.classification import evaluate_classification_model\n        from .evaluation.regression import evaluate_regression_model\n\n        if self.model is None:\n            raise ValueError(\n                \"Model has not been trained yet. Call fit_predict() first.\"\n            )\n\n        problem_type = self.calculator.problem_type\n\n        splits_payload = {}\n\n        # Container for raw predictions\n        evaluation_data = {\n            \"job_id\": job_id,\n            \"node_id\": self.node_id,\n            \"problem_type\": problem_type,\n            \"splits\": {},\n        }\n\n        # Helper to evaluate a single split\n        def evaluate_split(split_name: str, data: Any):\n            if isinstance(data, tuple):\n                X, y = data\n            elif isinstance(data, pd.DataFrame):\n                if target_column not in data.columns:\n                    return None  # Cannot evaluate without target\n                X = data.drop(columns=[target_column])\n                y = data[target_column]\n            else:\n                return None\n\n            y_pred = self.applier.predict(X, self.model)\n\n            # Try to get probabilities for classification\n            y_proba = None\n            if problem_type == \"classification\":\n                y_proba_df = self.applier.predict_proba(X, self.model)\n                if y_proba_df is not None:\n                    y_proba = {\n                        \"classes\": y_proba_df.columns.tolist(),\n                        \"values\": y_proba_df.values.tolist(),\n                    }\n\n            split_data = {\n                \"y_true\": y.tolist() if hasattr(y, \"tolist\") else list(y),\n                \"y_pred\": (\n                    y_pred.tolist() if hasattr(y_pred, \"tolist\") else list(y_pred)\n                ),\n            }\n\n            if y_proba:\n                split_data[\"y_proba\"] = y_proba\n\n            evaluation_data[\"splits\"][split_name] = split_data\n\n            # Unpack model if it's a tuple (from Tuner)\n            model_to_evaluate = self.model\n            if isinstance(self.model, tuple) and len(self.model) == 2:\n                # Check if first element looks like a model (has fit/predict)\n                # or if it's just a convention from TunerCalculator\n                model_to_evaluate = self.model[0]\n\n            if problem_type == \"classification\":\n                return evaluate_classification_model(\n                    model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n                )\n            elif problem_type == \"regression\":\n                return evaluate_regression_model(\n                    model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n                )\n            else:\n                raise ValueError(f\"Unknown problem type: {problem_type}\")\n\n        # 2. Evaluate Train\n        splits_payload[\"train\"] = evaluate_split(\"train\", dataset.train)\n\n        # 3. Evaluate Test\n        has_test = False\n        if isinstance(dataset.test, pd.DataFrame):\n            has_test = not dataset.test.empty\n        elif isinstance(dataset.test, tuple):\n            has_test = len(dataset.test) == 2 and len(dataset.test[0]) &gt; 0\n\n        if has_test:\n            splits_payload[\"test\"] = evaluate_split(\"test\", dataset.test)\n\n        # 4. Evaluate Validation\n        if dataset.validation is not None:\n            has_val = False\n            if isinstance(dataset.validation, pd.DataFrame):\n                has_val = not dataset.validation.empty\n            elif isinstance(dataset.validation, tuple):\n                has_val = (\n                    len(dataset.validation) == 2 and len(dataset.validation[0]) &gt; 0\n                )\n\n            if has_val:\n                splits_payload[\"validation\"] = evaluate_split(\n                    \"validation\", dataset.validation\n                )\n\n        # Return report object (simplified for now, assuming schema matches)\n        return {\n            \"problem_type\": problem_type,\n            \"splits\": splits_payload,\n            \"raw_data\": evaluation_data,\n        }\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator.cross_validate","title":"<code>cross_validate(dataset, target_column, config, n_folds=5, cv_type='k_fold', shuffle=True, random_state=42, progress_callback=None, log_callback=None)</code>","text":"<p>Performs cross-validation on the training split.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def cross_validate(\n    self,\n    dataset: SplitDataset,\n    target_column: str,\n    config: Dict[str, Any],\n    n_folds: int = 5,\n    cv_type: str = \"k_fold\",\n    shuffle: bool = True,\n    random_state: int = 42,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Performs cross-validation on the training split.\n    \"\"\"\n    # Import here to avoid circular dependency if any\n    from .cross_validation import perform_cross_validation\n\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n    return perform_cross_validation(\n        calculator=self.calculator,\n        applier=self.applier,\n        X=X_train,\n        y=y_train,\n        config=config,\n        n_folds=n_folds,\n        cv_type=cv_type,\n        shuffle=shuffle,\n        random_state=random_state,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n    )\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator.evaluate","title":"<code>evaluate(dataset, target_column, job_id='unknown')</code>","text":"<p>Evaluates the model on all splits and returns a detailed report.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def evaluate(  # noqa: C901\n    self, dataset: SplitDataset, target_column: str, job_id: str = \"unknown\"\n) -&gt; Any:\n    \"\"\"\n    Evaluates the model on all splits and returns a detailed report.\n    \"\"\"\n    # Import here to avoid circular dependency\n    from .evaluation.classification import evaluate_classification_model\n    from .evaluation.regression import evaluate_regression_model\n\n    if self.model is None:\n        raise ValueError(\n            \"Model has not been trained yet. Call fit_predict() first.\"\n        )\n\n    problem_type = self.calculator.problem_type\n\n    splits_payload = {}\n\n    # Container for raw predictions\n    evaluation_data = {\n        \"job_id\": job_id,\n        \"node_id\": self.node_id,\n        \"problem_type\": problem_type,\n        \"splits\": {},\n    }\n\n    # Helper to evaluate a single split\n    def evaluate_split(split_name: str, data: Any):\n        if isinstance(data, tuple):\n            X, y = data\n        elif isinstance(data, pd.DataFrame):\n            if target_column not in data.columns:\n                return None  # Cannot evaluate without target\n            X = data.drop(columns=[target_column])\n            y = data[target_column]\n        else:\n            return None\n\n        y_pred = self.applier.predict(X, self.model)\n\n        # Try to get probabilities for classification\n        y_proba = None\n        if problem_type == \"classification\":\n            y_proba_df = self.applier.predict_proba(X, self.model)\n            if y_proba_df is not None:\n                y_proba = {\n                    \"classes\": y_proba_df.columns.tolist(),\n                    \"values\": y_proba_df.values.tolist(),\n                }\n\n        split_data = {\n            \"y_true\": y.tolist() if hasattr(y, \"tolist\") else list(y),\n            \"y_pred\": (\n                y_pred.tolist() if hasattr(y_pred, \"tolist\") else list(y_pred)\n            ),\n        }\n\n        if y_proba:\n            split_data[\"y_proba\"] = y_proba\n\n        evaluation_data[\"splits\"][split_name] = split_data\n\n        # Unpack model if it's a tuple (from Tuner)\n        model_to_evaluate = self.model\n        if isinstance(self.model, tuple) and len(self.model) == 2:\n            # Check if first element looks like a model (has fit/predict)\n            # or if it's just a convention from TunerCalculator\n            model_to_evaluate = self.model[0]\n\n        if problem_type == \"classification\":\n            return evaluate_classification_model(\n                model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n            )\n        elif problem_type == \"regression\":\n            return evaluate_regression_model(\n                model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n            )\n        else:\n            raise ValueError(f\"Unknown problem type: {problem_type}\")\n\n    # 2. Evaluate Train\n    splits_payload[\"train\"] = evaluate_split(\"train\", dataset.train)\n\n    # 3. Evaluate Test\n    has_test = False\n    if isinstance(dataset.test, pd.DataFrame):\n        has_test = not dataset.test.empty\n    elif isinstance(dataset.test, tuple):\n        has_test = len(dataset.test) == 2 and len(dataset.test[0]) &gt; 0\n\n    if has_test:\n        splits_payload[\"test\"] = evaluate_split(\"test\", dataset.test)\n\n    # 4. Evaluate Validation\n    if dataset.validation is not None:\n        has_val = False\n        if isinstance(dataset.validation, pd.DataFrame):\n            has_val = not dataset.validation.empty\n        elif isinstance(dataset.validation, tuple):\n            has_val = (\n                len(dataset.validation) == 2 and len(dataset.validation[0]) &gt; 0\n            )\n\n        if has_val:\n            splits_payload[\"validation\"] = evaluate_split(\n                \"validation\", dataset.validation\n            )\n\n    # Return report object (simplified for now, assuming schema matches)\n    return {\n        \"problem_type\": problem_type,\n        \"splits\": splits_payload,\n        \"raw_data\": evaluation_data,\n    }\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator.fit_predict","title":"<code>fit_predict(dataset, target_column, config, progress_callback=None, log_callback=None, job_id='unknown')</code>","text":"<p>Fits the model on training data and returns predictions for all splits.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def fit_predict(\n    self,\n    dataset: Union[SplitDataset, pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n    target_column: str,\n    config: Dict[str, Any],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    job_id: str = \"unknown\",\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"\n    Fits the model on training data and returns predictions for all splits.\n    \"\"\"\n    # Handle raw DataFrame or Tuple input by wrapping it in a dummy SplitDataset\n    if isinstance(dataset, pd.DataFrame):\n        dataset = SplitDataset(train=dataset, test=pd.DataFrame(), validation=None)\n    elif isinstance(dataset, tuple):\n        # Check if it's (train_df, test_df) or (X, y)\n        elem0 = dataset[0]\n        if isinstance(elem0, pd.DataFrame) and target_column in elem0.columns:\n            # It's (train_df, test_df)\n            train_df, test_df = dataset\n            dataset = SplitDataset(train=train_df, test=test_df, validation=None)\n        else:\n            # It's (X, y) or something else, treat as train set\n            dataset = SplitDataset(\n                train=dataset, test=pd.DataFrame(), validation=None\n            )\n\n    # 1. Prepare Data\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n    validation_data = None\n    if dataset.validation is not None:\n        X_val, y_val = self._extract_xy(dataset.validation, target_column)\n        validation_data = (X_val, y_val)\n\n    # 2. Train Model\n    self.model = self.calculator.fit(\n        X_train,\n        y_train,\n        config,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n        validation_data=validation_data,\n    )\n\n    # 3. Predict on all splits\n    predictions = {}\n\n    # Train Predictions\n    predictions[\"train\"] = self.applier.predict(X_train, self.model)\n\n    # Test Predictions\n    is_test_empty = False\n    if isinstance(dataset.test, tuple):\n        is_test_empty = dataset.test[0].empty\n    else:\n        is_test_empty = dataset.test.empty\n\n    if not is_test_empty:\n        if isinstance(dataset.test, tuple):\n            X_test, _ = dataset.test\n        else:\n            if target_column in dataset.test.columns:\n                X_test = dataset.test.drop(columns=[target_column])\n            else:\n                X_test = dataset.test\n        predictions[\"test\"] = self.applier.predict(X_test, self.model)\n\n    # Validation Predictions\n    if dataset.validation is not None:\n        if isinstance(dataset.validation, tuple):\n            X_val, _ = dataset.validation\n        else:\n            if target_column in dataset.validation.columns:\n                X_val = dataset.validation.drop(columns=[target_column])\n            else:\n                X_val = dataset.validation\n        predictions[\"validation\"] = self.applier.predict(X_val, self.model)\n\n    return predictions\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator.refit","title":"<code>refit(dataset, target_column, config, job_id='unknown')</code>","text":"<p>Refits the model on Train + Validation data and updates the artifact.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def refit(\n    self,\n    dataset: SplitDataset,\n    target_column: str,\n    config: Dict[str, Any],\n    job_id: str = \"unknown\",\n) -&gt; None:\n    \"\"\"\n    Refits the model on Train + Validation data and updates the artifact.\n    \"\"\"\n    if dataset.validation is None:\n        # Fallback to normal fit if no validation set\n        self.fit_predict(dataset, target_column, config, job_id=job_id)\n        return\n\n    # 1. Prepare Combined Data\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n    X_val, y_val = self._extract_xy(dataset.validation, target_column)\n\n    X_combined = pd.concat([X_train, X_val], axis=0)\n    y_combined = pd.concat([y_train, y_val], axis=0)\n\n    # 2. Train Model\n    self.model = self.calculator.fit(X_combined, y_combined, config)\n</code></pre>"},{"location":"reference/api/modeling/classification.html","title":"API: modeling.classification","text":""},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification","title":"<code>skyulf.modeling.classification</code>","text":"<p>Classification models.</p>"},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification.LogisticRegressionApplier","title":"<code>LogisticRegressionApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Logistic Regression Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class LogisticRegressionApplier(SklearnApplier):\n    \"\"\"Logistic Regression Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification.LogisticRegressionCalculator","title":"<code>LogisticRegressionCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Logistic Regression Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class LogisticRegressionCalculator(SklearnCalculator):\n    \"\"\"Logistic Regression Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=LogisticRegression,\n            default_params={\n                \"max_iter\": 1000,\n                \"solver\": \"lbfgs\",\n                \"random_state\": 42,\n            },\n            problem_type=\"classification\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification.RandomForestClassifierApplier","title":"<code>RandomForestClassifierApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Random Forest Classifier Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class RandomForestClassifierApplier(SklearnApplier):\n    \"\"\"Random Forest Classifier Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification.RandomForestClassifierCalculator","title":"<code>RandomForestClassifierCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Random Forest Classifier Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class RandomForestClassifierCalculator(SklearnCalculator):\n    \"\"\"Random Forest Classifier Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=RandomForestClassifier,\n            default_params={\n                \"n_estimators\": 50,\n                \"max_depth\": 10,\n                \"min_samples_split\": 5,\n                \"min_samples_leaf\": 2,\n                \"n_jobs\": -1,\n                \"random_state\": 42,\n            },\n            problem_type=\"classification\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/cross_validation.html","title":"API: modeling.cross_validation","text":""},{"location":"reference/api/modeling/cross_validation.html#skyulf.modeling.cross_validation","title":"<code>skyulf.modeling.cross_validation</code>","text":"<p>Cross-validation logic for V2 modeling.</p>"},{"location":"reference/api/modeling/cross_validation.html#skyulf.modeling.cross_validation.perform_cross_validation","title":"<code>perform_cross_validation(calculator, applier, X, y, config, n_folds=5, cv_type='k_fold', shuffle=True, random_state=42, progress_callback=None, log_callback=None)</code>","text":"<p>Performs K-Fold cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>calculator</code> <code>BaseModelCalculator</code> <p>The model calculator (fit logic).</p> required <code>applier</code> <code>BaseModelApplier</code> <p>The model applier (predict logic).</p> required <code>X</code> <code>DataFrame</code> <p>Features.</p> required <code>y</code> <code>Series</code> <p>Target.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>Model configuration.</p> required <code>n_folds</code> <code>int</code> <p>Number of folds.</p> <code>5</code> <code>cv_type</code> <code>str</code> <p>Type of CV.</p> <code>'k_fold'</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle data before splitting (for KFold/Stratified).</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Random seed for shuffling.</p> <code>42</code> <code>progress_callback</code> <code>Optional[Callable[[int, int], None]]</code> <p>Optional callback(current_fold, total_folds).</p> <code>None</code> <code>log_callback</code> <code>Optional[Callable[[str], None]]</code> <p>Optional callback for logging messages.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing aggregated metrics and per-fold details.</p> Source code in <code>skyulf-core/skyulf/modeling/cross_validation.py</code> <pre><code>def perform_cross_validation(\n    calculator: BaseModelCalculator,\n    applier: BaseModelApplier,\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: Dict[str, Any],\n    n_folds: int = 5,\n    cv_type: str = \"k_fold\",  # k_fold, stratified_k_fold, time_series_split, shuffle_split\n    shuffle: bool = True,\n    random_state: int = 42,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Performs K-Fold cross-validation.\n\n    Args:\n        calculator: The model calculator (fit logic).\n        applier: The model applier (predict logic).\n        X: Features.\n        y: Target.\n        config: Model configuration.\n        n_folds: Number of folds.\n        cv_type: Type of CV.\n        shuffle: Whether to shuffle data before splitting (for KFold/Stratified).\n        random_state: Random seed for shuffling.\n        progress_callback: Optional callback(current_fold, total_folds).\n        log_callback: Optional callback for logging messages.\n\n    Returns:\n        Dict containing aggregated metrics and per-fold details.\n    \"\"\"\n\n    problem_type = calculator.problem_type\n\n    if log_callback:\n        log_callback(f\"Starting Cross-Validation (Folds: {n_folds}, Type: {cv_type})\")\n\n    # 1. Setup Splitter\n    if cv_type == \"time_series_split\":\n        splitter = TimeSeriesSplit(n_splits=n_folds)\n    elif cv_type == \"shuffle_split\":\n        splitter = ShuffleSplit(\n            n_splits=n_folds, test_size=0.2, random_state=random_state\n        )\n    elif cv_type == \"stratified_k_fold\" and problem_type == \"classification\":\n        splitter = StratifiedKFold(\n            n_splits=n_folds,\n            shuffle=shuffle,\n            random_state=random_state if shuffle else None,\n        )\n    else:\n        # Default to KFold\n        splitter = KFold(\n            n_splits=n_folds,\n            shuffle=shuffle,\n            random_state=random_state if shuffle else None,\n        )\n\n    fold_results = []\n\n    # Ensure numpy for splitting\n    X_arr = X.to_numpy() if hasattr(X, \"to_numpy\") else X\n    y_arr = y.to_numpy() if hasattr(y, \"to_numpy\") else y\n\n    # 2. Iterate Folds\n    for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X_arr, y_arr)):\n        if progress_callback:\n            progress_callback(fold_idx + 1, n_folds)\n\n        if log_callback:\n            log_callback(f\"Processing Fold {fold_idx + 1}/{n_folds}...\")\n\n        # Split Data\n        X_train_fold = X.iloc[train_idx] if hasattr(X, \"iloc\") else X[train_idx]\n        y_train_fold = y.iloc[train_idx] if hasattr(y, \"iloc\") else y[train_idx]\n        X_val_fold = X.iloc[val_idx] if hasattr(X, \"iloc\") else X[val_idx]\n        y_val_fold = y.iloc[val_idx] if hasattr(y, \"iloc\") else y[val_idx]\n\n        # Fit\n        model_artifact = calculator.fit(X_train_fold, y_train_fold, config)\n\n        # Evaluate\n        if problem_type == \"classification\":\n            metrics = calculate_classification_metrics(\n                model_artifact, X_val_fold, y_val_fold\n            )\n        else:\n            metrics = calculate_regression_metrics(\n                model_artifact, X_val_fold, y_val_fold\n            )\n\n        if log_callback:\n            # Log a key metric for the fold\n            key_metric = \"accuracy\" if problem_type == \"classification\" else \"r2\"\n            score = metrics.get(key_metric, 0.0)\n            log_callback(f\"Fold {fold_idx + 1} completed. {key_metric}: {score:.4f}\")\n\n        fold_results.append(\n            {\n                \"fold\": fold_idx + 1,\n                \"metrics\": sanitize_metrics(metrics),\n                # We could store predictions here if needed, but might be too heavy\n            }\n        )\n\n    # 3. Aggregate\n    fold_metrics = [cast(Dict[str, float], r[\"metrics\"]) for r in fold_results]\n    aggregated = _aggregate_metrics(fold_metrics)\n\n    if log_callback:\n        log_callback(f\"Cross-Validation Completed. Aggregated Metrics: {aggregated}\")\n\n    return {\n        \"aggregated_metrics\": aggregated,\n        \"folds\": fold_results,\n        \"cv_config\": {\n            \"n_folds\": n_folds,\n            \"cv_type\": cv_type,\n            \"shuffle\": shuffle,\n            \"random_state\": random_state,\n        },\n    }\n</code></pre>"},{"location":"reference/api/modeling/regression.html","title":"API: modeling.regression","text":""},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression","title":"<code>skyulf.modeling.regression</code>","text":"<p>Regression models.</p>"},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression.RandomForestRegressorApplier","title":"<code>RandomForestRegressorApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Random Forest Regressor Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RandomForestRegressorApplier(SklearnApplier):\n    \"\"\"Random Forest Regressor Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression.RandomForestRegressorCalculator","title":"<code>RandomForestRegressorCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Random Forest Regressor Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RandomForestRegressorCalculator(SklearnCalculator):\n    \"\"\"Random Forest Regressor Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=RandomForestRegressor,\n            default_params={\n                \"n_estimators\": 50,\n                \"max_depth\": 10,\n                \"min_samples_split\": 5,\n                \"min_samples_leaf\": 2,\n                \"n_jobs\": -1,\n                \"random_state\": 42,\n            },\n            problem_type=\"regression\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression.RidgeRegressionApplier","title":"<code>RidgeRegressionApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Ridge Regression Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RidgeRegressionApplier(SklearnApplier):\n    \"\"\"Ridge Regression Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression.RidgeRegressionCalculator","title":"<code>RidgeRegressionCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Ridge Regression Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RidgeRegressionCalculator(SklearnCalculator):\n    \"\"\"Ridge Regression Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=Ridge,\n            default_params={\n                \"alpha\": 1.0,\n                \"solver\": \"auto\",\n                \"random_state\": 42,\n            },\n            problem_type=\"regression\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/sklearn_wrapper.html","title":"API: modeling.sklearn_wrapper","text":""},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper","title":"<code>skyulf.modeling.sklearn_wrapper</code>","text":"<p>Wrapper for Scikit-Learn models.</p>"},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper.SklearnApplier","title":"<code>SklearnApplier</code>","text":"<p>               Bases: <code>BaseModelApplier</code></p> <p>Base applier for Scikit-Learn models.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>class SklearnApplier(BaseModelApplier):\n    \"\"\"Base applier for Scikit-Learn models.\"\"\"\n\n    def predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n        \"\"\"Generate predictions.\"\"\"\n        # model_artifact is the fitted sklearn estimator\n        return pd.Series(model_artifact.predict(df), index=df.index)\n\n    def predict_proba(\n        self, df: pd.DataFrame, model_artifact: Any\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Generate prediction probabilities.\"\"\"\n        if hasattr(model_artifact, \"predict_proba\"):\n            try:\n                probas = model_artifact.predict_proba(df)\n                # Handle binary vs multiclass\n                # If binary, classes_ usually has 2 entries.\n                classes = getattr(model_artifact, \"classes_\", None)\n                if classes is None:\n                    # Fallback if classes_ is missing (unlikely for sklearn classifiers)\n                    return pd.DataFrame(probas, index=df.index)\n\n                return pd.DataFrame(probas, columns=classes, index=df.index)\n            except Exception:\n                return None\n        return None\n</code></pre>"},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper.SklearnApplier.predict","title":"<code>predict(df, model_artifact)</code>","text":"<p>Generate predictions.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>def predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n    \"\"\"Generate predictions.\"\"\"\n    # model_artifact is the fitted sklearn estimator\n    return pd.Series(model_artifact.predict(df), index=df.index)\n</code></pre>"},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper.SklearnApplier.predict_proba","title":"<code>predict_proba(df, model_artifact)</code>","text":"<p>Generate prediction probabilities.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>def predict_proba(\n    self, df: pd.DataFrame, model_artifact: Any\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Generate prediction probabilities.\"\"\"\n    if hasattr(model_artifact, \"predict_proba\"):\n        try:\n            probas = model_artifact.predict_proba(df)\n            # Handle binary vs multiclass\n            # If binary, classes_ usually has 2 entries.\n            classes = getattr(model_artifact, \"classes_\", None)\n            if classes is None:\n                # Fallback if classes_ is missing (unlikely for sklearn classifiers)\n                return pd.DataFrame(probas, index=df.index)\n\n            return pd.DataFrame(probas, columns=classes, index=df.index)\n        except Exception:\n            return None\n    return None\n</code></pre>"},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper.SklearnCalculator","title":"<code>SklearnCalculator</code>","text":"<p>               Bases: <code>BaseModelCalculator</code></p> <p>Base calculator for Scikit-Learn models.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>class SklearnCalculator(BaseModelCalculator):\n    \"\"\"Base calculator for Scikit-Learn models.\"\"\"\n\n    def __init__(\n        self,\n        model_class: Type[BaseEstimator],\n        default_params: Dict[str, Any],\n        problem_type: str,\n    ):\n        self.model_class = model_class\n        self._default_params = default_params\n        self._problem_type = problem_type\n\n    @property\n    def default_params(self) -&gt; Dict[str, Any]:\n        return self._default_params\n\n    @property\n    def problem_type(self) -&gt; str:\n        return self._problem_type\n\n    def fit(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        config: Dict[str, Any],\n        progress_callback=None,\n        log_callback=None,\n        validation_data=None,\n    ) -&gt; Any:\n        \"\"\"Fit the Scikit-Learn model.\"\"\"\n        # 1. Merge Config with Defaults\n        params = self.default_params.copy()\n        if config:\n            # We support two configuration structures:\n            # 1. Nested: {'params': {'C': 1.0, ...}} - Preferred\n            # 2. Flat: {'C': 1.0, 'type': '...', ...} - Legacy/Simple support\n\n            # Check for explicit 'params' dictionary first\n            overrides = config.get(\"params\", {})\n\n            # If 'params' key exists but is None or empty, check if there are other keys at top level\n            # that might be params. But be careful not to mix them.\n            # If config has 'params', we assume it's the source of truth.\n\n            if not overrides and \"params\" not in config:\n                # Fallback to flat config if 'params' key is completely missing\n                reserved_keys = {\n                    \"type\",\n                    \"target_column\",\n                    \"node_id\",\n                    \"step_type\",\n                    \"inputs\",\n                }\n                overrides = {\n                    k: v\n                    for k, v in config.items()\n                    if k not in reserved_keys and not isinstance(v, dict)\n                }\n\n            if overrides:\n                params.update(overrides)\n\n        msg = f\"Initializing {self.model_class.__name__} with params: {params}\"\n        logger.info(msg)\n        if log_callback:\n            log_callback(msg)\n\n        # 2. Instantiate Model\n        # Filter params to only those accepted by the model class\n        # This prevents errors if extra config is passed\n        # (Though sklearn usually ignores extra kwargs in __init__ if **kwargs is present,\n        # strict models might not)\n        # For now, we assume the user/config provides valid params.\n\n        # Handle special cases like 'random_state' if needed, but usually passed directly.\n\n        model = self.model_class(**params)\n\n        # 3. Fit\n        model.fit(X, y)\n\n        return model\n</code></pre>"},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper.SklearnCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>","text":"<p>Fit the Scikit-Learn model.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>def fit(\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: Dict[str, Any],\n    progress_callback=None,\n    log_callback=None,\n    validation_data=None,\n) -&gt; Any:\n    \"\"\"Fit the Scikit-Learn model.\"\"\"\n    # 1. Merge Config with Defaults\n    params = self.default_params.copy()\n    if config:\n        # We support two configuration structures:\n        # 1. Nested: {'params': {'C': 1.0, ...}} - Preferred\n        # 2. Flat: {'C': 1.0, 'type': '...', ...} - Legacy/Simple support\n\n        # Check for explicit 'params' dictionary first\n        overrides = config.get(\"params\", {})\n\n        # If 'params' key exists but is None or empty, check if there are other keys at top level\n        # that might be params. But be careful not to mix them.\n        # If config has 'params', we assume it's the source of truth.\n\n        if not overrides and \"params\" not in config:\n            # Fallback to flat config if 'params' key is completely missing\n            reserved_keys = {\n                \"type\",\n                \"target_column\",\n                \"node_id\",\n                \"step_type\",\n                \"inputs\",\n            }\n            overrides = {\n                k: v\n                for k, v in config.items()\n                if k not in reserved_keys and not isinstance(v, dict)\n            }\n\n        if overrides:\n            params.update(overrides)\n\n    msg = f\"Initializing {self.model_class.__name__} with params: {params}\"\n    logger.info(msg)\n    if log_callback:\n        log_callback(msg)\n\n    # 2. Instantiate Model\n    # Filter params to only those accepted by the model class\n    # This prevents errors if extra config is passed\n    # (Though sklearn usually ignores extra kwargs in __init__ if **kwargs is present,\n    # strict models might not)\n    # For now, we assume the user/config provides valid params.\n\n    # Handle special cases like 'random_state' if needed, but usually passed directly.\n\n    model = self.model_class(**params)\n\n    # 3. Fit\n    model.fit(X, y)\n\n    return model\n</code></pre>"},{"location":"reference/api/modeling/tuning_tuner.html","title":"API: modeling.tuning.tuner","text":""},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner","title":"<code>skyulf.modeling.tuning.tuner</code>","text":"<p>Hyperparameter Tuner implementation.</p>"},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner.TunerApplier","title":"<code>TunerApplier</code>","text":"<p>               Bases: <code>BaseModelApplier</code></p> <p>Applier for TunerCalculator. Wraps the base model applier to provide predictions using the refitted best model.</p> Source code in <code>skyulf-core/skyulf/modeling/tuning/tuner.py</code> <pre><code>class TunerApplier(BaseModelApplier):\n    \"\"\"\n    Applier for TunerCalculator.\n    Wraps the base model applier to provide predictions using the refitted best model.\n    \"\"\"\n\n    def __init__(self, base_applier: BaseModelApplier):\n        self.base_applier = base_applier\n\n    def predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n        # model_artifact is (fitted_model, tuning_result)\n        if isinstance(model_artifact, tuple) and len(model_artifact) == 2:\n            model, _ = model_artifact\n            return self.base_applier.predict(df, model)\n        # Fallback if artifact is just the result (legacy)\n        return pd.Series(np.nan, index=df.index)\n\n    def predict_proba(\n        self, df: pd.DataFrame, model_artifact: Any\n    ) -&gt; Optional[pd.DataFrame]:\n        if isinstance(model_artifact, tuple) and len(model_artifact) == 2:\n            model, _ = model_artifact\n            return self.base_applier.predict_proba(df, model)\n        return None\n</code></pre>"},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner.TunerCalculator","title":"<code>TunerCalculator</code>","text":"<p>               Bases: <code>BaseModelCalculator</code></p> <p>Calculator for hyperparameter tuning.</p> Source code in <code>skyulf-core/skyulf/modeling/tuning/tuner.py</code> <pre><code>class TunerCalculator(BaseModelCalculator):\n    \"\"\"Calculator for hyperparameter tuning.\"\"\"\n\n    def __init__(self, model_calculator: BaseModelCalculator):\n        self.model_calculator = model_calculator\n\n    @property\n    def problem_type(self) -&gt; str:\n        return self.model_calculator.problem_type\n\n    def _clean_search_space(self, search_space: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Recursively cleans the search space.\n        - Converts \"none\" string to None.\n        \"\"\"\n        cleaned: Dict[str, Any] = {}\n        for k, v in search_space.items():\n            if isinstance(v, list):\n                cleaned[k] = [None if x == \"none\" else x for x in v]\n            elif isinstance(v, dict):\n                cleaned[k] = self._clean_search_space(v)\n            else:\n                cleaned[k] = None if v == \"none\" else v\n        return cleaned\n\n    def fit(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        config: Dict[str, Any],\n        progress_callback: Optional[\n            Callable[[int, int, Optional[float], Optional[Dict]], None]\n        ] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        validation_data: Optional[tuple[pd.DataFrame, pd.Series]] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Fits the tuner (runs tuning).\n        Adapts the generic fit interface to the specific tune method.\n        \"\"\"\n        # Convert config dict to TuningConfig\n        if isinstance(config, TuningConfig):\n            tuning_config = config\n        else:\n            # Extract valid keys for TuningConfig\n            valid_keys = TuningConfig.__annotations__.keys()\n            filtered_config = {k: v for k, v in config.items() if k in valid_keys}\n            tuning_config = TuningConfig(**filtered_config)  # type: ignore\n\n        tuning_result = self.tune(\n            X,\n            y,\n            tuning_config,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n            validation_data=validation_data,\n        )\n\n        # Refit the best model on the full dataset\n        best_params = tuning_result.best_params\n        final_params = {**self.model_calculator.default_params, **best_params}\n\n        # Ensure random_state is passed if available in config and not in params\n        if \"random_state\" not in final_params and hasattr(\n            tuning_config, \"random_state\"\n        ):\n            final_params[\"random_state\"] = tuning_config.random_state\n\n        if log_callback:\n            log_callback(f\"Refitting best model with params: {final_params}\")\n\n        model = self.model_calculator.model_class(**final_params)\n        model.fit(X, y)\n\n        return (model, tuning_result)\n\n    def tune(  # noqa: C901\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        config: TuningConfig,\n        progress_callback: Optional[\n            Callable[[int, int, Optional[float], Optional[Dict]], None]\n        ] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        validation_data: Optional[tuple[pd.DataFrame, pd.Series]] = None,\n    ) -&gt; TuningResult:\n        \"\"\"\n        Runs hyperparameter tuning.\n        \"\"\"\n        # 1. Prepare Estimator\n        # We need a base estimator. Since our Calculator wraps the class,\n        # we need to instantiate the underlying sklearn model with default params.\n        # Assuming model_calculator is SklearnCalculator\n        if not hasattr(self.model_calculator, \"model_class\"):\n            raise ValueError(\"Tuner currently only supports SklearnCalculator\")\n\n        base_estimator = self.model_calculator.model_class(\n            **self.model_calculator.default_params\n        )\n\n        # 2. Prepare Splitter\n        # If validation data is provided, use PredefinedSplit to train on X and validate on validation_data\n        # Otherwise use CV\n\n        X_for_search = X\n        y_for_search = y\n\n        if validation_data is not None:\n            from sklearn.model_selection import PredefinedSplit\n\n            X_val, y_val = validation_data\n\n            # Concatenate Train and Val\n            X_for_search = pd.concat([X, X_val], axis=0)\n            y_for_search = pd.concat([y, y_val], axis=0)\n\n            # Create test_fold array: -1 for train, 0 for val\n            # -1 means \"never in test set\" (so always in training set)\n            # 0 means \"in test set for fold 0\"\n            test_fold = np.concatenate([np.full(len(X), -1), np.full(len(X_val), 0)])\n\n            cv = PredefinedSplit(test_fold)\n        else:\n            if not config.cv_enabled:\n                # Single split validation (20% holdout)\n                cv = ShuffleSplit(\n                    n_splits=1, test_size=0.2, random_state=config.random_state\n                )\n            elif config.cv_type == \"time_series_split\":\n                cv = TimeSeriesSplit(n_splits=config.cv_folds)\n            elif config.cv_type == \"shuffle_split\":\n                cv = ShuffleSplit(\n                    n_splits=config.cv_folds,\n                    test_size=0.2,\n                    random_state=config.random_state,\n                )\n            elif (\n                config.cv_type == \"stratified_k_fold\"\n                and self.model_calculator.problem_type == \"classification\"\n            ):\n                cv = StratifiedKFold(\n                    n_splits=config.cv_folds,\n                    shuffle=True,\n                    random_state=config.random_state,\n                )\n            else:\n                # Default to KFold (also fallback for stratified if regression)\n                cv = KFold(\n                    n_splits=config.cv_folds,\n                    shuffle=True,\n                    random_state=config.random_state,\n                )\n\n        # 3. Select Search Strategy\n        searcher = None\n\n        # Handle multiclass metrics and map user-friendly names\n        metric = config.metric\n\n        # Map common user-friendly metrics to sklearn scoring strings\n        metric_map = {\n            \"mse\": \"neg_mean_squared_error\",\n            \"mae\": \"neg_mean_absolute_error\",\n            \"rmse\": \"neg_root_mean_squared_error\",\n            \"r2\": \"r2\",\n            \"accuracy\": \"accuracy\",\n            \"f1\": \"f1\",\n            \"precision\": \"precision\",\n            \"recall\": \"recall\",\n            \"roc_auc\": \"roc_auc\",\n        }\n\n        if metric in metric_map:\n            metric = metric_map[metric]\n\n        if self.model_calculator.problem_type == \"classification\":\n            # Check if target is multiclass\n            is_multiclass = False\n            if isinstance(y, pd.Series):\n                is_multiclass = y.nunique() &gt; 2\n            elif isinstance(y, np.ndarray):\n                is_multiclass = len(np.unique(y)) &gt; 2\n\n            # If multiclass and metric is binary-default, switch to weighted\n            # Note: We check against the mapped names now (e.g. \"f1\", \"precision\")\n            if is_multiclass and metric in [\"f1\", \"precision\", \"recall\", \"roc_auc\"]:\n                metric = f\"{metric}_weighted\"\n                # roc_auc needs special handling (ovr/ovo) usually, but weighted often works for simple cases\n                if (\n                    config.metric == \"roc_auc\"\n                ):  # Check original config metric name just in case\n                    metric = \"roc_auc_ovr_weighted\"\n\n        if config.strategy in [\"grid\", \"random\"]:\n            # Use custom loop to support progress and log callbacks\n            if log_callback:\n                log_callback(\n                    f\"Starting {config.strategy} search with custom loop for detailed logging...\"\n                )\n\n            # 1. Generate Candidates\n            param_space = self._clean_search_space(config.search_space)\n            candidates = []\n\n            if config.strategy == \"grid\":\n                candidates = list(ParameterGrid(param_space))\n            else:\n                # Random Search\n                candidates = list(\n                    ParameterSampler(\n                        param_space,\n                        n_iter=config.n_trials,\n                        random_state=config.random_state,\n                    )\n                )\n\n            total_candidates = len(candidates)\n            if log_callback:\n                log_callback(f\"Total candidates to evaluate: {total_candidates}\")\n\n            trials: List[Dict[str, Any]] = []\n            best_score = -float(\"inf\")\n            best_params = None\n\n            # 2. Iterate Candidates\n            for i, params in enumerate(candidates):\n                if log_callback:\n                    log_callback(\n                        f\"Evaluating Candidate {i + 1}/{total_candidates}: {params}\"\n                    )\n\n                # Use custom cross-validation loop to enable per-fold logging and progress tracking.\n                # We instantiate the model with the current candidate parameters and evaluate it\n                # using the configured CV strategy.\n\n                fold_scores = []\n\n                # Ensure numpy\n                X_arr = (\n                    X_for_search.to_numpy()\n                    if hasattr(X_for_search, \"to_numpy\")\n                    else X_for_search\n                )\n                y_arr = (\n                    y_for_search.to_numpy()\n                    if hasattr(y_for_search, \"to_numpy\")\n                    else y_for_search\n                )\n\n                for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_arr, y_arr)):\n                    # Split\n                    X_train_fold = (\n                        X_for_search.iloc[train_idx]\n                        if hasattr(X_for_search, \"iloc\")\n                        else X_for_search[train_idx]\n                    )\n                    y_train_fold = (\n                        y_for_search.iloc[train_idx]\n                        if hasattr(y_for_search, \"iloc\")\n                        else y_for_search[train_idx]\n                    )\n                    X_val_fold = (\n                        X_for_search.iloc[val_idx]\n                        if hasattr(X_for_search, \"iloc\")\n                        else X_for_search[val_idx]\n                    )\n                    y_val_fold = (\n                        y_for_search.iloc[val_idx]\n                        if hasattr(y_for_search, \"iloc\")\n                        else y_for_search[val_idx]\n                    )\n\n                    # Instantiate and Fit\n                    # Note: We must handle potential errors (e.g. incompatible params)\n                    try:\n                        model = self.model_calculator.model_class(\n                            **{**self.model_calculator.default_params, **params}\n                        )\n                        model.fit(X_train_fold, y_train_fold)\n\n                        # Score\n                        from sklearn.metrics import get_scorer\n\n                        scorer = get_scorer(metric)\n                        score = scorer(model, X_val_fold, y_val_fold)\n                        fold_scores.append(score)\n\n                        if log_callback:\n                            n_splits = cv.get_n_splits(X_arr, y_arr)\n                            log_callback(\n                                f\"  [Candidate {i + 1}] CV Fold {fold_idx + 1}/{n_splits} Score: {score:.4f}\"\n                            )\n                    except Exception as e:\n                        if log_callback:\n                            n_splits = cv.get_n_splits(X_arr, y_arr)\n                            log_callback(\n                                f\"  [Candidate {i + 1}] CV Fold {fold_idx + 1}/{n_splits} Failed: {str(e)}\"\n                            )\n                        fold_scores.append(-float(\"inf\"))\n\n                # Filter out failed folds for mean calculation if possible, or penalize\n                valid_scores = [s for s in fold_scores if s != -float(\"inf\")]\n                if valid_scores:\n                    mean_score = np.mean(valid_scores)\n                else:\n                    mean_score = -float(\"inf\")\n\n                if log_callback:\n                    log_callback(f\"Candidate {i + 1} Mean Score: {mean_score:.4f}\")\n\n                if progress_callback:\n                    progress_callback(i + 1, total_candidates, mean_score, params)\n\n                trials.append({\"params\": params, \"score\": mean_score})\n\n                if mean_score &gt; best_score:\n                    best_score = mean_score\n                    best_params = params\n\n            return TuningResult(\n                best_params=best_params if best_params is not None else {},\n                best_score=best_score,\n                n_trials=total_candidates,\n                trials=trials,\n            )\n\n        elif config.strategy == \"halving_grid\":\n            searcher = HalvingGridSearchCV(\n                estimator=base_estimator,\n                param_grid=self._clean_search_space(config.search_space),\n                scoring=metric,\n                cv=cv,\n                n_jobs=-1,\n                random_state=config.random_state,\n                refit=False,\n                error_score=np.nan,\n            )\n        elif config.strategy == \"halving_random\":\n            searcher = HalvingRandomSearchCV(\n                estimator=base_estimator,\n                param_distributions=self._clean_search_space(config.search_space),\n                n_candidates=config.n_trials,  # Map n_trials to n_candidates\n                scoring=metric,\n                cv=cv,\n                n_jobs=-1,\n                random_state=config.random_state,\n                refit=False,\n                error_score=np.nan,\n            )\n        elif config.strategy == \"optuna\":\n            if not HAS_OPTUNA:\n                raise ImportError(\n                    \"Optuna is not installed. Please install 'optuna' and 'optuna-integration'.\"\n                )\n\n            # Convert search space to Optuna distributions\n            distributions = {}\n            for k, v in config.search_space.items():\n                if isinstance(v, list):\n                    distributions[k] = optuna.distributions.CategoricalDistribution(v)\n                else:\n                    distributions[k] = v\n\n            # Optuna callbacks\n            callbacks = []\n            if progress_callback:\n\n                def _optuna_callback(study, trial):\n                    # Optuna doesn't know total trials upfront easily if not set, but we have config.n_trials\n                    # trial.value is the score (or None if failed/pruned)\n                    score = trial.value if trial.value is not None else None\n\n                    if log_callback:\n                        log_callback(\n                            f\"Optuna Trial {trial.number + 1} finished. Mean CV Score: {score}\"\n                        )\n\n                    progress_callback(\n                        trial.number + 1, config.n_trials, score, trial.params\n                    )\n\n                callbacks.append(_optuna_callback)\n\n            searcher = OptunaSearchCV(\n                estimator=base_estimator,\n                param_distributions=distributions,\n                n_trials=config.n_trials,\n                timeout=config.timeout,\n                cv=cv,  # type: ignore\n                scoring=metric,\n                n_jobs=-1,\n                random_state=config.random_state,\n                refit=False,\n                verbose=0,\n                callbacks=callbacks,\n            )\n        else:\n            raise ValueError(f\"Unknown tuning strategy: {config.strategy}\")\n\n        # 4. Run Search\n        # Ensure numpy\n        X_arr = (\n            X_for_search.to_numpy()\n            if hasattr(X_for_search, \"to_numpy\")\n            else X_for_search\n        )\n        y_arr = (\n            y_for_search.to_numpy()\n            if hasattr(y_for_search, \"to_numpy\")\n            else y_for_search\n        )\n\n        try:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\",\n                    message=\"Failed to report cross validation scores for TerminatorCallback\",\n                )\n                searcher.fit(X_arr, y_arr)\n        except Exception as e:\n            logger.error(f\"Hyperparameter tuning failed: {str(e)}\")\n            error_msg = str(e)\n            if \"No trials are completed yet\" in error_msg:\n                raise ValueError(\n                    \"Hyperparameter tuning failed: No trials completed successfully. \"\n                    \"This usually means the model failed to train with the provided hyperparameter combinations. \"\n                    \"Please check your search space and data.\"\n                ) from e\n\n            if (\n                \"n_samples\" in error_msg\n                and \"resample\" in error_msg\n                and \"Got 0\" in error_msg\n            ):\n                raise ValueError(\n                    \"Hyperparameter tuning with Halving strategy failed because the dataset is too small \"\n                    \"for the configured halving parameters. Please try using 'Random Search' or 'Grid Search' instead, \"\n                    \"or increase your dataset size.\"\n                ) from e\n\n            raise e\n\n        # 5. Extract Results\n        if not hasattr(searcher, \"best_params_\"):\n            raise ValueError(\n                \"Hyperparameter tuning failed to find any valid combination of parameters. All trials likely failed.\"\n            )\n\n        best_params = searcher.best_params_\n        best_score = searcher.best_score_\n\n        # Collect trials\n        trials = []\n        # Special handling for Optuna\n        if config.strategy == \"optuna\" and hasattr(searcher, \"study_\"):\n            for trial in searcher.study_.trials:\n                # Only include completed trials\n                if trial.state.name == \"COMPLETE\":\n                    trials.append({\"params\": trial.params, \"score\": trial.value})\n        elif hasattr(searcher, \"cv_results_\"):\n            results = searcher.cv_results_\n            if \"params\" in results:\n                n_candidates = len(results[\"params\"])\n                for i in range(n_candidates):\n                    trials.append(\n                        {\n                            \"params\": results[\"params\"][i],\n                            \"score\": results[\"mean_test_score\"][i],\n                        }\n                    )\n\n        return TuningResult(\n            best_params=best_params,\n            best_score=best_score,\n            n_trials=len(trials),\n            trials=trials,\n        )\n</code></pre>"},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner.TunerCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>","text":"<p>Fits the tuner (runs tuning). Adapts the generic fit interface to the specific tune method.</p> Source code in <code>skyulf-core/skyulf/modeling/tuning/tuner.py</code> <pre><code>def fit(\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: Dict[str, Any],\n    progress_callback: Optional[\n        Callable[[int, int, Optional[float], Optional[Dict]], None]\n    ] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    validation_data: Optional[tuple[pd.DataFrame, pd.Series]] = None,\n) -&gt; Any:\n    \"\"\"\n    Fits the tuner (runs tuning).\n    Adapts the generic fit interface to the specific tune method.\n    \"\"\"\n    # Convert config dict to TuningConfig\n    if isinstance(config, TuningConfig):\n        tuning_config = config\n    else:\n        # Extract valid keys for TuningConfig\n        valid_keys = TuningConfig.__annotations__.keys()\n        filtered_config = {k: v for k, v in config.items() if k in valid_keys}\n        tuning_config = TuningConfig(**filtered_config)  # type: ignore\n\n    tuning_result = self.tune(\n        X,\n        y,\n        tuning_config,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n        validation_data=validation_data,\n    )\n\n    # Refit the best model on the full dataset\n    best_params = tuning_result.best_params\n    final_params = {**self.model_calculator.default_params, **best_params}\n\n    # Ensure random_state is passed if available in config and not in params\n    if \"random_state\" not in final_params and hasattr(\n        tuning_config, \"random_state\"\n    ):\n        final_params[\"random_state\"] = tuning_config.random_state\n\n    if log_callback:\n        log_callback(f\"Refitting best model with params: {final_params}\")\n\n    model = self.model_calculator.model_class(**final_params)\n    model.fit(X, y)\n\n    return (model, tuning_result)\n</code></pre>"},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner.TunerCalculator.tune","title":"<code>tune(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>","text":"<p>Runs hyperparameter tuning.</p> Source code in <code>skyulf-core/skyulf/modeling/tuning/tuner.py</code> <pre><code>def tune(  # noqa: C901\n    self,\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: TuningConfig,\n    progress_callback: Optional[\n        Callable[[int, int, Optional[float], Optional[Dict]], None]\n    ] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    validation_data: Optional[tuple[pd.DataFrame, pd.Series]] = None,\n) -&gt; TuningResult:\n    \"\"\"\n    Runs hyperparameter tuning.\n    \"\"\"\n    # 1. Prepare Estimator\n    # We need a base estimator. Since our Calculator wraps the class,\n    # we need to instantiate the underlying sklearn model with default params.\n    # Assuming model_calculator is SklearnCalculator\n    if not hasattr(self.model_calculator, \"model_class\"):\n        raise ValueError(\"Tuner currently only supports SklearnCalculator\")\n\n    base_estimator = self.model_calculator.model_class(\n        **self.model_calculator.default_params\n    )\n\n    # 2. Prepare Splitter\n    # If validation data is provided, use PredefinedSplit to train on X and validate on validation_data\n    # Otherwise use CV\n\n    X_for_search = X\n    y_for_search = y\n\n    if validation_data is not None:\n        from sklearn.model_selection import PredefinedSplit\n\n        X_val, y_val = validation_data\n\n        # Concatenate Train and Val\n        X_for_search = pd.concat([X, X_val], axis=0)\n        y_for_search = pd.concat([y, y_val], axis=0)\n\n        # Create test_fold array: -1 for train, 0 for val\n        # -1 means \"never in test set\" (so always in training set)\n        # 0 means \"in test set for fold 0\"\n        test_fold = np.concatenate([np.full(len(X), -1), np.full(len(X_val), 0)])\n\n        cv = PredefinedSplit(test_fold)\n    else:\n        if not config.cv_enabled:\n            # Single split validation (20% holdout)\n            cv = ShuffleSplit(\n                n_splits=1, test_size=0.2, random_state=config.random_state\n            )\n        elif config.cv_type == \"time_series_split\":\n            cv = TimeSeriesSplit(n_splits=config.cv_folds)\n        elif config.cv_type == \"shuffle_split\":\n            cv = ShuffleSplit(\n                n_splits=config.cv_folds,\n                test_size=0.2,\n                random_state=config.random_state,\n            )\n        elif (\n            config.cv_type == \"stratified_k_fold\"\n            and self.model_calculator.problem_type == \"classification\"\n        ):\n            cv = StratifiedKFold(\n                n_splits=config.cv_folds,\n                shuffle=True,\n                random_state=config.random_state,\n            )\n        else:\n            # Default to KFold (also fallback for stratified if regression)\n            cv = KFold(\n                n_splits=config.cv_folds,\n                shuffle=True,\n                random_state=config.random_state,\n            )\n\n    # 3. Select Search Strategy\n    searcher = None\n\n    # Handle multiclass metrics and map user-friendly names\n    metric = config.metric\n\n    # Map common user-friendly metrics to sklearn scoring strings\n    metric_map = {\n        \"mse\": \"neg_mean_squared_error\",\n        \"mae\": \"neg_mean_absolute_error\",\n        \"rmse\": \"neg_root_mean_squared_error\",\n        \"r2\": \"r2\",\n        \"accuracy\": \"accuracy\",\n        \"f1\": \"f1\",\n        \"precision\": \"precision\",\n        \"recall\": \"recall\",\n        \"roc_auc\": \"roc_auc\",\n    }\n\n    if metric in metric_map:\n        metric = metric_map[metric]\n\n    if self.model_calculator.problem_type == \"classification\":\n        # Check if target is multiclass\n        is_multiclass = False\n        if isinstance(y, pd.Series):\n            is_multiclass = y.nunique() &gt; 2\n        elif isinstance(y, np.ndarray):\n            is_multiclass = len(np.unique(y)) &gt; 2\n\n        # If multiclass and metric is binary-default, switch to weighted\n        # Note: We check against the mapped names now (e.g. \"f1\", \"precision\")\n        if is_multiclass and metric in [\"f1\", \"precision\", \"recall\", \"roc_auc\"]:\n            metric = f\"{metric}_weighted\"\n            # roc_auc needs special handling (ovr/ovo) usually, but weighted often works for simple cases\n            if (\n                config.metric == \"roc_auc\"\n            ):  # Check original config metric name just in case\n                metric = \"roc_auc_ovr_weighted\"\n\n    if config.strategy in [\"grid\", \"random\"]:\n        # Use custom loop to support progress and log callbacks\n        if log_callback:\n            log_callback(\n                f\"Starting {config.strategy} search with custom loop for detailed logging...\"\n            )\n\n        # 1. Generate Candidates\n        param_space = self._clean_search_space(config.search_space)\n        candidates = []\n\n        if config.strategy == \"grid\":\n            candidates = list(ParameterGrid(param_space))\n        else:\n            # Random Search\n            candidates = list(\n                ParameterSampler(\n                    param_space,\n                    n_iter=config.n_trials,\n                    random_state=config.random_state,\n                )\n            )\n\n        total_candidates = len(candidates)\n        if log_callback:\n            log_callback(f\"Total candidates to evaluate: {total_candidates}\")\n\n        trials: List[Dict[str, Any]] = []\n        best_score = -float(\"inf\")\n        best_params = None\n\n        # 2. Iterate Candidates\n        for i, params in enumerate(candidates):\n            if log_callback:\n                log_callback(\n                    f\"Evaluating Candidate {i + 1}/{total_candidates}: {params}\"\n                )\n\n            # Use custom cross-validation loop to enable per-fold logging and progress tracking.\n            # We instantiate the model with the current candidate parameters and evaluate it\n            # using the configured CV strategy.\n\n            fold_scores = []\n\n            # Ensure numpy\n            X_arr = (\n                X_for_search.to_numpy()\n                if hasattr(X_for_search, \"to_numpy\")\n                else X_for_search\n            )\n            y_arr = (\n                y_for_search.to_numpy()\n                if hasattr(y_for_search, \"to_numpy\")\n                else y_for_search\n            )\n\n            for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_arr, y_arr)):\n                # Split\n                X_train_fold = (\n                    X_for_search.iloc[train_idx]\n                    if hasattr(X_for_search, \"iloc\")\n                    else X_for_search[train_idx]\n                )\n                y_train_fold = (\n                    y_for_search.iloc[train_idx]\n                    if hasattr(y_for_search, \"iloc\")\n                    else y_for_search[train_idx]\n                )\n                X_val_fold = (\n                    X_for_search.iloc[val_idx]\n                    if hasattr(X_for_search, \"iloc\")\n                    else X_for_search[val_idx]\n                )\n                y_val_fold = (\n                    y_for_search.iloc[val_idx]\n                    if hasattr(y_for_search, \"iloc\")\n                    else y_for_search[val_idx]\n                )\n\n                # Instantiate and Fit\n                # Note: We must handle potential errors (e.g. incompatible params)\n                try:\n                    model = self.model_calculator.model_class(\n                        **{**self.model_calculator.default_params, **params}\n                    )\n                    model.fit(X_train_fold, y_train_fold)\n\n                    # Score\n                    from sklearn.metrics import get_scorer\n\n                    scorer = get_scorer(metric)\n                    score = scorer(model, X_val_fold, y_val_fold)\n                    fold_scores.append(score)\n\n                    if log_callback:\n                        n_splits = cv.get_n_splits(X_arr, y_arr)\n                        log_callback(\n                            f\"  [Candidate {i + 1}] CV Fold {fold_idx + 1}/{n_splits} Score: {score:.4f}\"\n                        )\n                except Exception as e:\n                    if log_callback:\n                        n_splits = cv.get_n_splits(X_arr, y_arr)\n                        log_callback(\n                            f\"  [Candidate {i + 1}] CV Fold {fold_idx + 1}/{n_splits} Failed: {str(e)}\"\n                        )\n                    fold_scores.append(-float(\"inf\"))\n\n            # Filter out failed folds for mean calculation if possible, or penalize\n            valid_scores = [s for s in fold_scores if s != -float(\"inf\")]\n            if valid_scores:\n                mean_score = np.mean(valid_scores)\n            else:\n                mean_score = -float(\"inf\")\n\n            if log_callback:\n                log_callback(f\"Candidate {i + 1} Mean Score: {mean_score:.4f}\")\n\n            if progress_callback:\n                progress_callback(i + 1, total_candidates, mean_score, params)\n\n            trials.append({\"params\": params, \"score\": mean_score})\n\n            if mean_score &gt; best_score:\n                best_score = mean_score\n                best_params = params\n\n        return TuningResult(\n            best_params=best_params if best_params is not None else {},\n            best_score=best_score,\n            n_trials=total_candidates,\n            trials=trials,\n        )\n\n    elif config.strategy == \"halving_grid\":\n        searcher = HalvingGridSearchCV(\n            estimator=base_estimator,\n            param_grid=self._clean_search_space(config.search_space),\n            scoring=metric,\n            cv=cv,\n            n_jobs=-1,\n            random_state=config.random_state,\n            refit=False,\n            error_score=np.nan,\n        )\n    elif config.strategy == \"halving_random\":\n        searcher = HalvingRandomSearchCV(\n            estimator=base_estimator,\n            param_distributions=self._clean_search_space(config.search_space),\n            n_candidates=config.n_trials,  # Map n_trials to n_candidates\n            scoring=metric,\n            cv=cv,\n            n_jobs=-1,\n            random_state=config.random_state,\n            refit=False,\n            error_score=np.nan,\n        )\n    elif config.strategy == \"optuna\":\n        if not HAS_OPTUNA:\n            raise ImportError(\n                \"Optuna is not installed. Please install 'optuna' and 'optuna-integration'.\"\n            )\n\n        # Convert search space to Optuna distributions\n        distributions = {}\n        for k, v in config.search_space.items():\n            if isinstance(v, list):\n                distributions[k] = optuna.distributions.CategoricalDistribution(v)\n            else:\n                distributions[k] = v\n\n        # Optuna callbacks\n        callbacks = []\n        if progress_callback:\n\n            def _optuna_callback(study, trial):\n                # Optuna doesn't know total trials upfront easily if not set, but we have config.n_trials\n                # trial.value is the score (or None if failed/pruned)\n                score = trial.value if trial.value is not None else None\n\n                if log_callback:\n                    log_callback(\n                        f\"Optuna Trial {trial.number + 1} finished. Mean CV Score: {score}\"\n                    )\n\n                progress_callback(\n                    trial.number + 1, config.n_trials, score, trial.params\n                )\n\n            callbacks.append(_optuna_callback)\n\n        searcher = OptunaSearchCV(\n            estimator=base_estimator,\n            param_distributions=distributions,\n            n_trials=config.n_trials,\n            timeout=config.timeout,\n            cv=cv,  # type: ignore\n            scoring=metric,\n            n_jobs=-1,\n            random_state=config.random_state,\n            refit=False,\n            verbose=0,\n            callbacks=callbacks,\n        )\n    else:\n        raise ValueError(f\"Unknown tuning strategy: {config.strategy}\")\n\n    # 4. Run Search\n    # Ensure numpy\n    X_arr = (\n        X_for_search.to_numpy()\n        if hasattr(X_for_search, \"to_numpy\")\n        else X_for_search\n    )\n    y_arr = (\n        y_for_search.to_numpy()\n        if hasattr(y_for_search, \"to_numpy\")\n        else y_for_search\n    )\n\n    try:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Failed to report cross validation scores for TerminatorCallback\",\n            )\n            searcher.fit(X_arr, y_arr)\n    except Exception as e:\n        logger.error(f\"Hyperparameter tuning failed: {str(e)}\")\n        error_msg = str(e)\n        if \"No trials are completed yet\" in error_msg:\n            raise ValueError(\n                \"Hyperparameter tuning failed: No trials completed successfully. \"\n                \"This usually means the model failed to train with the provided hyperparameter combinations. \"\n                \"Please check your search space and data.\"\n            ) from e\n\n        if (\n            \"n_samples\" in error_msg\n            and \"resample\" in error_msg\n            and \"Got 0\" in error_msg\n        ):\n            raise ValueError(\n                \"Hyperparameter tuning with Halving strategy failed because the dataset is too small \"\n                \"for the configured halving parameters. Please try using 'Random Search' or 'Grid Search' instead, \"\n                \"or increase your dataset size.\"\n            ) from e\n\n        raise e\n\n    # 5. Extract Results\n    if not hasattr(searcher, \"best_params_\"):\n        raise ValueError(\n            \"Hyperparameter tuning failed to find any valid combination of parameters. All trials likely failed.\"\n        )\n\n    best_params = searcher.best_params_\n    best_score = searcher.best_score_\n\n    # Collect trials\n    trials = []\n    # Special handling for Optuna\n    if config.strategy == \"optuna\" and hasattr(searcher, \"study_\"):\n        for trial in searcher.study_.trials:\n            # Only include completed trials\n            if trial.state.name == \"COMPLETE\":\n                trials.append({\"params\": trial.params, \"score\": trial.value})\n    elif hasattr(searcher, \"cv_results_\"):\n        results = searcher.cv_results_\n        if \"params\" in results:\n            n_candidates = len(results[\"params\"])\n            for i in range(n_candidates):\n                trials.append(\n                    {\n                        \"params\": results[\"params\"][i],\n                        \"score\": results[\"mean_test_score\"][i],\n                    }\n                )\n\n    return TuningResult(\n        best_params=best_params,\n        best_score=best_score,\n        n_trials=len(trials),\n        trials=trials,\n    )\n</code></pre>"},{"location":"reference/api/preprocessing/index.html","title":"API: preprocessing","text":"<p>The preprocessing package contains Calculator/Applier nodes and the <code>FeatureEngineer</code> orchestrator.</p>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing","title":"<code>skyulf.preprocessing</code>","text":""},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.BaseApplier","title":"<code>BaseApplier</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>class BaseApplier(ABC):\n    @abstractmethod\n    def apply(\n        self, df: Union[pd.DataFrame, tuple], params: Dict[str, Any]\n    ) -&gt; Union[pd.DataFrame, tuple, SplitDataset]:\n        \"\"\"\n        Applies the transformation using fitted parameters.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.BaseApplier.apply","title":"<code>apply(df, params)</code>  <code>abstractmethod</code>","text":"<p>Applies the transformation using fitted parameters.</p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>@abstractmethod\ndef apply(\n    self, df: Union[pd.DataFrame, tuple], params: Dict[str, Any]\n) -&gt; Union[pd.DataFrame, tuple, SplitDataset]:\n    \"\"\"\n    Applies the transformation using fitted parameters.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.BaseCalculator","title":"<code>BaseCalculator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>class BaseCalculator(ABC):\n    @abstractmethod\n    def fit(\n        self, df: Union[pd.DataFrame, tuple], config: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Calculates parameters from the training data.\n        Returns a dictionary of fitted parameters (serializable).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.BaseCalculator.fit","title":"<code>fit(df, config)</code>  <code>abstractmethod</code>","text":"<p>Calculates parameters from the training data. Returns a dictionary of fitted parameters (serializable).</p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>@abstractmethod\ndef fit(\n    self, df: Union[pd.DataFrame, tuple], config: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Calculates parameters from the training data.\n    Returns a dictionary of fitted parameters (serializable).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.CustomBinningCalculator","title":"<code>CustomBinningCalculator</code>","text":"<p>               Bases: <code>BaseCalculator</code></p> <p>Calculator for CustomBinning node. Applies specific bin edges to selected columns.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>class CustomBinningCalculator(BaseCalculator):\n    \"\"\"\n    Calculator for CustomBinning node.\n    Applies specific bin edges to selected columns.\n    \"\"\"\n\n    def fit(\n        self,\n        df: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n        columns = resolve_columns(X, config, detect_numeric_columns)\n        bins = config.get(\"bins\")\n\n        bin_edges_map = {}\n        if bins:\n            sorted_bins = sorted(bins)\n            for col in columns:\n                if col in X.columns:\n                    bin_edges_map[col] = sorted_bins\n\n        return {\n            \"type\": \"general_binning\",  # Use GeneralBinningApplier\n            \"bin_edges\": bin_edges_map,\n            \"output_suffix\": config.get(\"output_suffix\", \"_binned\"),\n            \"drop_original\": config.get(\"drop_original\", False),\n            \"label_format\": config.get(\"label_format\", \"ordinal\"),\n            \"missing_strategy\": config.get(\"missing_strategy\", \"keep\"),\n            \"missing_label\": config.get(\"missing_label\", \"Missing\"),\n            \"include_lowest\": config.get(\"include_lowest\", True),\n            \"precision\": config.get(\"precision\", 3),\n        }\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.FeatureEngineer","title":"<code>FeatureEngineer</code>","text":"<p>Orchestrates a sequence of feature engineering steps.</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>class FeatureEngineer:\n    \"\"\"\n    Orchestrates a sequence of feature engineering steps.\n    \"\"\"\n\n    def __init__(self, steps_config: List[Dict[str, Any]]):\n        self.steps_config = steps_config\n        self.fitted_steps: List[Dict[str, Any]] = []\n\n    def transform(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply fitted transformations to new data.\n        \"\"\"\n        current_data = data\n\n        for step in self.fitted_steps:\n            name = step[\"name\"]\n            transformer_type = step[\"type\"]\n            applier = step[\"applier\"]\n            artifact = step[\"artifact\"]\n\n            # Skip splitters during inference/transform\n            if transformer_type in [\n                \"TrainTestSplitter\",\n                \"feature_target_split\",\n                \"Oversampling\",\n                \"Undersampling\",\n            ]:\n                continue\n\n            logger.debug(f\"Applying step: {name} ({transformer_type})\")\n            current_data = applier.apply(current_data, artifact)\n\n        return current_data\n\n    def fit_transform(self, data: Union[pd.DataFrame, Any], node_id_prefix=\"\") -&gt; Any:  # noqa: C901\n        \"\"\"\n        Runs the pipeline on data.\n        Returns: (transformed_data, metrics_dict)\n        \"\"\"\n        self.fitted_steps = []  # Reset fitted steps\n        current_data = data\n        metrics: Dict[str, Any] = {}\n\n        for i, step in enumerate(self.steps_config):\n            name = step[\"name\"]\n            transformer_type = step[\"transformer\"]\n            params = step.get(\"params\", {})\n\n            logger.info(f\"Running step {i}: {name} ({transformer_type})\")\n            logger.debug(\n                f\"FeatureEngineer running step {i}: {name} ({transformer_type})\"\n            )\n            logger.debug(f\"current_data type: {type(current_data)}\")\n\n            # Capture metrics before\n            rows_before, cols_before = get_data_stats(current_data)\n\n            # Keep reference for comparison (for Winsorize metrics)\n            data_before = current_data\n\n            calculator, applier = self._get_transformer_components(transformer_type)\n\n            # We need a unique ID for this step's artifacts\n            step_node_id = f\"{node_id_prefix}_{name}\"\n\n            transformer = StatefulTransformer(calculator, applier, step_node_id)\n\n            # Handle special transformers that change data structure\n            # Splitters return SplitDataset or (X, y) tuples instead of a simple DataFrame,\n            # so they bypass the standard StatefulTransformer wrapper.\n\n            # Initialize fitted_params\n            fitted_params = {}\n\n            if transformer_type == \"TrainTestSplitter\":\n                logger.debug(\"Handling TrainTestSplitter\")\n                # TrainTestSplitter changes DataFrame -&gt; SplitDataset.\n                # We bypass StatefulTransformer to allow this structural change.\n                # It can also handle (X, y) tuple if FeatureTargetSplit was done first.\n                if isinstance(current_data, (pd.DataFrame, tuple)):\n                    logger.debug(\"Executing TrainTestSplitter logic\")\n                    params = calculator.fit(current_data, params)\n                    current_data = applier.apply(current_data, params)\n                    # In SDK, params are returned but not auto-saved to artifact store here.\n                    # The Pipeline object will handle state persistence.\n                else:\n                    logger.debug(\n                        f\"Skipping TrainTestSplitter. current_data is {type(current_data)}\"\n                    )\n                    logger.warning(\n                        \"Attempting to split an already split dataset. Skipping TrainTestSplitter.\"\n                    )\n\n            elif transformer_type == \"feature_target_split\":\n                logger.debug(\"Handling feature_target_split\")\n                # FeatureTargetSplitter changes structure to (X, y) or Dict of (X, y).\n                # We bypass StatefulTransformer to allow this structural change.\n                params = calculator.fit(current_data, params)\n                current_data = applier.apply(current_data, params)\n\n            else:\n                logger.debug(\"Handling standard transformer via StatefulTransformer\")\n                current_data = transformer.fit_transform(current_data, params)\n                # In SDK, transformer.params holds the state.\n                fitted_params = transformer.params\n\n                self.fitted_steps.append(\n                    {\n                        \"name\": name,\n                        \"type\": transformer_type,\n                        \"applier\": applier,\n                        \"artifact\": fitted_params,\n                    }\n                )\n\n            logger.debug(f\"Step {i} complete. New data type: {type(current_data)}\")\n\n            # Retrieve fitted params to get metrics from the calculator\n            try:\n                if fitted_params:\n                    # Imputation Metrics\n                    if transformer_type in [\n                        \"SimpleImputer\",\n                        \"KNNImputer\",\n                        \"IterativeImputer\",\n                    ]:\n                        if \"missing_counts\" in fitted_params:\n                            metrics[\"missing_counts\"] = fitted_params[\"missing_counts\"]\n                        if \"total_missing\" in fitted_params:\n                            metrics[\"total_missing\"] = fitted_params[\"total_missing\"]\n                        if \"fill_values\" in fitted_params:\n                            metrics[\"fill_values\"] = fitted_params[\"fill_values\"]\n\n                    # Feature Selection Metrics\n                    if transformer_type in [\n                        \"feature_selection\",\n                        \"UnivariateSelection\",\n                        \"ModelBasedSelection\",\n                        \"VarianceThreshold\",\n                    ]:\n                        if \"feature_scores\" in fitted_params:\n                            metrics[\"feature_scores\"] = fitted_params[\"feature_scores\"]\n                        if \"p_values\" in fitted_params:\n                            metrics[\"p_values\"] = fitted_params[\"p_values\"]\n                        if \"feature_importances\" in fitted_params:\n                            metrics[\"feature_importances\"] = fitted_params[\n                                \"feature_importances\"\n                            ]\n                        if \"variances\" in fitted_params:\n                            metrics[\"variances\"] = fitted_params[\"variances\"]\n                        if \"ranking\" in fitted_params:\n                            metrics[\"ranking\"] = fitted_params[\"ranking\"]\n                        if \"selected_columns\" in fitted_params:\n                            metrics[\"selected_columns\"] = fitted_params[\n                                \"selected_columns\"\n                            ]\n\n                    # Scaling Metrics\n                    if transformer_type in [\n                        \"StandardScaler\",\n                        \"MinMaxScaler\",\n                        \"RobustScaler\",\n                        \"MaxAbsScaler\",\n                    ]:\n                        if \"mean\" in fitted_params:\n                            metrics[\"mean\"] = fitted_params[\"mean\"]\n                        if \"scale\" in fitted_params:\n                            metrics[\"scale\"] = fitted_params[\"scale\"]\n                        if \"var\" in fitted_params:\n                            metrics[\"var\"] = fitted_params[\"var\"]\n                        if \"min\" in fitted_params:\n                            metrics[\"min\"] = fitted_params[\"min\"]\n                        if \"data_min\" in fitted_params:\n                            metrics[\"data_min\"] = fitted_params[\"data_min\"]\n                        if \"data_max\" in fitted_params:\n                            metrics[\"data_max\"] = fitted_params[\"data_max\"]\n                        if \"center\" in fitted_params:\n                            metrics[\"center\"] = fitted_params[\"center\"]\n                        if \"max_abs\" in fitted_params:\n                            metrics[\"max_abs\"] = fitted_params[\"max_abs\"]\n                        if \"columns\" in fitted_params:\n                            metrics[\"columns\"] = fitted_params[\"columns\"]\n\n                    # Outlier Metrics\n                    if transformer_type in [\n                        \"IQR\",\n                        \"Winsorize\",\n                        \"ZScore\",\n                        \"EllipticEnvelope\",\n                    ]:\n                        if \"warnings\" in fitted_params:\n                            metrics[\"warnings\"] = fitted_params[\"warnings\"]\n\n                    if transformer_type in [\"IQR\", \"Winsorize\"]:\n                        if \"bounds\" in fitted_params:\n                            metrics[\"bounds\"] = fitted_params[\"bounds\"]\n\n                    if transformer_type == \"ZScore\":\n                        if \"stats\" in fitted_params:\n                            metrics[\"stats\"] = fitted_params[\"stats\"]\n\n                    if transformer_type == \"EllipticEnvelope\":\n                        if \"contamination\" in fitted_params:\n                            metrics[\"contamination\"] = fitted_params[\"contamination\"]\n\n                    # Bucketing Metrics\n                    if transformer_type in [\n                        \"GeneralBinning\",\n                        \"EqualWidthBinning\",\n                        \"EqualFrequencyBinning\",\n                        \"CustomBinning\",\n                        \"KBinsDiscretizer\",\n                    ]:\n                        if \"bin_edges\" in fitted_params:\n                            metrics[\"bin_edges\"] = fitted_params[\"bin_edges\"]\n                        if \"n_bins\" in fitted_params:\n                            metrics[\"n_bins\"] = fitted_params[\"n_bins\"]\n\n                    # Feature Generation Metrics\n                    if transformer_type in [\"FeatureMath\", \"FeatureGenerationNode\"]:\n                        if \"operations\" in fitted_params:\n                            metrics[\"operations_count\"] = len(\n                                fitted_params[\"operations\"]\n                            )\n                            metrics[\"operations\"] = fitted_params[\"operations\"]\n                        # Calculate generated features by comparing columns\n                        if isinstance(data_before, pd.DataFrame) and isinstance(\n                            current_data, pd.DataFrame\n                        ):\n                            new_cols = list(\n                                set(current_data.columns) - set(data_before.columns)\n                            )\n                            metrics[\"generated_features\"] = new_cols\n                        elif isinstance(data_before, SplitDataset) and isinstance(\n                            current_data, SplitDataset\n                        ):\n                            # Check train set\n                            if isinstance(\n                                data_before.train, pd.DataFrame\n                            ) and isinstance(current_data.train, pd.DataFrame):\n                                new_cols = list(\n                                    set(current_data.train.columns)\n                                    - set(data_before.train.columns)\n                                )\n                                metrics[\"generated_features\"] = new_cols\n                            elif isinstance(data_before.train, tuple) and isinstance(\n                                current_data.train, tuple\n                            ):\n                                # (X, y) tuple\n                                X_before, _ = data_before.train\n                                X_after, _ = current_data.train\n                                if isinstance(X_before, pd.DataFrame) and isinstance(\n                                    X_after, pd.DataFrame\n                                ):\n                                    new_cols = list(\n                                        set(X_after.columns) - set(X_before.columns)\n                                    )\n                                    metrics[\"generated_features\"] = new_cols\n\n            except Exception as e:\n                logger.warning(f\"Failed to retrieve metrics for step {name}: {e}\")\n\n            # Capture metrics after\n            rows_after, cols_after = get_data_stats(current_data)\n\n            # Resampling Metrics (Calculated from data)\n            if transformer_type in [\"Oversampling\", \"Undersampling\"]:\n                try:\n                    # Extract y to calculate class counts\n                    y_res = None\n                    if isinstance(current_data, SplitDataset):\n                        if isinstance(current_data.train, tuple):\n                            _, y_res = current_data.train\n                        elif isinstance(current_data.train, pd.DataFrame):\n                            # Try to find target column from params\n                            target_col = params.get(\"target_column\")\n                            if target_col and target_col in current_data.train.columns:\n                                y_res = current_data.train[target_col]\n                    elif isinstance(current_data, tuple):\n                        _, y_res = current_data\n                    elif isinstance(current_data, pd.DataFrame):\n                        target_col = params.get(\"target_column\")\n                        if target_col and target_col in current_data.columns:\n                            y_res = current_data[target_col]\n\n                    if y_res is not None:\n                        counts = y_res.value_counts().to_dict()\n                        # Convert keys to string to ensure JSON serializability\n                        metrics[\"class_counts\"] = {\n                            str(k): int(v) for k, v in counts.items()\n                        }\n                        metrics[\"total_samples\"] = int(len(y_res))\n                except Exception as e:\n                    logger.warning(f\"Failed to calculate resampling metrics: {e}\")\n\n            if rows_after &gt; 0 or cols_after:\n                if transformer_type in [\n                    \"DropMissingRows\",\n                    \"Deduplicate\",\n                    \"IQR\",\n                    \"ZScore\",\n                    \"EllipticEnvelope\",\n                    \"Winsorize\",\n                ]:\n                    dropped = rows_before - rows_after\n                    metrics[f\"{transformer_type}_rows_removed\"] = dropped\n                    metrics[f\"{transformer_type}_rows_remaining\"] = rows_after\n                    metrics[f\"{transformer_type}_rows_total\"] = rows_before\n                    metrics[\"rows_removed\"] = dropped\n                    metrics[\"rows_total\"] = rows_before\n\n                    # Special metric for Winsorize: Values Clipped\n                    if transformer_type == \"Winsorize\":\n                        try:\n                            clipped_count = 0\n\n                            # Helper to count diffs\n                            def count_diffs(df1, df2):\n                                if isinstance(df1, pd.DataFrame) and isinstance(\n                                    df2, pd.DataFrame\n                                ):\n                                    if df1.shape == df2.shape:\n                                        return int(df1.ne(df2).sum().sum())\n                                elif (\n                                    isinstance(df1, tuple)\n                                    and isinstance(df2, tuple)\n                                    and len(df1) == 2\n                                    and len(df2) == 2\n                                ):\n                                    # Handle (X, y) tuple\n                                    diffs = 0\n                                    # Compare X (index 0)\n                                    if isinstance(df1[0], pd.DataFrame) and isinstance(\n                                        df2[0], pd.DataFrame\n                                    ):\n                                        if df1[0].shape == df2[0].shape:\n                                            diffs += int(df1[0].ne(df2[0]).sum().sum())\n                                    # Compare y (index 1) - usually Series\n                                    if isinstance(\n                                        df1[1], (pd.DataFrame, pd.Series)\n                                    ) and isinstance(df2[1], (pd.DataFrame, pd.Series)):\n                                        if df1[1].shape == df2[1].shape:\n                                            diffs += int(df1[1].ne(df2[1]).sum().sum())  # type: ignore\n                                    return diffs\n                                return 0\n\n                            if isinstance(data_before, pd.DataFrame) and isinstance(\n                                current_data, pd.DataFrame\n                            ):\n                                clipped_count = count_diffs(data_before, current_data)\n                            elif isinstance(data_before, SplitDataset) and isinstance(\n                                current_data, SplitDataset\n                            ):\n                                clipped_count += count_diffs(\n                                    data_before.train, current_data.train\n                                )\n                                clipped_count += count_diffs(\n                                    data_before.test, current_data.test\n                                )\n                                clipped_count += count_diffs(\n                                    data_before.validation, current_data.validation\n                                )\n\n                            metrics[\"values_clipped\"] = clipped_count\n                        except Exception as e:\n                            logger.warning(\n                                f\"Failed to calculate values_clipped for Winsorize: {e}\"\n                            )\n                            pass\n\n                if transformer_type == \"MissingIndicator\":\n                    new_cols_set = cols_after - cols_before\n                    metrics[\"missing_indicators_created\"] = len(new_cols_set)\n                    cast(Dict[str, Any], metrics)[\"missing_indicators_columns\"] = list(\n                        new_cols_set\n                    )\n\n                if transformer_type == \"DropMissingColumns\":\n                    dropped_cols_set = cols_before - cols_after\n                    cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                        dropped_cols_set\n                    )\n                    metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n                if transformer_type == \"feature_selection\":\n                    dropped_cols_set = cols_before - cols_after\n                    cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                        dropped_cols_set\n                    )\n                    metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n                if transformer_type in [\n                    \"OneHotEncoder\",\n                    \"LabelEncoder\",\n                    \"OrdinalEncoder\",\n                    \"TargetEncoder\",\n                    \"HashEncoder\",\n                    \"DummyEncoder\",\n                ]:\n                    new_cols_set = cols_after - cols_before\n                    metrics[\"new_features_count\"] = len(new_cols_set)\n                    metrics[\"encoded_columns_count\"] = len(params.get(\"columns\", []))\n\n                    if \"categories_count\" in params:\n                        metrics[\"categories_count\"] = params[\"categories_count\"]\n                    if \"classes_count\" in params:\n                        metrics[\"classes_count\"] = params[\"classes_count\"]\n\n        return current_data, metrics\n\n    def _get_transformer_components(self, type_name: str):  # noqa: C901\n        if type_name == \"TrainTestSplitter\":\n            return SplitCalculator(), SplitApplier()\n        elif type_name == \"feature_target_split\":\n            return FeatureTargetSplitCalculator(), FeatureTargetSplitApplier()\n        elif type_name == \"TextCleaning\":\n            return TextCleaningCalculator(), TextCleaningApplier()\n        elif type_name == \"ValueReplacement\":\n            return ValueReplacementCalculator(), ValueReplacementApplier()\n        elif type_name == \"Deduplicate\":\n            return DeduplicateCalculator(), DeduplicateApplier()\n        elif type_name == \"DropMissingColumns\":\n            return DropMissingColumnsCalculator(), DropMissingColumnsApplier()\n        elif type_name == \"DropMissingRows\":\n            return DropMissingRowsCalculator(), DropMissingRowsApplier()\n        elif type_name == \"MissingIndicator\":\n            return MissingIndicatorCalculator(), MissingIndicatorApplier()\n        elif type_name == \"AliasReplacement\":\n            return AliasReplacementCalculator(), AliasReplacementApplier()\n        elif type_name == \"InvalidValueReplacement\":\n            return InvalidValueReplacementCalculator(), InvalidValueReplacementApplier()\n        elif type_name == \"SimpleImputer\":\n            return SimpleImputerCalculator(), SimpleImputerApplier()\n        elif type_name == \"KNNImputer\":\n            return KNNImputerCalculator(), KNNImputerApplier()\n        elif type_name == \"IterativeImputer\":\n            return IterativeImputerCalculator(), IterativeImputerApplier()\n        elif type_name == \"OneHotEncoder\":\n            return OneHotEncoderCalculator(), OneHotEncoderApplier()\n        elif type_name == \"DummyEncoder\":\n            from .encoding import DummyEncoderApplier, DummyEncoderCalculator\n\n            return DummyEncoderCalculator(), DummyEncoderApplier()\n        elif type_name == \"OrdinalEncoder\":\n            return OrdinalEncoderCalculator(), OrdinalEncoderApplier()\n        elif type_name == \"LabelEncoder\":\n            return LabelEncoderCalculator(), LabelEncoderApplier()\n        elif type_name == \"TargetEncoder\":\n            return TargetEncoderCalculator(), TargetEncoderApplier()\n        elif type_name == \"HashEncoder\":\n            return HashEncoderCalculator(), HashEncoderApplier()\n        elif type_name == \"StandardScaler\":\n            return StandardScalerCalculator(), StandardScalerApplier()\n        elif type_name == \"MinMaxScaler\":\n            return MinMaxScalerCalculator(), MinMaxScalerApplier()\n        elif type_name == \"RobustScaler\":\n            return RobustScalerCalculator(), RobustScalerApplier()\n        elif type_name == \"MaxAbsScaler\":\n            return MaxAbsScalerCalculator(), MaxAbsScalerApplier()\n        elif type_name == \"IQR\":\n            return IQRCalculator(), IQRApplier()\n        elif type_name == \"ZScore\":\n            return ZScoreCalculator(), ZScoreApplier()\n        elif type_name == \"Winsorize\":\n            return WinsorizeCalculator(), WinsorizeApplier()\n        elif type_name == \"ManualBounds\":\n            return ManualBoundsCalculator(), ManualBoundsApplier()\n        elif type_name == \"EllipticEnvelope\":\n            return EllipticEnvelopeCalculator(), EllipticEnvelopeApplier()\n        elif type_name == \"PowerTransformer\":\n            return PowerTransformerCalculator(), PowerTransformerApplier()\n        elif type_name == \"SimpleTransformation\":\n            return SimpleTransformationCalculator(), SimpleTransformationApplier()\n        elif type_name == \"GeneralTransformation\":\n            return GeneralTransformationCalculator(), GeneralTransformationApplier()\n        elif type_name == \"GeneralBinning\":\n            return GeneralBinningCalculator(), GeneralBinningApplier()\n        elif type_name == \"CustomBinning\":\n            return CustomBinningCalculator(), CustomBinningApplier()\n        elif type_name == \"KBinsDiscretizer\":\n            return KBinsDiscretizerCalculator(), KBinsDiscretizerApplier()\n        elif type_name == \"VarianceThreshold\":\n            return VarianceThresholdCalculator(), VarianceThresholdApplier()\n        elif type_name == \"CorrelationThreshold\":\n            return CorrelationThresholdCalculator(), CorrelationThresholdApplier()\n        elif type_name == \"UnivariateSelection\":\n            return UnivariateSelectionCalculator(), UnivariateSelectionApplier()\n        elif type_name == \"ModelBasedSelection\":\n            return ModelBasedSelectionCalculator(), ModelBasedSelectionApplier()\n        elif type_name == \"feature_selection\":\n            return FeatureSelectionCalculator(), FeatureSelectionApplier()\n        elif type_name == \"Casting\":\n            return CastingCalculator(), CastingApplier()\n        elif type_name == \"PolynomialFeatures\":\n            return PolynomialFeaturesCalculator(), PolynomialFeaturesApplier()\n        elif type_name == \"FeatureMath\" or type_name == \"FeatureGenerationNode\":\n            return FeatureGenerationCalculator(), FeatureGenerationApplier()\n        elif type_name == \"Oversampling\":\n            return OversamplingCalculator(), OversamplingApplier()\n        elif type_name == \"Undersampling\":\n            return UndersamplingCalculator(), UndersamplingApplier()\n        elif type_name == \"DatasetProfile\":\n            return DatasetProfileCalculator(), DatasetProfileApplier()\n        elif type_name == \"DataSnapshot\":\n            return DataSnapshotCalculator(), DataSnapshotApplier()\n        else:\n            raise ValueError(f\"Unknown transformer type: {type_name}\")\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.FeatureEngineer.fit_transform","title":"<code>fit_transform(data, node_id_prefix='')</code>","text":"<p>Runs the pipeline on data. Returns: (transformed_data, metrics_dict)</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>def fit_transform(self, data: Union[pd.DataFrame, Any], node_id_prefix=\"\") -&gt; Any:  # noqa: C901\n    \"\"\"\n    Runs the pipeline on data.\n    Returns: (transformed_data, metrics_dict)\n    \"\"\"\n    self.fitted_steps = []  # Reset fitted steps\n    current_data = data\n    metrics: Dict[str, Any] = {}\n\n    for i, step in enumerate(self.steps_config):\n        name = step[\"name\"]\n        transformer_type = step[\"transformer\"]\n        params = step.get(\"params\", {})\n\n        logger.info(f\"Running step {i}: {name} ({transformer_type})\")\n        logger.debug(\n            f\"FeatureEngineer running step {i}: {name} ({transformer_type})\"\n        )\n        logger.debug(f\"current_data type: {type(current_data)}\")\n\n        # Capture metrics before\n        rows_before, cols_before = get_data_stats(current_data)\n\n        # Keep reference for comparison (for Winsorize metrics)\n        data_before = current_data\n\n        calculator, applier = self._get_transformer_components(transformer_type)\n\n        # We need a unique ID for this step's artifacts\n        step_node_id = f\"{node_id_prefix}_{name}\"\n\n        transformer = StatefulTransformer(calculator, applier, step_node_id)\n\n        # Handle special transformers that change data structure\n        # Splitters return SplitDataset or (X, y) tuples instead of a simple DataFrame,\n        # so they bypass the standard StatefulTransformer wrapper.\n\n        # Initialize fitted_params\n        fitted_params = {}\n\n        if transformer_type == \"TrainTestSplitter\":\n            logger.debug(\"Handling TrainTestSplitter\")\n            # TrainTestSplitter changes DataFrame -&gt; SplitDataset.\n            # We bypass StatefulTransformer to allow this structural change.\n            # It can also handle (X, y) tuple if FeatureTargetSplit was done first.\n            if isinstance(current_data, (pd.DataFrame, tuple)):\n                logger.debug(\"Executing TrainTestSplitter logic\")\n                params = calculator.fit(current_data, params)\n                current_data = applier.apply(current_data, params)\n                # In SDK, params are returned but not auto-saved to artifact store here.\n                # The Pipeline object will handle state persistence.\n            else:\n                logger.debug(\n                    f\"Skipping TrainTestSplitter. current_data is {type(current_data)}\"\n                )\n                logger.warning(\n                    \"Attempting to split an already split dataset. Skipping TrainTestSplitter.\"\n                )\n\n        elif transformer_type == \"feature_target_split\":\n            logger.debug(\"Handling feature_target_split\")\n            # FeatureTargetSplitter changes structure to (X, y) or Dict of (X, y).\n            # We bypass StatefulTransformer to allow this structural change.\n            params = calculator.fit(current_data, params)\n            current_data = applier.apply(current_data, params)\n\n        else:\n            logger.debug(\"Handling standard transformer via StatefulTransformer\")\n            current_data = transformer.fit_transform(current_data, params)\n            # In SDK, transformer.params holds the state.\n            fitted_params = transformer.params\n\n            self.fitted_steps.append(\n                {\n                    \"name\": name,\n                    \"type\": transformer_type,\n                    \"applier\": applier,\n                    \"artifact\": fitted_params,\n                }\n            )\n\n        logger.debug(f\"Step {i} complete. New data type: {type(current_data)}\")\n\n        # Retrieve fitted params to get metrics from the calculator\n        try:\n            if fitted_params:\n                # Imputation Metrics\n                if transformer_type in [\n                    \"SimpleImputer\",\n                    \"KNNImputer\",\n                    \"IterativeImputer\",\n                ]:\n                    if \"missing_counts\" in fitted_params:\n                        metrics[\"missing_counts\"] = fitted_params[\"missing_counts\"]\n                    if \"total_missing\" in fitted_params:\n                        metrics[\"total_missing\"] = fitted_params[\"total_missing\"]\n                    if \"fill_values\" in fitted_params:\n                        metrics[\"fill_values\"] = fitted_params[\"fill_values\"]\n\n                # Feature Selection Metrics\n                if transformer_type in [\n                    \"feature_selection\",\n                    \"UnivariateSelection\",\n                    \"ModelBasedSelection\",\n                    \"VarianceThreshold\",\n                ]:\n                    if \"feature_scores\" in fitted_params:\n                        metrics[\"feature_scores\"] = fitted_params[\"feature_scores\"]\n                    if \"p_values\" in fitted_params:\n                        metrics[\"p_values\"] = fitted_params[\"p_values\"]\n                    if \"feature_importances\" in fitted_params:\n                        metrics[\"feature_importances\"] = fitted_params[\n                            \"feature_importances\"\n                        ]\n                    if \"variances\" in fitted_params:\n                        metrics[\"variances\"] = fitted_params[\"variances\"]\n                    if \"ranking\" in fitted_params:\n                        metrics[\"ranking\"] = fitted_params[\"ranking\"]\n                    if \"selected_columns\" in fitted_params:\n                        metrics[\"selected_columns\"] = fitted_params[\n                            \"selected_columns\"\n                        ]\n\n                # Scaling Metrics\n                if transformer_type in [\n                    \"StandardScaler\",\n                    \"MinMaxScaler\",\n                    \"RobustScaler\",\n                    \"MaxAbsScaler\",\n                ]:\n                    if \"mean\" in fitted_params:\n                        metrics[\"mean\"] = fitted_params[\"mean\"]\n                    if \"scale\" in fitted_params:\n                        metrics[\"scale\"] = fitted_params[\"scale\"]\n                    if \"var\" in fitted_params:\n                        metrics[\"var\"] = fitted_params[\"var\"]\n                    if \"min\" in fitted_params:\n                        metrics[\"min\"] = fitted_params[\"min\"]\n                    if \"data_min\" in fitted_params:\n                        metrics[\"data_min\"] = fitted_params[\"data_min\"]\n                    if \"data_max\" in fitted_params:\n                        metrics[\"data_max\"] = fitted_params[\"data_max\"]\n                    if \"center\" in fitted_params:\n                        metrics[\"center\"] = fitted_params[\"center\"]\n                    if \"max_abs\" in fitted_params:\n                        metrics[\"max_abs\"] = fitted_params[\"max_abs\"]\n                    if \"columns\" in fitted_params:\n                        metrics[\"columns\"] = fitted_params[\"columns\"]\n\n                # Outlier Metrics\n                if transformer_type in [\n                    \"IQR\",\n                    \"Winsorize\",\n                    \"ZScore\",\n                    \"EllipticEnvelope\",\n                ]:\n                    if \"warnings\" in fitted_params:\n                        metrics[\"warnings\"] = fitted_params[\"warnings\"]\n\n                if transformer_type in [\"IQR\", \"Winsorize\"]:\n                    if \"bounds\" in fitted_params:\n                        metrics[\"bounds\"] = fitted_params[\"bounds\"]\n\n                if transformer_type == \"ZScore\":\n                    if \"stats\" in fitted_params:\n                        metrics[\"stats\"] = fitted_params[\"stats\"]\n\n                if transformer_type == \"EllipticEnvelope\":\n                    if \"contamination\" in fitted_params:\n                        metrics[\"contamination\"] = fitted_params[\"contamination\"]\n\n                # Bucketing Metrics\n                if transformer_type in [\n                    \"GeneralBinning\",\n                    \"EqualWidthBinning\",\n                    \"EqualFrequencyBinning\",\n                    \"CustomBinning\",\n                    \"KBinsDiscretizer\",\n                ]:\n                    if \"bin_edges\" in fitted_params:\n                        metrics[\"bin_edges\"] = fitted_params[\"bin_edges\"]\n                    if \"n_bins\" in fitted_params:\n                        metrics[\"n_bins\"] = fitted_params[\"n_bins\"]\n\n                # Feature Generation Metrics\n                if transformer_type in [\"FeatureMath\", \"FeatureGenerationNode\"]:\n                    if \"operations\" in fitted_params:\n                        metrics[\"operations_count\"] = len(\n                            fitted_params[\"operations\"]\n                        )\n                        metrics[\"operations\"] = fitted_params[\"operations\"]\n                    # Calculate generated features by comparing columns\n                    if isinstance(data_before, pd.DataFrame) and isinstance(\n                        current_data, pd.DataFrame\n                    ):\n                        new_cols = list(\n                            set(current_data.columns) - set(data_before.columns)\n                        )\n                        metrics[\"generated_features\"] = new_cols\n                    elif isinstance(data_before, SplitDataset) and isinstance(\n                        current_data, SplitDataset\n                    ):\n                        # Check train set\n                        if isinstance(\n                            data_before.train, pd.DataFrame\n                        ) and isinstance(current_data.train, pd.DataFrame):\n                            new_cols = list(\n                                set(current_data.train.columns)\n                                - set(data_before.train.columns)\n                            )\n                            metrics[\"generated_features\"] = new_cols\n                        elif isinstance(data_before.train, tuple) and isinstance(\n                            current_data.train, tuple\n                        ):\n                            # (X, y) tuple\n                            X_before, _ = data_before.train\n                            X_after, _ = current_data.train\n                            if isinstance(X_before, pd.DataFrame) and isinstance(\n                                X_after, pd.DataFrame\n                            ):\n                                new_cols = list(\n                                    set(X_after.columns) - set(X_before.columns)\n                                )\n                                metrics[\"generated_features\"] = new_cols\n\n        except Exception as e:\n            logger.warning(f\"Failed to retrieve metrics for step {name}: {e}\")\n\n        # Capture metrics after\n        rows_after, cols_after = get_data_stats(current_data)\n\n        # Resampling Metrics (Calculated from data)\n        if transformer_type in [\"Oversampling\", \"Undersampling\"]:\n            try:\n                # Extract y to calculate class counts\n                y_res = None\n                if isinstance(current_data, SplitDataset):\n                    if isinstance(current_data.train, tuple):\n                        _, y_res = current_data.train\n                    elif isinstance(current_data.train, pd.DataFrame):\n                        # Try to find target column from params\n                        target_col = params.get(\"target_column\")\n                        if target_col and target_col in current_data.train.columns:\n                            y_res = current_data.train[target_col]\n                elif isinstance(current_data, tuple):\n                    _, y_res = current_data\n                elif isinstance(current_data, pd.DataFrame):\n                    target_col = params.get(\"target_column\")\n                    if target_col and target_col in current_data.columns:\n                        y_res = current_data[target_col]\n\n                if y_res is not None:\n                    counts = y_res.value_counts().to_dict()\n                    # Convert keys to string to ensure JSON serializability\n                    metrics[\"class_counts\"] = {\n                        str(k): int(v) for k, v in counts.items()\n                    }\n                    metrics[\"total_samples\"] = int(len(y_res))\n            except Exception as e:\n                logger.warning(f\"Failed to calculate resampling metrics: {e}\")\n\n        if rows_after &gt; 0 or cols_after:\n            if transformer_type in [\n                \"DropMissingRows\",\n                \"Deduplicate\",\n                \"IQR\",\n                \"ZScore\",\n                \"EllipticEnvelope\",\n                \"Winsorize\",\n            ]:\n                dropped = rows_before - rows_after\n                metrics[f\"{transformer_type}_rows_removed\"] = dropped\n                metrics[f\"{transformer_type}_rows_remaining\"] = rows_after\n                metrics[f\"{transformer_type}_rows_total\"] = rows_before\n                metrics[\"rows_removed\"] = dropped\n                metrics[\"rows_total\"] = rows_before\n\n                # Special metric for Winsorize: Values Clipped\n                if transformer_type == \"Winsorize\":\n                    try:\n                        clipped_count = 0\n\n                        # Helper to count diffs\n                        def count_diffs(df1, df2):\n                            if isinstance(df1, pd.DataFrame) and isinstance(\n                                df2, pd.DataFrame\n                            ):\n                                if df1.shape == df2.shape:\n                                    return int(df1.ne(df2).sum().sum())\n                            elif (\n                                isinstance(df1, tuple)\n                                and isinstance(df2, tuple)\n                                and len(df1) == 2\n                                and len(df2) == 2\n                            ):\n                                # Handle (X, y) tuple\n                                diffs = 0\n                                # Compare X (index 0)\n                                if isinstance(df1[0], pd.DataFrame) and isinstance(\n                                    df2[0], pd.DataFrame\n                                ):\n                                    if df1[0].shape == df2[0].shape:\n                                        diffs += int(df1[0].ne(df2[0]).sum().sum())\n                                # Compare y (index 1) - usually Series\n                                if isinstance(\n                                    df1[1], (pd.DataFrame, pd.Series)\n                                ) and isinstance(df2[1], (pd.DataFrame, pd.Series)):\n                                    if df1[1].shape == df2[1].shape:\n                                        diffs += int(df1[1].ne(df2[1]).sum().sum())  # type: ignore\n                                return diffs\n                            return 0\n\n                        if isinstance(data_before, pd.DataFrame) and isinstance(\n                            current_data, pd.DataFrame\n                        ):\n                            clipped_count = count_diffs(data_before, current_data)\n                        elif isinstance(data_before, SplitDataset) and isinstance(\n                            current_data, SplitDataset\n                        ):\n                            clipped_count += count_diffs(\n                                data_before.train, current_data.train\n                            )\n                            clipped_count += count_diffs(\n                                data_before.test, current_data.test\n                            )\n                            clipped_count += count_diffs(\n                                data_before.validation, current_data.validation\n                            )\n\n                        metrics[\"values_clipped\"] = clipped_count\n                    except Exception as e:\n                        logger.warning(\n                            f\"Failed to calculate values_clipped for Winsorize: {e}\"\n                        )\n                        pass\n\n            if transformer_type == \"MissingIndicator\":\n                new_cols_set = cols_after - cols_before\n                metrics[\"missing_indicators_created\"] = len(new_cols_set)\n                cast(Dict[str, Any], metrics)[\"missing_indicators_columns\"] = list(\n                    new_cols_set\n                )\n\n            if transformer_type == \"DropMissingColumns\":\n                dropped_cols_set = cols_before - cols_after\n                cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                    dropped_cols_set\n                )\n                metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n            if transformer_type == \"feature_selection\":\n                dropped_cols_set = cols_before - cols_after\n                cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                    dropped_cols_set\n                )\n                metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n            if transformer_type in [\n                \"OneHotEncoder\",\n                \"LabelEncoder\",\n                \"OrdinalEncoder\",\n                \"TargetEncoder\",\n                \"HashEncoder\",\n                \"DummyEncoder\",\n            ]:\n                new_cols_set = cols_after - cols_before\n                metrics[\"new_features_count\"] = len(new_cols_set)\n                metrics[\"encoded_columns_count\"] = len(params.get(\"columns\", []))\n\n                if \"categories_count\" in params:\n                    metrics[\"categories_count\"] = params[\"categories_count\"]\n                if \"classes_count\" in params:\n                    metrics[\"classes_count\"] = params[\"classes_count\"]\n\n    return current_data, metrics\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.FeatureEngineer.transform","title":"<code>transform(data)</code>","text":"<p>Apply fitted transformations to new data.</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>def transform(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply fitted transformations to new data.\n    \"\"\"\n    current_data = data\n\n    for step in self.fitted_steps:\n        name = step[\"name\"]\n        transformer_type = step[\"type\"]\n        applier = step[\"applier\"]\n        artifact = step[\"artifact\"]\n\n        # Skip splitters during inference/transform\n        if transformer_type in [\n            \"TrainTestSplitter\",\n            \"feature_target_split\",\n            \"Oversampling\",\n            \"Undersampling\",\n        ]:\n            continue\n\n        logger.debug(f\"Applying step: {name} ({transformer_type})\")\n        current_data = applier.apply(current_data, artifact)\n\n    return current_data\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.GeneralBinningCalculator","title":"<code>GeneralBinningCalculator</code>","text":"<p>               Bases: <code>BaseCalculator</code></p> <p>Master calculator that handles mixed strategies and overrides.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>class GeneralBinningCalculator(BaseCalculator):\n    \"\"\"\n    Master calculator that handles mixed strategies and overrides.\n    \"\"\"\n\n    def fit(  # noqa: C901\n        self, df: Union[pd.DataFrame, tuple], config: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n        columns = resolve_columns(X, config, detect_numeric_columns)\n\n        global_strategy = config.get(\"strategy\", \"equal_width\")\n        column_strategies = config.get(\"column_strategies\", {})\n\n        # Global settings\n        default_n_bins = config.get(\"n_bins\", 5)\n        n_bins_global = config.get(\"equal_width_bins\", default_n_bins)\n        q_bins_global = config.get(\"equal_frequency_bins\", default_n_bins)\n        duplicates_global = config.get(\"duplicates\", \"drop\")\n\n        valid_cols = [c for c in columns if c in X.columns]\n        bin_edges_map = {}\n        custom_labels_map = {}\n\n        for col in valid_cols:\n            # Determine strategy and params for this column\n            override = column_strategies.get(col, {})\n            strategy = override.get(\"strategy\", global_strategy)\n\n            try:\n                series = X[col].dropna()\n                if series.empty:\n                    continue\n\n                edges = None\n\n                if strategy == \"equal_width\":\n                    n_bins = override.get(\"equal_width_bins\", n_bins_global)\n                    _, edges = pd.cut(series, bins=n_bins, retbins=True)\n                    # Clamp first edge to min if it was extended\n                    if len(edges) &gt; 0 and edges[0] &lt; series.min():\n                        edges[0] = series.min()\n\n                elif strategy == \"equal_frequency\":\n                    n_bins = override.get(\"equal_frequency_bins\", q_bins_global)\n                    duplicates = override.get(\"duplicates\", duplicates_global)\n                    _, edges = pd.qcut(\n                        series, q=n_bins, retbins=True, duplicates=duplicates\n                    )\n                    # Clamp first edge to min if it was extended\n                    if len(edges) &gt; 0 and edges[0] &lt; series.min():\n                        edges[0] = series.min()\n\n                elif strategy == \"kmeans\":\n                    n_bins = override.get(\"n_bins\", default_n_bins)\n                    est = KBinsDiscretizer(\n                        n_bins=n_bins, strategy=\"kmeans\", encode=\"ordinal\"\n                    )\n                    est.fit(series.values.reshape(-1, 1))  # type: ignore\n                    edges = est.bin_edges_[0]\n\n                elif strategy == \"custom\":\n                    # Check override first, then global custom_bins\n                    custom_bins = override.get(\"custom_bins\")\n                    if not custom_bins:\n                        custom_bins = config.get(\"custom_bins\", {}).get(col)\n\n                    if custom_bins:\n                        edges = np.array(sorted(custom_bins))\n\n                    # Handle custom labels\n                    labels = override.get(\"custom_labels\")\n                    if not labels:\n                        labels = config.get(\"custom_labels\", {}).get(col)\n                    if labels:\n                        custom_labels_map[col] = labels\n\n                elif strategy == \"kbins\":\n                    n_bins = override.get(\n                        \"kbins_n_bins\", config.get(\"kbins_n_bins\", default_n_bins)\n                    )\n                    k_strategy = override.get(\n                        \"kbins_strategy\", config.get(\"kbins_strategy\", \"quantile\")\n                    )\n\n                    # Map strategy names\n                    sklearn_strategy = k_strategy\n                    if k_strategy == \"equal_width\":\n                        sklearn_strategy = \"uniform\"\n                    elif k_strategy == \"equal_frequency\":\n                        sklearn_strategy = \"quantile\"\n\n                    est = KBinsDiscretizer(\n                        n_bins=n_bins, strategy=sklearn_strategy, encode=\"ordinal\"\n                    )\n                    est.fit(series.values.reshape(-1, 1))  # type: ignore\n                    edges = est.bin_edges_[0]\n\n                if edges is not None:\n                    bin_edges_map[col] = edges.tolist()\n\n            except Exception:\n                continue\n\n        return {\n            \"type\": \"general_binning\",\n            \"bin_edges\": bin_edges_map,\n            \"custom_labels\": custom_labels_map,\n            \"output_suffix\": config.get(\"output_suffix\", \"_binned\"),\n            \"drop_original\": config.get(\"drop_original\", False),\n            \"label_format\": config.get(\"label_format\", \"ordinal\"),\n            \"missing_strategy\": config.get(\"missing_strategy\", \"keep\"),\n            \"missing_label\": config.get(\"missing_label\", \"Missing\"),\n            \"include_lowest\": config.get(\"include_lowest\", True),\n            \"precision\": config.get(\"precision\", 3),\n        }\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.KBinsDiscretizerCalculator","title":"<code>KBinsDiscretizerCalculator</code>","text":"<p>               Bases: <code>GeneralBinningCalculator</code></p> <p>Calculator for KBinsDiscretizer node. Wraps GeneralBinningCalculator with kbins strategy.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>class KBinsDiscretizerCalculator(GeneralBinningCalculator):\n    \"\"\"\n    Calculator for KBinsDiscretizer node.\n    Wraps GeneralBinningCalculator with kbins strategy.\n    \"\"\"\n\n    def fit(\n        self,\n        df: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        new_config = config.copy()\n        new_config[\"strategy\"] = \"kbins\"\n\n        # Map sklearn params to GeneralBinning params\n        if \"n_bins\" in config:\n            new_config[\"kbins_n_bins\"] = config[\"n_bins\"]\n\n        # sklearn strategy: uniform, quantile, kmeans\n        # GeneralBinning kbins_strategy: same\n        if \"strategy\" in config and config[\"strategy\"] != \"kbins\":\n            new_config[\"kbins_strategy\"] = config[\"strategy\"]\n\n        return super().fit(df, new_config)\n</code></pre>"},{"location":"reference/api/preprocessing/base.html","title":"API: preprocessing.base","text":""},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base","title":"<code>skyulf.preprocessing.base</code>","text":""},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base.BaseApplier","title":"<code>BaseApplier</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>class BaseApplier(ABC):\n    @abstractmethod\n    def apply(\n        self, df: Union[pd.DataFrame, tuple], params: Dict[str, Any]\n    ) -&gt; Union[pd.DataFrame, tuple, SplitDataset]:\n        \"\"\"\n        Applies the transformation using fitted parameters.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base.BaseApplier.apply","title":"<code>apply(df, params)</code>  <code>abstractmethod</code>","text":"<p>Applies the transformation using fitted parameters.</p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>@abstractmethod\ndef apply(\n    self, df: Union[pd.DataFrame, tuple], params: Dict[str, Any]\n) -&gt; Union[pd.DataFrame, tuple, SplitDataset]:\n    \"\"\"\n    Applies the transformation using fitted parameters.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base.BaseCalculator","title":"<code>BaseCalculator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>class BaseCalculator(ABC):\n    @abstractmethod\n    def fit(\n        self, df: Union[pd.DataFrame, tuple], config: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Calculates parameters from the training data.\n        Returns a dictionary of fitted parameters (serializable).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base.BaseCalculator.fit","title":"<code>fit(df, config)</code>  <code>abstractmethod</code>","text":"<p>Calculates parameters from the training data. Returns a dictionary of fitted parameters (serializable).</p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>@abstractmethod\ndef fit(\n    self, df: Union[pd.DataFrame, tuple], config: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Calculates parameters from the training data.\n    Returns a dictionary of fitted parameters (serializable).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/preprocessing/bucketing.html","title":"API: preprocessing.bucketing","text":""},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing","title":"<code>skyulf.preprocessing.bucketing</code>","text":""},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing.BaseBinningApplier","title":"<code>BaseBinningApplier</code>","text":"<p>               Bases: <code>BaseApplier</code></p> <p>Base class for applying binning transformations. Expects 'bin_edges' in params: Dict[str, List[float]] mapping column names to bin edges.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>class BaseBinningApplier(BaseApplier):\n    \"\"\"\n    Base class for applying binning transformations.\n    Expects 'bin_edges' in params: Dict[str, List[float]] mapping column names to bin edges.\n    \"\"\"\n\n    def apply(  # noqa: C901\n        self,\n        df: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        params: Dict[str, Any],\n    ) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]]:\n        X, y, is_tuple = unpack_pipeline_input(df)\n\n        bin_edges_map = params.get(\"bin_edges\", {})\n        if not bin_edges_map:\n            return pack_pipeline_output(X, y, is_tuple)\n\n        output_suffix = params.get(\"output_suffix\", \"_binned\")\n        drop_original = params.get(\"drop_original\", False)\n        label_format = params.get(\n            \"label_format\", \"ordinal\"\n        )  # ordinal, range, bin_index\n        missing_strategy = params.get(\"missing_strategy\", \"keep\")  # keep, label\n        missing_label = params.get(\"missing_label\", \"Missing\")\n        include_lowest = params.get(\"include_lowest\", True)\n        precision = params.get(\"precision\", 3)\n        custom_labels_map = params.get(\"custom_labels\", {})\n\n        df_out = X.copy()\n        processed_cols = []\n\n        for col, edges in bin_edges_map.items():\n            if col not in df_out.columns:\n                continue\n\n            processed_cols.append(col)\n\n            # Determine labels for pd.cut\n            labels: Union[Literal[False], List[Any], None] = (\n                False  # Default for ordinal (returns integers)\n            )\n\n            # Check for custom labels first\n            col_custom_labels = custom_labels_map.get(col)\n            if col_custom_labels and len(col_custom_labels) == len(edges) - 1:\n                labels = col_custom_labels\n            elif label_format == \"range\":\n                labels = None  # Returns intervals\n            elif label_format == \"bin_index\":\n                labels = False  # Returns integers 0..n-1\n\n            # Apply cut\n            try:\n                # Ensure edges are unique and sorted\n                sorted_edges = sorted(list(set(edges)))\n                if len(sorted_edges) &lt; 2:\n                    continue\n\n                binned_series = pd.cut(\n                    df_out[col],\n                    bins=sorted_edges,\n                    labels=labels,\n                    include_lowest=include_lowest,\n                )\n\n                # Handle missing values\n                if missing_strategy == \"label\":\n                    # If categorical (range or custom labels), add category\n                    if isinstance(binned_series.dtype, pd.CategoricalDtype):\n                        if missing_label not in binned_series.cat.categories:\n                            binned_series = binned_series.cat.add_categories(\n                                [missing_label]\n                            )\n                        binned_series = binned_series.fillna(missing_label)\n                    else:\n                        # If numeric (ordinal/bin_index), we convert to object/str to support \"Missing\" label\n                        binned_series = binned_series.astype(object).fillna(\n                            missing_label\n                        )\n\n                # Format ranges if needed\n                if label_format == \"range\" and labels is None:\n                    # Convert intervals to string with precision\n                    if isinstance(binned_series.dtype, pd.CategoricalDtype):\n                        # It's a categorical of intervals\n                        def format_interval(iv):\n                            if pd.isna(iv) or isinstance(iv, str):\n                                return iv\n\n                            # Use the logical left edge if it's the first bin and include_lowest is True\n                            left_val = iv.left\n                            if (\n                                include_lowest\n                                and len(sorted_edges) &gt; 0\n                                and left_val &lt; sorted_edges[0]\n                            ):\n                                left_val = sorted_edges[0]\n\n                            l_val = round(left_val, precision)\n                            r_val = round(iv.right, precision)\n                            if include_lowest:\n                                return f\"[{l_val}, {r_val}]\"\n                            else:\n                                return f\"({l_val}, {r_val}]\"\n\n                        # We need to map the categories themselves\n                        new_categories = [\n                            format_interval(c) for c in binned_series.cat.categories\n                        ]\n                        binned_series = binned_series.cat.rename_categories(\n                            new_categories\n                        )\n                        binned_series = binned_series.astype(str)\n                    else:\n                        binned_series = binned_series.astype(str)\n\n                    if missing_strategy == \"keep\":\n                        # Restore NaNs if they were converted to 'nan' string\n                        binned_series = binned_series.replace(\"nan\", np.nan)\n\n                out_col = f\"{col}{output_suffix}\"\n                df_out[out_col] = binned_series\n\n            except Exception:\n                # Log error or skip\n                pass\n\n        if drop_original:\n            df_out = df_out.drop(columns=processed_cols)\n\n        return pack_pipeline_output(df_out, y, is_tuple)\n</code></pre>"},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing.CustomBinningCalculator","title":"<code>CustomBinningCalculator</code>","text":"<p>               Bases: <code>BaseCalculator</code></p> <p>Calculator for CustomBinning node. Applies specific bin edges to selected columns.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>class CustomBinningCalculator(BaseCalculator):\n    \"\"\"\n    Calculator for CustomBinning node.\n    Applies specific bin edges to selected columns.\n    \"\"\"\n\n    def fit(\n        self,\n        df: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n        columns = resolve_columns(X, config, detect_numeric_columns)\n        bins = config.get(\"bins\")\n\n        bin_edges_map = {}\n        if bins:\n            sorted_bins = sorted(bins)\n            for col in columns:\n                if col in X.columns:\n                    bin_edges_map[col] = sorted_bins\n\n        return {\n            \"type\": \"general_binning\",  # Use GeneralBinningApplier\n            \"bin_edges\": bin_edges_map,\n            \"output_suffix\": config.get(\"output_suffix\", \"_binned\"),\n            \"drop_original\": config.get(\"drop_original\", False),\n            \"label_format\": config.get(\"label_format\", \"ordinal\"),\n            \"missing_strategy\": config.get(\"missing_strategy\", \"keep\"),\n            \"missing_label\": config.get(\"missing_label\", \"Missing\"),\n            \"include_lowest\": config.get(\"include_lowest\", True),\n            \"precision\": config.get(\"precision\", 3),\n        }\n</code></pre>"},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing.GeneralBinningCalculator","title":"<code>GeneralBinningCalculator</code>","text":"<p>               Bases: <code>BaseCalculator</code></p> <p>Master calculator that handles mixed strategies and overrides.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>class GeneralBinningCalculator(BaseCalculator):\n    \"\"\"\n    Master calculator that handles mixed strategies and overrides.\n    \"\"\"\n\n    def fit(  # noqa: C901\n        self, df: Union[pd.DataFrame, tuple], config: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n        columns = resolve_columns(X, config, detect_numeric_columns)\n\n        global_strategy = config.get(\"strategy\", \"equal_width\")\n        column_strategies = config.get(\"column_strategies\", {})\n\n        # Global settings\n        default_n_bins = config.get(\"n_bins\", 5)\n        n_bins_global = config.get(\"equal_width_bins\", default_n_bins)\n        q_bins_global = config.get(\"equal_frequency_bins\", default_n_bins)\n        duplicates_global = config.get(\"duplicates\", \"drop\")\n\n        valid_cols = [c for c in columns if c in X.columns]\n        bin_edges_map = {}\n        custom_labels_map = {}\n\n        for col in valid_cols:\n            # Determine strategy and params for this column\n            override = column_strategies.get(col, {})\n            strategy = override.get(\"strategy\", global_strategy)\n\n            try:\n                series = X[col].dropna()\n                if series.empty:\n                    continue\n\n                edges = None\n\n                if strategy == \"equal_width\":\n                    n_bins = override.get(\"equal_width_bins\", n_bins_global)\n                    _, edges = pd.cut(series, bins=n_bins, retbins=True)\n                    # Clamp first edge to min if it was extended\n                    if len(edges) &gt; 0 and edges[0] &lt; series.min():\n                        edges[0] = series.min()\n\n                elif strategy == \"equal_frequency\":\n                    n_bins = override.get(\"equal_frequency_bins\", q_bins_global)\n                    duplicates = override.get(\"duplicates\", duplicates_global)\n                    _, edges = pd.qcut(\n                        series, q=n_bins, retbins=True, duplicates=duplicates\n                    )\n                    # Clamp first edge to min if it was extended\n                    if len(edges) &gt; 0 and edges[0] &lt; series.min():\n                        edges[0] = series.min()\n\n                elif strategy == \"kmeans\":\n                    n_bins = override.get(\"n_bins\", default_n_bins)\n                    est = KBinsDiscretizer(\n                        n_bins=n_bins, strategy=\"kmeans\", encode=\"ordinal\"\n                    )\n                    est.fit(series.values.reshape(-1, 1))  # type: ignore\n                    edges = est.bin_edges_[0]\n\n                elif strategy == \"custom\":\n                    # Check override first, then global custom_bins\n                    custom_bins = override.get(\"custom_bins\")\n                    if not custom_bins:\n                        custom_bins = config.get(\"custom_bins\", {}).get(col)\n\n                    if custom_bins:\n                        edges = np.array(sorted(custom_bins))\n\n                    # Handle custom labels\n                    labels = override.get(\"custom_labels\")\n                    if not labels:\n                        labels = config.get(\"custom_labels\", {}).get(col)\n                    if labels:\n                        custom_labels_map[col] = labels\n\n                elif strategy == \"kbins\":\n                    n_bins = override.get(\n                        \"kbins_n_bins\", config.get(\"kbins_n_bins\", default_n_bins)\n                    )\n                    k_strategy = override.get(\n                        \"kbins_strategy\", config.get(\"kbins_strategy\", \"quantile\")\n                    )\n\n                    # Map strategy names\n                    sklearn_strategy = k_strategy\n                    if k_strategy == \"equal_width\":\n                        sklearn_strategy = \"uniform\"\n                    elif k_strategy == \"equal_frequency\":\n                        sklearn_strategy = \"quantile\"\n\n                    est = KBinsDiscretizer(\n                        n_bins=n_bins, strategy=sklearn_strategy, encode=\"ordinal\"\n                    )\n                    est.fit(series.values.reshape(-1, 1))  # type: ignore\n                    edges = est.bin_edges_[0]\n\n                if edges is not None:\n                    bin_edges_map[col] = edges.tolist()\n\n            except Exception:\n                continue\n\n        return {\n            \"type\": \"general_binning\",\n            \"bin_edges\": bin_edges_map,\n            \"custom_labels\": custom_labels_map,\n            \"output_suffix\": config.get(\"output_suffix\", \"_binned\"),\n            \"drop_original\": config.get(\"drop_original\", False),\n            \"label_format\": config.get(\"label_format\", \"ordinal\"),\n            \"missing_strategy\": config.get(\"missing_strategy\", \"keep\"),\n            \"missing_label\": config.get(\"missing_label\", \"Missing\"),\n            \"include_lowest\": config.get(\"include_lowest\", True),\n            \"precision\": config.get(\"precision\", 3),\n        }\n</code></pre>"},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing.KBinsDiscretizerCalculator","title":"<code>KBinsDiscretizerCalculator</code>","text":"<p>               Bases: <code>GeneralBinningCalculator</code></p> <p>Calculator for KBinsDiscretizer node. Wraps GeneralBinningCalculator with kbins strategy.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>class KBinsDiscretizerCalculator(GeneralBinningCalculator):\n    \"\"\"\n    Calculator for KBinsDiscretizer node.\n    Wraps GeneralBinningCalculator with kbins strategy.\n    \"\"\"\n\n    def fit(\n        self,\n        df: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        new_config = config.copy()\n        new_config[\"strategy\"] = \"kbins\"\n\n        # Map sklearn params to GeneralBinning params\n        if \"n_bins\" in config:\n            new_config[\"kbins_n_bins\"] = config[\"n_bins\"]\n\n        # sklearn strategy: uniform, quantile, kmeans\n        # GeneralBinning kbins_strategy: same\n        if \"strategy\" in config and config[\"strategy\"] != \"kbins\":\n            new_config[\"kbins_strategy\"] = config[\"strategy\"]\n\n        return super().fit(df, new_config)\n</code></pre>"},{"location":"reference/api/preprocessing/casting.html","title":"API: preprocessing.casting","text":""},{"location":"reference/api/preprocessing/casting.html#skyulf.preprocessing.casting","title":"<code>skyulf.preprocessing.casting</code>","text":""},{"location":"reference/api/preprocessing/cleaning.html","title":"API: preprocessing.cleaning","text":""},{"location":"reference/api/preprocessing/cleaning.html#skyulf.preprocessing.cleaning","title":"<code>skyulf.preprocessing.cleaning</code>","text":""},{"location":"reference/api/preprocessing/drop_and_missing.html","title":"API: preprocessing.drop_and_missing","text":""},{"location":"reference/api/preprocessing/drop_and_missing.html#skyulf.preprocessing.drop_and_missing","title":"<code>skyulf.preprocessing.drop_and_missing</code>","text":""},{"location":"reference/api/preprocessing/encoding.html","title":"API: preprocessing.encoding","text":""},{"location":"reference/api/preprocessing/encoding.html#skyulf.preprocessing.encoding","title":"<code>skyulf.preprocessing.encoding</code>","text":""},{"location":"reference/api/preprocessing/feature_generation.html","title":"API: preprocessing.feature_generation","text":""},{"location":"reference/api/preprocessing/feature_generation.html#skyulf.preprocessing.feature_generation","title":"<code>skyulf.preprocessing.feature_generation</code>","text":""},{"location":"reference/api/preprocessing/feature_selection.html","title":"API: preprocessing.feature_selection","text":""},{"location":"reference/api/preprocessing/feature_selection.html#skyulf.preprocessing.feature_selection","title":"<code>skyulf.preprocessing.feature_selection</code>","text":""},{"location":"reference/api/preprocessing/imputation.html","title":"API: preprocessing.imputation","text":""},{"location":"reference/api/preprocessing/imputation.html#skyulf.preprocessing.imputation","title":"<code>skyulf.preprocessing.imputation</code>","text":""},{"location":"reference/api/preprocessing/inspection.html","title":"API: preprocessing.inspection","text":""},{"location":"reference/api/preprocessing/inspection.html#skyulf.preprocessing.inspection","title":"<code>skyulf.preprocessing.inspection</code>","text":""},{"location":"reference/api/preprocessing/outliers.html","title":"API: preprocessing.outliers","text":""},{"location":"reference/api/preprocessing/outliers.html#skyulf.preprocessing.outliers","title":"<code>skyulf.preprocessing.outliers</code>","text":""},{"location":"reference/api/preprocessing/pipeline.html","title":"API: preprocessing.pipeline","text":""},{"location":"reference/api/preprocessing/pipeline.html#skyulf.preprocessing.pipeline","title":"<code>skyulf.preprocessing.pipeline</code>","text":"<p>Feature Engineering Pipeline Orchestrator.</p>"},{"location":"reference/api/preprocessing/pipeline.html#skyulf.preprocessing.pipeline.FeatureEngineer","title":"<code>FeatureEngineer</code>","text":"<p>Orchestrates a sequence of feature engineering steps.</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>class FeatureEngineer:\n    \"\"\"\n    Orchestrates a sequence of feature engineering steps.\n    \"\"\"\n\n    def __init__(self, steps_config: List[Dict[str, Any]]):\n        self.steps_config = steps_config\n        self.fitted_steps: List[Dict[str, Any]] = []\n\n    def transform(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply fitted transformations to new data.\n        \"\"\"\n        current_data = data\n\n        for step in self.fitted_steps:\n            name = step[\"name\"]\n            transformer_type = step[\"type\"]\n            applier = step[\"applier\"]\n            artifact = step[\"artifact\"]\n\n            # Skip splitters during inference/transform\n            if transformer_type in [\n                \"TrainTestSplitter\",\n                \"feature_target_split\",\n                \"Oversampling\",\n                \"Undersampling\",\n            ]:\n                continue\n\n            logger.debug(f\"Applying step: {name} ({transformer_type})\")\n            current_data = applier.apply(current_data, artifact)\n\n        return current_data\n\n    def fit_transform(self, data: Union[pd.DataFrame, Any], node_id_prefix=\"\") -&gt; Any:  # noqa: C901\n        \"\"\"\n        Runs the pipeline on data.\n        Returns: (transformed_data, metrics_dict)\n        \"\"\"\n        self.fitted_steps = []  # Reset fitted steps\n        current_data = data\n        metrics: Dict[str, Any] = {}\n\n        for i, step in enumerate(self.steps_config):\n            name = step[\"name\"]\n            transformer_type = step[\"transformer\"]\n            params = step.get(\"params\", {})\n\n            logger.info(f\"Running step {i}: {name} ({transformer_type})\")\n            logger.debug(\n                f\"FeatureEngineer running step {i}: {name} ({transformer_type})\"\n            )\n            logger.debug(f\"current_data type: {type(current_data)}\")\n\n            # Capture metrics before\n            rows_before, cols_before = get_data_stats(current_data)\n\n            # Keep reference for comparison (for Winsorize metrics)\n            data_before = current_data\n\n            calculator, applier = self._get_transformer_components(transformer_type)\n\n            # We need a unique ID for this step's artifacts\n            step_node_id = f\"{node_id_prefix}_{name}\"\n\n            transformer = StatefulTransformer(calculator, applier, step_node_id)\n\n            # Handle special transformers that change data structure\n            # Splitters return SplitDataset or (X, y) tuples instead of a simple DataFrame,\n            # so they bypass the standard StatefulTransformer wrapper.\n\n            # Initialize fitted_params\n            fitted_params = {}\n\n            if transformer_type == \"TrainTestSplitter\":\n                logger.debug(\"Handling TrainTestSplitter\")\n                # TrainTestSplitter changes DataFrame -&gt; SplitDataset.\n                # We bypass StatefulTransformer to allow this structural change.\n                # It can also handle (X, y) tuple if FeatureTargetSplit was done first.\n                if isinstance(current_data, (pd.DataFrame, tuple)):\n                    logger.debug(\"Executing TrainTestSplitter logic\")\n                    params = calculator.fit(current_data, params)\n                    current_data = applier.apply(current_data, params)\n                    # In SDK, params are returned but not auto-saved to artifact store here.\n                    # The Pipeline object will handle state persistence.\n                else:\n                    logger.debug(\n                        f\"Skipping TrainTestSplitter. current_data is {type(current_data)}\"\n                    )\n                    logger.warning(\n                        \"Attempting to split an already split dataset. Skipping TrainTestSplitter.\"\n                    )\n\n            elif transformer_type == \"feature_target_split\":\n                logger.debug(\"Handling feature_target_split\")\n                # FeatureTargetSplitter changes structure to (X, y) or Dict of (X, y).\n                # We bypass StatefulTransformer to allow this structural change.\n                params = calculator.fit(current_data, params)\n                current_data = applier.apply(current_data, params)\n\n            else:\n                logger.debug(\"Handling standard transformer via StatefulTransformer\")\n                current_data = transformer.fit_transform(current_data, params)\n                # In SDK, transformer.params holds the state.\n                fitted_params = transformer.params\n\n                self.fitted_steps.append(\n                    {\n                        \"name\": name,\n                        \"type\": transformer_type,\n                        \"applier\": applier,\n                        \"artifact\": fitted_params,\n                    }\n                )\n\n            logger.debug(f\"Step {i} complete. New data type: {type(current_data)}\")\n\n            # Retrieve fitted params to get metrics from the calculator\n            try:\n                if fitted_params:\n                    # Imputation Metrics\n                    if transformer_type in [\n                        \"SimpleImputer\",\n                        \"KNNImputer\",\n                        \"IterativeImputer\",\n                    ]:\n                        if \"missing_counts\" in fitted_params:\n                            metrics[\"missing_counts\"] = fitted_params[\"missing_counts\"]\n                        if \"total_missing\" in fitted_params:\n                            metrics[\"total_missing\"] = fitted_params[\"total_missing\"]\n                        if \"fill_values\" in fitted_params:\n                            metrics[\"fill_values\"] = fitted_params[\"fill_values\"]\n\n                    # Feature Selection Metrics\n                    if transformer_type in [\n                        \"feature_selection\",\n                        \"UnivariateSelection\",\n                        \"ModelBasedSelection\",\n                        \"VarianceThreshold\",\n                    ]:\n                        if \"feature_scores\" in fitted_params:\n                            metrics[\"feature_scores\"] = fitted_params[\"feature_scores\"]\n                        if \"p_values\" in fitted_params:\n                            metrics[\"p_values\"] = fitted_params[\"p_values\"]\n                        if \"feature_importances\" in fitted_params:\n                            metrics[\"feature_importances\"] = fitted_params[\n                                \"feature_importances\"\n                            ]\n                        if \"variances\" in fitted_params:\n                            metrics[\"variances\"] = fitted_params[\"variances\"]\n                        if \"ranking\" in fitted_params:\n                            metrics[\"ranking\"] = fitted_params[\"ranking\"]\n                        if \"selected_columns\" in fitted_params:\n                            metrics[\"selected_columns\"] = fitted_params[\n                                \"selected_columns\"\n                            ]\n\n                    # Scaling Metrics\n                    if transformer_type in [\n                        \"StandardScaler\",\n                        \"MinMaxScaler\",\n                        \"RobustScaler\",\n                        \"MaxAbsScaler\",\n                    ]:\n                        if \"mean\" in fitted_params:\n                            metrics[\"mean\"] = fitted_params[\"mean\"]\n                        if \"scale\" in fitted_params:\n                            metrics[\"scale\"] = fitted_params[\"scale\"]\n                        if \"var\" in fitted_params:\n                            metrics[\"var\"] = fitted_params[\"var\"]\n                        if \"min\" in fitted_params:\n                            metrics[\"min\"] = fitted_params[\"min\"]\n                        if \"data_min\" in fitted_params:\n                            metrics[\"data_min\"] = fitted_params[\"data_min\"]\n                        if \"data_max\" in fitted_params:\n                            metrics[\"data_max\"] = fitted_params[\"data_max\"]\n                        if \"center\" in fitted_params:\n                            metrics[\"center\"] = fitted_params[\"center\"]\n                        if \"max_abs\" in fitted_params:\n                            metrics[\"max_abs\"] = fitted_params[\"max_abs\"]\n                        if \"columns\" in fitted_params:\n                            metrics[\"columns\"] = fitted_params[\"columns\"]\n\n                    # Outlier Metrics\n                    if transformer_type in [\n                        \"IQR\",\n                        \"Winsorize\",\n                        \"ZScore\",\n                        \"EllipticEnvelope\",\n                    ]:\n                        if \"warnings\" in fitted_params:\n                            metrics[\"warnings\"] = fitted_params[\"warnings\"]\n\n                    if transformer_type in [\"IQR\", \"Winsorize\"]:\n                        if \"bounds\" in fitted_params:\n                            metrics[\"bounds\"] = fitted_params[\"bounds\"]\n\n                    if transformer_type == \"ZScore\":\n                        if \"stats\" in fitted_params:\n                            metrics[\"stats\"] = fitted_params[\"stats\"]\n\n                    if transformer_type == \"EllipticEnvelope\":\n                        if \"contamination\" in fitted_params:\n                            metrics[\"contamination\"] = fitted_params[\"contamination\"]\n\n                    # Bucketing Metrics\n                    if transformer_type in [\n                        \"GeneralBinning\",\n                        \"EqualWidthBinning\",\n                        \"EqualFrequencyBinning\",\n                        \"CustomBinning\",\n                        \"KBinsDiscretizer\",\n                    ]:\n                        if \"bin_edges\" in fitted_params:\n                            metrics[\"bin_edges\"] = fitted_params[\"bin_edges\"]\n                        if \"n_bins\" in fitted_params:\n                            metrics[\"n_bins\"] = fitted_params[\"n_bins\"]\n\n                    # Feature Generation Metrics\n                    if transformer_type in [\"FeatureMath\", \"FeatureGenerationNode\"]:\n                        if \"operations\" in fitted_params:\n                            metrics[\"operations_count\"] = len(\n                                fitted_params[\"operations\"]\n                            )\n                            metrics[\"operations\"] = fitted_params[\"operations\"]\n                        # Calculate generated features by comparing columns\n                        if isinstance(data_before, pd.DataFrame) and isinstance(\n                            current_data, pd.DataFrame\n                        ):\n                            new_cols = list(\n                                set(current_data.columns) - set(data_before.columns)\n                            )\n                            metrics[\"generated_features\"] = new_cols\n                        elif isinstance(data_before, SplitDataset) and isinstance(\n                            current_data, SplitDataset\n                        ):\n                            # Check train set\n                            if isinstance(\n                                data_before.train, pd.DataFrame\n                            ) and isinstance(current_data.train, pd.DataFrame):\n                                new_cols = list(\n                                    set(current_data.train.columns)\n                                    - set(data_before.train.columns)\n                                )\n                                metrics[\"generated_features\"] = new_cols\n                            elif isinstance(data_before.train, tuple) and isinstance(\n                                current_data.train, tuple\n                            ):\n                                # (X, y) tuple\n                                X_before, _ = data_before.train\n                                X_after, _ = current_data.train\n                                if isinstance(X_before, pd.DataFrame) and isinstance(\n                                    X_after, pd.DataFrame\n                                ):\n                                    new_cols = list(\n                                        set(X_after.columns) - set(X_before.columns)\n                                    )\n                                    metrics[\"generated_features\"] = new_cols\n\n            except Exception as e:\n                logger.warning(f\"Failed to retrieve metrics for step {name}: {e}\")\n\n            # Capture metrics after\n            rows_after, cols_after = get_data_stats(current_data)\n\n            # Resampling Metrics (Calculated from data)\n            if transformer_type in [\"Oversampling\", \"Undersampling\"]:\n                try:\n                    # Extract y to calculate class counts\n                    y_res = None\n                    if isinstance(current_data, SplitDataset):\n                        if isinstance(current_data.train, tuple):\n                            _, y_res = current_data.train\n                        elif isinstance(current_data.train, pd.DataFrame):\n                            # Try to find target column from params\n                            target_col = params.get(\"target_column\")\n                            if target_col and target_col in current_data.train.columns:\n                                y_res = current_data.train[target_col]\n                    elif isinstance(current_data, tuple):\n                        _, y_res = current_data\n                    elif isinstance(current_data, pd.DataFrame):\n                        target_col = params.get(\"target_column\")\n                        if target_col and target_col in current_data.columns:\n                            y_res = current_data[target_col]\n\n                    if y_res is not None:\n                        counts = y_res.value_counts().to_dict()\n                        # Convert keys to string to ensure JSON serializability\n                        metrics[\"class_counts\"] = {\n                            str(k): int(v) for k, v in counts.items()\n                        }\n                        metrics[\"total_samples\"] = int(len(y_res))\n                except Exception as e:\n                    logger.warning(f\"Failed to calculate resampling metrics: {e}\")\n\n            if rows_after &gt; 0 or cols_after:\n                if transformer_type in [\n                    \"DropMissingRows\",\n                    \"Deduplicate\",\n                    \"IQR\",\n                    \"ZScore\",\n                    \"EllipticEnvelope\",\n                    \"Winsorize\",\n                ]:\n                    dropped = rows_before - rows_after\n                    metrics[f\"{transformer_type}_rows_removed\"] = dropped\n                    metrics[f\"{transformer_type}_rows_remaining\"] = rows_after\n                    metrics[f\"{transformer_type}_rows_total\"] = rows_before\n                    metrics[\"rows_removed\"] = dropped\n                    metrics[\"rows_total\"] = rows_before\n\n                    # Special metric for Winsorize: Values Clipped\n                    if transformer_type == \"Winsorize\":\n                        try:\n                            clipped_count = 0\n\n                            # Helper to count diffs\n                            def count_diffs(df1, df2):\n                                if isinstance(df1, pd.DataFrame) and isinstance(\n                                    df2, pd.DataFrame\n                                ):\n                                    if df1.shape == df2.shape:\n                                        return int(df1.ne(df2).sum().sum())\n                                elif (\n                                    isinstance(df1, tuple)\n                                    and isinstance(df2, tuple)\n                                    and len(df1) == 2\n                                    and len(df2) == 2\n                                ):\n                                    # Handle (X, y) tuple\n                                    diffs = 0\n                                    # Compare X (index 0)\n                                    if isinstance(df1[0], pd.DataFrame) and isinstance(\n                                        df2[0], pd.DataFrame\n                                    ):\n                                        if df1[0].shape == df2[0].shape:\n                                            diffs += int(df1[0].ne(df2[0]).sum().sum())\n                                    # Compare y (index 1) - usually Series\n                                    if isinstance(\n                                        df1[1], (pd.DataFrame, pd.Series)\n                                    ) and isinstance(df2[1], (pd.DataFrame, pd.Series)):\n                                        if df1[1].shape == df2[1].shape:\n                                            diffs += int(df1[1].ne(df2[1]).sum().sum())  # type: ignore\n                                    return diffs\n                                return 0\n\n                            if isinstance(data_before, pd.DataFrame) and isinstance(\n                                current_data, pd.DataFrame\n                            ):\n                                clipped_count = count_diffs(data_before, current_data)\n                            elif isinstance(data_before, SplitDataset) and isinstance(\n                                current_data, SplitDataset\n                            ):\n                                clipped_count += count_diffs(\n                                    data_before.train, current_data.train\n                                )\n                                clipped_count += count_diffs(\n                                    data_before.test, current_data.test\n                                )\n                                clipped_count += count_diffs(\n                                    data_before.validation, current_data.validation\n                                )\n\n                            metrics[\"values_clipped\"] = clipped_count\n                        except Exception as e:\n                            logger.warning(\n                                f\"Failed to calculate values_clipped for Winsorize: {e}\"\n                            )\n                            pass\n\n                if transformer_type == \"MissingIndicator\":\n                    new_cols_set = cols_after - cols_before\n                    metrics[\"missing_indicators_created\"] = len(new_cols_set)\n                    cast(Dict[str, Any], metrics)[\"missing_indicators_columns\"] = list(\n                        new_cols_set\n                    )\n\n                if transformer_type == \"DropMissingColumns\":\n                    dropped_cols_set = cols_before - cols_after\n                    cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                        dropped_cols_set\n                    )\n                    metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n                if transformer_type == \"feature_selection\":\n                    dropped_cols_set = cols_before - cols_after\n                    cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                        dropped_cols_set\n                    )\n                    metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n                if transformer_type in [\n                    \"OneHotEncoder\",\n                    \"LabelEncoder\",\n                    \"OrdinalEncoder\",\n                    \"TargetEncoder\",\n                    \"HashEncoder\",\n                    \"DummyEncoder\",\n                ]:\n                    new_cols_set = cols_after - cols_before\n                    metrics[\"new_features_count\"] = len(new_cols_set)\n                    metrics[\"encoded_columns_count\"] = len(params.get(\"columns\", []))\n\n                    if \"categories_count\" in params:\n                        metrics[\"categories_count\"] = params[\"categories_count\"]\n                    if \"classes_count\" in params:\n                        metrics[\"classes_count\"] = params[\"classes_count\"]\n\n        return current_data, metrics\n\n    def _get_transformer_components(self, type_name: str):  # noqa: C901\n        if type_name == \"TrainTestSplitter\":\n            return SplitCalculator(), SplitApplier()\n        elif type_name == \"feature_target_split\":\n            return FeatureTargetSplitCalculator(), FeatureTargetSplitApplier()\n        elif type_name == \"TextCleaning\":\n            return TextCleaningCalculator(), TextCleaningApplier()\n        elif type_name == \"ValueReplacement\":\n            return ValueReplacementCalculator(), ValueReplacementApplier()\n        elif type_name == \"Deduplicate\":\n            return DeduplicateCalculator(), DeduplicateApplier()\n        elif type_name == \"DropMissingColumns\":\n            return DropMissingColumnsCalculator(), DropMissingColumnsApplier()\n        elif type_name == \"DropMissingRows\":\n            return DropMissingRowsCalculator(), DropMissingRowsApplier()\n        elif type_name == \"MissingIndicator\":\n            return MissingIndicatorCalculator(), MissingIndicatorApplier()\n        elif type_name == \"AliasReplacement\":\n            return AliasReplacementCalculator(), AliasReplacementApplier()\n        elif type_name == \"InvalidValueReplacement\":\n            return InvalidValueReplacementCalculator(), InvalidValueReplacementApplier()\n        elif type_name == \"SimpleImputer\":\n            return SimpleImputerCalculator(), SimpleImputerApplier()\n        elif type_name == \"KNNImputer\":\n            return KNNImputerCalculator(), KNNImputerApplier()\n        elif type_name == \"IterativeImputer\":\n            return IterativeImputerCalculator(), IterativeImputerApplier()\n        elif type_name == \"OneHotEncoder\":\n            return OneHotEncoderCalculator(), OneHotEncoderApplier()\n        elif type_name == \"DummyEncoder\":\n            from .encoding import DummyEncoderApplier, DummyEncoderCalculator\n\n            return DummyEncoderCalculator(), DummyEncoderApplier()\n        elif type_name == \"OrdinalEncoder\":\n            return OrdinalEncoderCalculator(), OrdinalEncoderApplier()\n        elif type_name == \"LabelEncoder\":\n            return LabelEncoderCalculator(), LabelEncoderApplier()\n        elif type_name == \"TargetEncoder\":\n            return TargetEncoderCalculator(), TargetEncoderApplier()\n        elif type_name == \"HashEncoder\":\n            return HashEncoderCalculator(), HashEncoderApplier()\n        elif type_name == \"StandardScaler\":\n            return StandardScalerCalculator(), StandardScalerApplier()\n        elif type_name == \"MinMaxScaler\":\n            return MinMaxScalerCalculator(), MinMaxScalerApplier()\n        elif type_name == \"RobustScaler\":\n            return RobustScalerCalculator(), RobustScalerApplier()\n        elif type_name == \"MaxAbsScaler\":\n            return MaxAbsScalerCalculator(), MaxAbsScalerApplier()\n        elif type_name == \"IQR\":\n            return IQRCalculator(), IQRApplier()\n        elif type_name == \"ZScore\":\n            return ZScoreCalculator(), ZScoreApplier()\n        elif type_name == \"Winsorize\":\n            return WinsorizeCalculator(), WinsorizeApplier()\n        elif type_name == \"ManualBounds\":\n            return ManualBoundsCalculator(), ManualBoundsApplier()\n        elif type_name == \"EllipticEnvelope\":\n            return EllipticEnvelopeCalculator(), EllipticEnvelopeApplier()\n        elif type_name == \"PowerTransformer\":\n            return PowerTransformerCalculator(), PowerTransformerApplier()\n        elif type_name == \"SimpleTransformation\":\n            return SimpleTransformationCalculator(), SimpleTransformationApplier()\n        elif type_name == \"GeneralTransformation\":\n            return GeneralTransformationCalculator(), GeneralTransformationApplier()\n        elif type_name == \"GeneralBinning\":\n            return GeneralBinningCalculator(), GeneralBinningApplier()\n        elif type_name == \"CustomBinning\":\n            return CustomBinningCalculator(), CustomBinningApplier()\n        elif type_name == \"KBinsDiscretizer\":\n            return KBinsDiscretizerCalculator(), KBinsDiscretizerApplier()\n        elif type_name == \"VarianceThreshold\":\n            return VarianceThresholdCalculator(), VarianceThresholdApplier()\n        elif type_name == \"CorrelationThreshold\":\n            return CorrelationThresholdCalculator(), CorrelationThresholdApplier()\n        elif type_name == \"UnivariateSelection\":\n            return UnivariateSelectionCalculator(), UnivariateSelectionApplier()\n        elif type_name == \"ModelBasedSelection\":\n            return ModelBasedSelectionCalculator(), ModelBasedSelectionApplier()\n        elif type_name == \"feature_selection\":\n            return FeatureSelectionCalculator(), FeatureSelectionApplier()\n        elif type_name == \"Casting\":\n            return CastingCalculator(), CastingApplier()\n        elif type_name == \"PolynomialFeatures\":\n            return PolynomialFeaturesCalculator(), PolynomialFeaturesApplier()\n        elif type_name == \"FeatureMath\" or type_name == \"FeatureGenerationNode\":\n            return FeatureGenerationCalculator(), FeatureGenerationApplier()\n        elif type_name == \"Oversampling\":\n            return OversamplingCalculator(), OversamplingApplier()\n        elif type_name == \"Undersampling\":\n            return UndersamplingCalculator(), UndersamplingApplier()\n        elif type_name == \"DatasetProfile\":\n            return DatasetProfileCalculator(), DatasetProfileApplier()\n        elif type_name == \"DataSnapshot\":\n            return DataSnapshotCalculator(), DataSnapshotApplier()\n        else:\n            raise ValueError(f\"Unknown transformer type: {type_name}\")\n</code></pre>"},{"location":"reference/api/preprocessing/pipeline.html#skyulf.preprocessing.pipeline.FeatureEngineer.fit_transform","title":"<code>fit_transform(data, node_id_prefix='')</code>","text":"<p>Runs the pipeline on data. Returns: (transformed_data, metrics_dict)</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>def fit_transform(self, data: Union[pd.DataFrame, Any], node_id_prefix=\"\") -&gt; Any:  # noqa: C901\n    \"\"\"\n    Runs the pipeline on data.\n    Returns: (transformed_data, metrics_dict)\n    \"\"\"\n    self.fitted_steps = []  # Reset fitted steps\n    current_data = data\n    metrics: Dict[str, Any] = {}\n\n    for i, step in enumerate(self.steps_config):\n        name = step[\"name\"]\n        transformer_type = step[\"transformer\"]\n        params = step.get(\"params\", {})\n\n        logger.info(f\"Running step {i}: {name} ({transformer_type})\")\n        logger.debug(\n            f\"FeatureEngineer running step {i}: {name} ({transformer_type})\"\n        )\n        logger.debug(f\"current_data type: {type(current_data)}\")\n\n        # Capture metrics before\n        rows_before, cols_before = get_data_stats(current_data)\n\n        # Keep reference for comparison (for Winsorize metrics)\n        data_before = current_data\n\n        calculator, applier = self._get_transformer_components(transformer_type)\n\n        # We need a unique ID for this step's artifacts\n        step_node_id = f\"{node_id_prefix}_{name}\"\n\n        transformer = StatefulTransformer(calculator, applier, step_node_id)\n\n        # Handle special transformers that change data structure\n        # Splitters return SplitDataset or (X, y) tuples instead of a simple DataFrame,\n        # so they bypass the standard StatefulTransformer wrapper.\n\n        # Initialize fitted_params\n        fitted_params = {}\n\n        if transformer_type == \"TrainTestSplitter\":\n            logger.debug(\"Handling TrainTestSplitter\")\n            # TrainTestSplitter changes DataFrame -&gt; SplitDataset.\n            # We bypass StatefulTransformer to allow this structural change.\n            # It can also handle (X, y) tuple if FeatureTargetSplit was done first.\n            if isinstance(current_data, (pd.DataFrame, tuple)):\n                logger.debug(\"Executing TrainTestSplitter logic\")\n                params = calculator.fit(current_data, params)\n                current_data = applier.apply(current_data, params)\n                # In SDK, params are returned but not auto-saved to artifact store here.\n                # The Pipeline object will handle state persistence.\n            else:\n                logger.debug(\n                    f\"Skipping TrainTestSplitter. current_data is {type(current_data)}\"\n                )\n                logger.warning(\n                    \"Attempting to split an already split dataset. Skipping TrainTestSplitter.\"\n                )\n\n        elif transformer_type == \"feature_target_split\":\n            logger.debug(\"Handling feature_target_split\")\n            # FeatureTargetSplitter changes structure to (X, y) or Dict of (X, y).\n            # We bypass StatefulTransformer to allow this structural change.\n            params = calculator.fit(current_data, params)\n            current_data = applier.apply(current_data, params)\n\n        else:\n            logger.debug(\"Handling standard transformer via StatefulTransformer\")\n            current_data = transformer.fit_transform(current_data, params)\n            # In SDK, transformer.params holds the state.\n            fitted_params = transformer.params\n\n            self.fitted_steps.append(\n                {\n                    \"name\": name,\n                    \"type\": transformer_type,\n                    \"applier\": applier,\n                    \"artifact\": fitted_params,\n                }\n            )\n\n        logger.debug(f\"Step {i} complete. New data type: {type(current_data)}\")\n\n        # Retrieve fitted params to get metrics from the calculator\n        try:\n            if fitted_params:\n                # Imputation Metrics\n                if transformer_type in [\n                    \"SimpleImputer\",\n                    \"KNNImputer\",\n                    \"IterativeImputer\",\n                ]:\n                    if \"missing_counts\" in fitted_params:\n                        metrics[\"missing_counts\"] = fitted_params[\"missing_counts\"]\n                    if \"total_missing\" in fitted_params:\n                        metrics[\"total_missing\"] = fitted_params[\"total_missing\"]\n                    if \"fill_values\" in fitted_params:\n                        metrics[\"fill_values\"] = fitted_params[\"fill_values\"]\n\n                # Feature Selection Metrics\n                if transformer_type in [\n                    \"feature_selection\",\n                    \"UnivariateSelection\",\n                    \"ModelBasedSelection\",\n                    \"VarianceThreshold\",\n                ]:\n                    if \"feature_scores\" in fitted_params:\n                        metrics[\"feature_scores\"] = fitted_params[\"feature_scores\"]\n                    if \"p_values\" in fitted_params:\n                        metrics[\"p_values\"] = fitted_params[\"p_values\"]\n                    if \"feature_importances\" in fitted_params:\n                        metrics[\"feature_importances\"] = fitted_params[\n                            \"feature_importances\"\n                        ]\n                    if \"variances\" in fitted_params:\n                        metrics[\"variances\"] = fitted_params[\"variances\"]\n                    if \"ranking\" in fitted_params:\n                        metrics[\"ranking\"] = fitted_params[\"ranking\"]\n                    if \"selected_columns\" in fitted_params:\n                        metrics[\"selected_columns\"] = fitted_params[\n                            \"selected_columns\"\n                        ]\n\n                # Scaling Metrics\n                if transformer_type in [\n                    \"StandardScaler\",\n                    \"MinMaxScaler\",\n                    \"RobustScaler\",\n                    \"MaxAbsScaler\",\n                ]:\n                    if \"mean\" in fitted_params:\n                        metrics[\"mean\"] = fitted_params[\"mean\"]\n                    if \"scale\" in fitted_params:\n                        metrics[\"scale\"] = fitted_params[\"scale\"]\n                    if \"var\" in fitted_params:\n                        metrics[\"var\"] = fitted_params[\"var\"]\n                    if \"min\" in fitted_params:\n                        metrics[\"min\"] = fitted_params[\"min\"]\n                    if \"data_min\" in fitted_params:\n                        metrics[\"data_min\"] = fitted_params[\"data_min\"]\n                    if \"data_max\" in fitted_params:\n                        metrics[\"data_max\"] = fitted_params[\"data_max\"]\n                    if \"center\" in fitted_params:\n                        metrics[\"center\"] = fitted_params[\"center\"]\n                    if \"max_abs\" in fitted_params:\n                        metrics[\"max_abs\"] = fitted_params[\"max_abs\"]\n                    if \"columns\" in fitted_params:\n                        metrics[\"columns\"] = fitted_params[\"columns\"]\n\n                # Outlier Metrics\n                if transformer_type in [\n                    \"IQR\",\n                    \"Winsorize\",\n                    \"ZScore\",\n                    \"EllipticEnvelope\",\n                ]:\n                    if \"warnings\" in fitted_params:\n                        metrics[\"warnings\"] = fitted_params[\"warnings\"]\n\n                if transformer_type in [\"IQR\", \"Winsorize\"]:\n                    if \"bounds\" in fitted_params:\n                        metrics[\"bounds\"] = fitted_params[\"bounds\"]\n\n                if transformer_type == \"ZScore\":\n                    if \"stats\" in fitted_params:\n                        metrics[\"stats\"] = fitted_params[\"stats\"]\n\n                if transformer_type == \"EllipticEnvelope\":\n                    if \"contamination\" in fitted_params:\n                        metrics[\"contamination\"] = fitted_params[\"contamination\"]\n\n                # Bucketing Metrics\n                if transformer_type in [\n                    \"GeneralBinning\",\n                    \"EqualWidthBinning\",\n                    \"EqualFrequencyBinning\",\n                    \"CustomBinning\",\n                    \"KBinsDiscretizer\",\n                ]:\n                    if \"bin_edges\" in fitted_params:\n                        metrics[\"bin_edges\"] = fitted_params[\"bin_edges\"]\n                    if \"n_bins\" in fitted_params:\n                        metrics[\"n_bins\"] = fitted_params[\"n_bins\"]\n\n                # Feature Generation Metrics\n                if transformer_type in [\"FeatureMath\", \"FeatureGenerationNode\"]:\n                    if \"operations\" in fitted_params:\n                        metrics[\"operations_count\"] = len(\n                            fitted_params[\"operations\"]\n                        )\n                        metrics[\"operations\"] = fitted_params[\"operations\"]\n                    # Calculate generated features by comparing columns\n                    if isinstance(data_before, pd.DataFrame) and isinstance(\n                        current_data, pd.DataFrame\n                    ):\n                        new_cols = list(\n                            set(current_data.columns) - set(data_before.columns)\n                        )\n                        metrics[\"generated_features\"] = new_cols\n                    elif isinstance(data_before, SplitDataset) and isinstance(\n                        current_data, SplitDataset\n                    ):\n                        # Check train set\n                        if isinstance(\n                            data_before.train, pd.DataFrame\n                        ) and isinstance(current_data.train, pd.DataFrame):\n                            new_cols = list(\n                                set(current_data.train.columns)\n                                - set(data_before.train.columns)\n                            )\n                            metrics[\"generated_features\"] = new_cols\n                        elif isinstance(data_before.train, tuple) and isinstance(\n                            current_data.train, tuple\n                        ):\n                            # (X, y) tuple\n                            X_before, _ = data_before.train\n                            X_after, _ = current_data.train\n                            if isinstance(X_before, pd.DataFrame) and isinstance(\n                                X_after, pd.DataFrame\n                            ):\n                                new_cols = list(\n                                    set(X_after.columns) - set(X_before.columns)\n                                )\n                                metrics[\"generated_features\"] = new_cols\n\n        except Exception as e:\n            logger.warning(f\"Failed to retrieve metrics for step {name}: {e}\")\n\n        # Capture metrics after\n        rows_after, cols_after = get_data_stats(current_data)\n\n        # Resampling Metrics (Calculated from data)\n        if transformer_type in [\"Oversampling\", \"Undersampling\"]:\n            try:\n                # Extract y to calculate class counts\n                y_res = None\n                if isinstance(current_data, SplitDataset):\n                    if isinstance(current_data.train, tuple):\n                        _, y_res = current_data.train\n                    elif isinstance(current_data.train, pd.DataFrame):\n                        # Try to find target column from params\n                        target_col = params.get(\"target_column\")\n                        if target_col and target_col in current_data.train.columns:\n                            y_res = current_data.train[target_col]\n                elif isinstance(current_data, tuple):\n                    _, y_res = current_data\n                elif isinstance(current_data, pd.DataFrame):\n                    target_col = params.get(\"target_column\")\n                    if target_col and target_col in current_data.columns:\n                        y_res = current_data[target_col]\n\n                if y_res is not None:\n                    counts = y_res.value_counts().to_dict()\n                    # Convert keys to string to ensure JSON serializability\n                    metrics[\"class_counts\"] = {\n                        str(k): int(v) for k, v in counts.items()\n                    }\n                    metrics[\"total_samples\"] = int(len(y_res))\n            except Exception as e:\n                logger.warning(f\"Failed to calculate resampling metrics: {e}\")\n\n        if rows_after &gt; 0 or cols_after:\n            if transformer_type in [\n                \"DropMissingRows\",\n                \"Deduplicate\",\n                \"IQR\",\n                \"ZScore\",\n                \"EllipticEnvelope\",\n                \"Winsorize\",\n            ]:\n                dropped = rows_before - rows_after\n                metrics[f\"{transformer_type}_rows_removed\"] = dropped\n                metrics[f\"{transformer_type}_rows_remaining\"] = rows_after\n                metrics[f\"{transformer_type}_rows_total\"] = rows_before\n                metrics[\"rows_removed\"] = dropped\n                metrics[\"rows_total\"] = rows_before\n\n                # Special metric for Winsorize: Values Clipped\n                if transformer_type == \"Winsorize\":\n                    try:\n                        clipped_count = 0\n\n                        # Helper to count diffs\n                        def count_diffs(df1, df2):\n                            if isinstance(df1, pd.DataFrame) and isinstance(\n                                df2, pd.DataFrame\n                            ):\n                                if df1.shape == df2.shape:\n                                    return int(df1.ne(df2).sum().sum())\n                            elif (\n                                isinstance(df1, tuple)\n                                and isinstance(df2, tuple)\n                                and len(df1) == 2\n                                and len(df2) == 2\n                            ):\n                                # Handle (X, y) tuple\n                                diffs = 0\n                                # Compare X (index 0)\n                                if isinstance(df1[0], pd.DataFrame) and isinstance(\n                                    df2[0], pd.DataFrame\n                                ):\n                                    if df1[0].shape == df2[0].shape:\n                                        diffs += int(df1[0].ne(df2[0]).sum().sum())\n                                # Compare y (index 1) - usually Series\n                                if isinstance(\n                                    df1[1], (pd.DataFrame, pd.Series)\n                                ) and isinstance(df2[1], (pd.DataFrame, pd.Series)):\n                                    if df1[1].shape == df2[1].shape:\n                                        diffs += int(df1[1].ne(df2[1]).sum().sum())  # type: ignore\n                                return diffs\n                            return 0\n\n                        if isinstance(data_before, pd.DataFrame) and isinstance(\n                            current_data, pd.DataFrame\n                        ):\n                            clipped_count = count_diffs(data_before, current_data)\n                        elif isinstance(data_before, SplitDataset) and isinstance(\n                            current_data, SplitDataset\n                        ):\n                            clipped_count += count_diffs(\n                                data_before.train, current_data.train\n                            )\n                            clipped_count += count_diffs(\n                                data_before.test, current_data.test\n                            )\n                            clipped_count += count_diffs(\n                                data_before.validation, current_data.validation\n                            )\n\n                        metrics[\"values_clipped\"] = clipped_count\n                    except Exception as e:\n                        logger.warning(\n                            f\"Failed to calculate values_clipped for Winsorize: {e}\"\n                        )\n                        pass\n\n            if transformer_type == \"MissingIndicator\":\n                new_cols_set = cols_after - cols_before\n                metrics[\"missing_indicators_created\"] = len(new_cols_set)\n                cast(Dict[str, Any], metrics)[\"missing_indicators_columns\"] = list(\n                    new_cols_set\n                )\n\n            if transformer_type == \"DropMissingColumns\":\n                dropped_cols_set = cols_before - cols_after\n                cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                    dropped_cols_set\n                )\n                metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n            if transformer_type == \"feature_selection\":\n                dropped_cols_set = cols_before - cols_after\n                cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                    dropped_cols_set\n                )\n                metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n            if transformer_type in [\n                \"OneHotEncoder\",\n                \"LabelEncoder\",\n                \"OrdinalEncoder\",\n                \"TargetEncoder\",\n                \"HashEncoder\",\n                \"DummyEncoder\",\n            ]:\n                new_cols_set = cols_after - cols_before\n                metrics[\"new_features_count\"] = len(new_cols_set)\n                metrics[\"encoded_columns_count\"] = len(params.get(\"columns\", []))\n\n                if \"categories_count\" in params:\n                    metrics[\"categories_count\"] = params[\"categories_count\"]\n                if \"classes_count\" in params:\n                    metrics[\"classes_count\"] = params[\"classes_count\"]\n\n    return current_data, metrics\n</code></pre>"},{"location":"reference/api/preprocessing/pipeline.html#skyulf.preprocessing.pipeline.FeatureEngineer.transform","title":"<code>transform(data)</code>","text":"<p>Apply fitted transformations to new data.</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>def transform(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply fitted transformations to new data.\n    \"\"\"\n    current_data = data\n\n    for step in self.fitted_steps:\n        name = step[\"name\"]\n        transformer_type = step[\"type\"]\n        applier = step[\"applier\"]\n        artifact = step[\"artifact\"]\n\n        # Skip splitters during inference/transform\n        if transformer_type in [\n            \"TrainTestSplitter\",\n            \"feature_target_split\",\n            \"Oversampling\",\n            \"Undersampling\",\n        ]:\n            continue\n\n        logger.debug(f\"Applying step: {name} ({transformer_type})\")\n        current_data = applier.apply(current_data, artifact)\n\n    return current_data\n</code></pre>"},{"location":"reference/api/preprocessing/resampling.html","title":"API: preprocessing.resampling","text":""},{"location":"reference/api/preprocessing/resampling.html#skyulf.preprocessing.resampling","title":"<code>skyulf.preprocessing.resampling</code>","text":""},{"location":"reference/api/preprocessing/scaling.html","title":"API: preprocessing.scaling","text":""},{"location":"reference/api/preprocessing/scaling.html#skyulf.preprocessing.scaling","title":"<code>skyulf.preprocessing.scaling</code>","text":""},{"location":"reference/api/preprocessing/split.html","title":"API: preprocessing.split","text":""},{"location":"reference/api/preprocessing/split.html#skyulf.preprocessing.split","title":"<code>skyulf.preprocessing.split</code>","text":""},{"location":"reference/api/preprocessing/split.html#skyulf.preprocessing.split.DataSplitter","title":"<code>DataSplitter</code>","text":"<p>Splits a DataFrame into Train, Test, and optionally Validation sets.</p> Source code in <code>skyulf-core/skyulf/preprocessing/split.py</code> <pre><code>class DataSplitter:\n    \"\"\"\n    Splits a DataFrame into Train, Test, and optionally Validation sets.\n    \"\"\"\n\n    def __init__(\n        self,\n        test_size: float = 0.2,\n        validation_size: float = 0.0,\n        random_state: int = 42,\n        shuffle: bool = True,\n        stratify_col: Optional[str] = None,\n    ):\n        self.test_size = test_size\n        self.validation_size = validation_size\n        self.random_state = random_state\n        self.shuffle = shuffle\n        self.stratify_col = stratify_col\n\n    def split_xy(self, X: pd.DataFrame, y: pd.Series) -&gt; SplitDataset:\n        \"\"\"\n        Splits X and y arrays.\n        \"\"\"\n        stratify = y if self.stratify_col else None  # If stratify is requested, use y\n\n        if stratify is not None:\n            class_counts = y.value_counts()\n            if class_counts.min() &lt; 2:\n                logger.warning(\n                    f\"Stratified split requested but the least populated class has only {class_counts.min()} \"\n                    \"member(s). Stratification will be disabled.\"\n                )\n                stratify = None\n\n        # First split: Train+Val vs Test\n        X_train_val, X_test, y_train_val, y_test = train_test_split(\n            X,\n            y,\n            test_size=self.test_size,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n            stratify=stratify,\n        )\n\n        validation = None\n        if self.validation_size &gt; 0:\n            relative_val_size = self.validation_size / (1 - self.test_size)\n            stratify_val = y_train_val if self.stratify_col else None\n\n            if stratify_val is not None:\n                class_counts_val = y_train_val.value_counts()\n                if class_counts_val.min() &lt; 2:\n                    logger.warning(\n                        \"Stratified validation split requested but the least populated class has only \"\n                        f\"{class_counts_val.min()} member(s). Stratification will be disabled for validation split.\"\n                    )\n                    stratify_val = None\n\n            X_train, X_val, y_train, y_val = train_test_split(\n                X_train_val,\n                y_train_val,\n                test_size=relative_val_size,\n                random_state=self.random_state,\n                shuffle=self.shuffle,\n                stratify=stratify_val,\n            )\n            validation = (X_val, y_val)\n        else:\n            X_train, y_train = X_train_val, y_train_val\n\n        return SplitDataset(\n            train=(X_train, y_train), test=(X_test, y_test), validation=validation\n        )\n\n    def split(self, df: pd.DataFrame) -&gt; SplitDataset:\n        \"\"\"\n        Splits a DataFrame.\n        \"\"\"\n        stratify = None\n        if self.stratify_col and self.stratify_col in df.columns:\n            stratify = df[self.stratify_col]\n            class_counts = stratify.value_counts()\n            if class_counts.min() &lt; 2:\n                logger.warning(\n                    f\"Stratified split requested but the least populated class has only {class_counts.min()} \"\n                    \"member(s). Stratification will be disabled.\"\n                )\n                stratify = None\n\n        train_val, test = train_test_split(\n            df,\n            test_size=self.test_size,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n            stratify=stratify,\n        )\n\n        validation = None\n        if self.validation_size &gt; 0:\n            relative_val_size = self.validation_size / (1 - self.test_size)\n\n            stratify_val = None\n            if self.stratify_col and self.stratify_col in train_val.columns:\n                stratify_val = train_val[self.stratify_col]\n                class_counts_val = stratify_val.value_counts()\n                if class_counts_val.min() &lt; 2:\n                    logger.warning(\n                        f\"Stratified validation split requested but the least populated class has only {\n                            class_counts_val.min()} member(s). Stratification will be disabled for validation split.\"\n                    )\n                    stratify_val = None\n\n            train, val = train_test_split(\n                train_val,\n                test_size=relative_val_size,\n                random_state=self.random_state,\n                shuffle=self.shuffle,\n                stratify=stratify_val,\n            )\n            validation = val\n        else:\n            train = train_val\n\n        return SplitDataset(train=train, test=test, validation=validation)\n</code></pre>"},{"location":"reference/api/preprocessing/split.html#skyulf.preprocessing.split.DataSplitter.split","title":"<code>split(df)</code>","text":"<p>Splits a DataFrame.</p> Source code in <code>skyulf-core/skyulf/preprocessing/split.py</code> <pre><code>def split(self, df: pd.DataFrame) -&gt; SplitDataset:\n    \"\"\"\n    Splits a DataFrame.\n    \"\"\"\n    stratify = None\n    if self.stratify_col and self.stratify_col in df.columns:\n        stratify = df[self.stratify_col]\n        class_counts = stratify.value_counts()\n        if class_counts.min() &lt; 2:\n            logger.warning(\n                f\"Stratified split requested but the least populated class has only {class_counts.min()} \"\n                \"member(s). Stratification will be disabled.\"\n            )\n            stratify = None\n\n    train_val, test = train_test_split(\n        df,\n        test_size=self.test_size,\n        random_state=self.random_state,\n        shuffle=self.shuffle,\n        stratify=stratify,\n    )\n\n    validation = None\n    if self.validation_size &gt; 0:\n        relative_val_size = self.validation_size / (1 - self.test_size)\n\n        stratify_val = None\n        if self.stratify_col and self.stratify_col in train_val.columns:\n            stratify_val = train_val[self.stratify_col]\n            class_counts_val = stratify_val.value_counts()\n            if class_counts_val.min() &lt; 2:\n                logger.warning(\n                    f\"Stratified validation split requested but the least populated class has only {\n                        class_counts_val.min()} member(s). Stratification will be disabled for validation split.\"\n                )\n                stratify_val = None\n\n        train, val = train_test_split(\n            train_val,\n            test_size=relative_val_size,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n            stratify=stratify_val,\n        )\n        validation = val\n    else:\n        train = train_val\n\n    return SplitDataset(train=train, test=test, validation=validation)\n</code></pre>"},{"location":"reference/api/preprocessing/split.html#skyulf.preprocessing.split.DataSplitter.split_xy","title":"<code>split_xy(X, y)</code>","text":"<p>Splits X and y arrays.</p> Source code in <code>skyulf-core/skyulf/preprocessing/split.py</code> <pre><code>def split_xy(self, X: pd.DataFrame, y: pd.Series) -&gt; SplitDataset:\n    \"\"\"\n    Splits X and y arrays.\n    \"\"\"\n    stratify = y if self.stratify_col else None  # If stratify is requested, use y\n\n    if stratify is not None:\n        class_counts = y.value_counts()\n        if class_counts.min() &lt; 2:\n            logger.warning(\n                f\"Stratified split requested but the least populated class has only {class_counts.min()} \"\n                \"member(s). Stratification will be disabled.\"\n            )\n            stratify = None\n\n    # First split: Train+Val vs Test\n    X_train_val, X_test, y_train_val, y_test = train_test_split(\n        X,\n        y,\n        test_size=self.test_size,\n        random_state=self.random_state,\n        shuffle=self.shuffle,\n        stratify=stratify,\n    )\n\n    validation = None\n    if self.validation_size &gt; 0:\n        relative_val_size = self.validation_size / (1 - self.test_size)\n        stratify_val = y_train_val if self.stratify_col else None\n\n        if stratify_val is not None:\n            class_counts_val = y_train_val.value_counts()\n            if class_counts_val.min() &lt; 2:\n                logger.warning(\n                    \"Stratified validation split requested but the least populated class has only \"\n                    f\"{class_counts_val.min()} member(s). Stratification will be disabled for validation split.\"\n                )\n                stratify_val = None\n\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_train_val,\n            y_train_val,\n            test_size=relative_val_size,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n            stratify=stratify_val,\n        )\n        validation = (X_val, y_val)\n    else:\n        X_train, y_train = X_train_val, y_train_val\n\n    return SplitDataset(\n        train=(X_train, y_train), test=(X_test, y_test), validation=validation\n    )\n</code></pre>"},{"location":"reference/api/preprocessing/transformations.html","title":"API: preprocessing.transformations","text":""},{"location":"reference/api/preprocessing/transformations.html#skyulf.preprocessing.transformations","title":"<code>skyulf.preprocessing.transformations</code>","text":""},{"location":"user_guide/configuration.html","title":"Configuration","text":"<p>This page documents the configuration schema consumed by <code>SkyulfPipeline</code> and <code>FeatureEngineer</code>.</p>"},{"location":"user_guide/configuration.html#pipeline-config","title":"Pipeline config","text":"<p><code>SkyulfPipeline</code> expects:</p> <pre><code>{\n  \"preprocessing\": [ ... ],\n  \"modeling\": { ... }\n}\n</code></pre>"},{"location":"user_guide/configuration.html#preprocessing-config","title":"Preprocessing config","text":"<p>The preprocessing list is executed in order.</p> <p>Each step is:</p> <pre><code>{\n  \"name\": \"step_name\",\n  \"transformer\": \"TransformerType\",\n  \"params\": { ... }\n}\n</code></pre> <p><code>TransformerType</code> is a string key that <code>FeatureEngineer</code> dispatches to a Calculator/Applier pair. For the full list and per-node parameters, see:</p> <ul> <li>Reference \u2192 Preprocessing Nodes</li> <li>Reference \u2192 API \u2192 Preprocessing \u2192 pipeline</li> </ul>"},{"location":"user_guide/configuration.html#minimal-examples","title":"Minimal examples","text":"<pre><code># Split to avoid leakage\n{\"name\": \"split\", \"transformer\": \"TrainTestSplitter\", \"params\": {\"test_size\": 0.2, \"random_state\": 42, \"target_column\": \"target\"}}\n</code></pre> <pre><code># Impute missing numeric values\n{\"name\": \"impute\", \"transformer\": \"SimpleImputer\", \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]}}\n</code></pre> <pre><code># Encode categoricals\n{\"name\": \"encode\", \"transformer\": \"OneHotEncoder\", \"params\": {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"}}\n</code></pre> <pre><code># Scale numeric columns\n{\"name\": \"scale\", \"transformer\": \"StandardScaler\", \"params\": {\"auto_detect\": True}}\n</code></pre>"},{"location":"user_guide/configuration.html#modeling-config","title":"Modeling config","text":"<p><code>SkyulfPipeline</code> currently supports these model types:</p> <ul> <li><code>logistic_regression</code></li> <li><code>random_forest_classifier</code></li> <li><code>ridge_regression</code></li> <li><code>random_forest_regressor</code></li> <li><code>hyperparameter_tuner</code></li> </ul> <p>Example:</p> <pre><code>{\n  \"type\": \"random_forest_classifier\",\n  \"node_id\": \"model_node\",\n  \"params\": {\n    \"n_estimators\": 200,\n    \"max_depth\": 10\n  }\n}\n</code></pre> <p>Tuner example:</p> <pre><code>{\n  \"type\": \"hyperparameter_tuner\",\n  \"base_model\": {\"type\": \"logistic_regression\"},\n  \"strategy\": \"random\",\n  \"search_space\": {\"C\": [0.1, 1.0, 10.0]},\n  \"n_trials\": 25,\n  \"metric\": \"accuracy\"\n}\n</code></pre> <p>See \u201cModeling Nodes\u201d in Reference for details.</p>"},{"location":"user_guide/extending_custom_nodes.html","title":"Extending Skyulf-Core","text":"<p>Skyulf-core is intentionally simple: calculators learn parameters, appliers apply them.</p>"},{"location":"user_guide/extending_custom_nodes.html#add-a-new-preprocessing-node","title":"Add a new preprocessing node","text":"<ol> <li>Create a new module in <code>skyulf.preprocessing</code>.</li> <li>Implement a <code>Calculator</code> and an <code>Applier</code>.</li> <li>Register the node type string in the <code>FeatureEngineer</code> dispatcher.</li> </ol>"},{"location":"user_guide/extending_custom_nodes.html#example-skeleton","title":"Example skeleton","text":"<pre><code>from typing import Any, Dict, Tuple, Union\n\nimport pandas as pd\n\nfrom skyulf.preprocessing.base import BaseApplier, BaseCalculator\nfrom skyulf.utils import pack_pipeline_output, unpack_pipeline_input\n\n\nclass MyNodeCalculator(BaseCalculator):\n    def fit(\n        self,\n        df: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n        # Learn something from X...\n        return {\"type\": \"my_node\", \"columns\": config.get(\"columns\", [])}\n\n\nclass MyNodeApplier(BaseApplier):\n    def apply(\n        self,\n        df: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        params: Dict[str, Any],\n    ) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]]:\n        X, y, is_tuple = unpack_pipeline_input(df)\n        # Apply transformation...\n        return pack_pipeline_output(X, y, is_tuple)\n</code></pre>"},{"location":"user_guide/extending_custom_nodes.html#add-a-new-modeling-estimator","title":"Add a new modeling estimator","text":"<ol> <li>Implement a new <code>BaseModelCalculator</code> and <code>BaseModelApplier</code> (or subclass <code>SklearnCalculator/SklearnApplier</code>).</li> <li>Add a mapping entry in <code>SkyulfPipeline._init_model_estimator()</code>.</li> </ol>"},{"location":"user_guide/extending_custom_nodes.html#testing-guidance","title":"Testing guidance","text":"<p>Prefer integration tests that run:</p> <p><code>Calculator.fit</code> \u2192 <code>Applier.apply</code> on a small synthetic dataframe.</p>"},{"location":"user_guide/installation.html","title":"Installation","text":""},{"location":"user_guide/installation.html#editable-install-repo-checkout","title":"Editable install (repo checkout)","text":"<p>From the repository root:</p> <pre><code>pip install -e ./skyulf-core\n</code></pre>"},{"location":"user_guide/installation.html#runtime-dependencies","title":"Runtime dependencies","text":"<p><code>skyulf-core</code> primarily relies on:</p> <ul> <li>Pandas</li> <li>NumPy</li> <li>Scikit-Learn</li> </ul> <p>Some preprocessing nodes use optional dependencies (e.g., <code>rapidfuzz</code> for string similarity in feature generation).</p>"},{"location":"user_guide/installation.html#import-check","title":"Import check","text":"<pre><code>from skyulf.pipeline import SkyulfPipeline\nfrom skyulf.data.dataset import SplitDataset\n</code></pre>"},{"location":"user_guide/overview.html","title":"Overview","text":"<p><code>skyulf-core</code> is a standalone ML pipeline library designed for reproducible feature engineering and modeling.</p>"},{"location":"user_guide/overview.html#key-idea-explicit-learned-state","title":"Key idea: explicit learned state","text":"<p>Skyulf-core uses a strict Calculator \u2192 Applier pattern:</p> <ul> <li>A Calculator learns from data and returns a <code>params</code> dictionary.</li> <li>An Applier takes <code>params</code> and transforms data.</li> </ul> <p>This differs from scikit-learn\u2019s default pattern:</p> <ul> <li>In scikit-learn, <code>fit()</code> mutates the estimator/transformer object (e.g. <code>self.mean_</code>, <code>self.categories_</code>), and <code>transform()</code> uses those hidden internal attributes.</li> <li>In Skyulf-core, <code>fit()</code> returns the learned state explicitly as a plain <code>params</code> dictionary (ideally JSON-serializable; sometimes pickled for complex sklearn objects), and <code>apply()</code> uses only that dictionary.</li> </ul> <p>Practically, this makes learned state easier to inspect, persist, and apply consistently across train/test/inference.</p>"},{"location":"user_guide/overview.html#two-ways-to-use-the-library","title":"Two ways to use the library","text":""},{"location":"user_guide/overview.html#1-pipeline-way-recommended","title":"1) Pipeline way (recommended)","text":"<p>Use <code>SkyulfPipeline</code> to run preprocessing + modeling end-to-end.</p>"},{"location":"user_guide/overview.html#2-component-way-low-level","title":"2) Component way (low-level)","text":"<p>Call calculators/appliers directly for debugging, testing, or custom scripts.</p> <p>This is also where you\u2019ll see <code>StatefulEstimator</code> used: it\u2019s a small convenience wrapper that keeps a fitted model artifact in memory and can run <code>fit_predict()</code> on a <code>SplitDataset</code>. <code>SkyulfPipeline</code> uses the same underlying idea internally.</p>"},{"location":"user_guide/overview.html#how-fit-transform-works-in-skyulf","title":"How <code>fit</code> / <code>transform</code> works in Skyulf","text":"<p>At a high level:</p> <ul> <li> <p><code>SkyulfPipeline.fit(data, target_column=...)</code></p> <ul> <li>Runs preprocessing in order.</li> <li>For each preprocessing step: Calculator learns params (typically from train only), then Applier applies those params to train/test/validation.</li> <li>Trains the model and reports metrics.</li> </ul> </li> <li> <p><code>SkyulfPipeline.predict(df)</code></p> <ul> <li>Applies the already-learned preprocessing params (no re-fitting).</li> <li>Skips steps that only make sense during training (e.g., splitters / resampling).</li> <li>Runs the trained model to produce predictions.</li> </ul> </li> </ul> <p>If you want reproducible \u201cproof-style\u201d checks (sklearn-style <code>X/y</code> split + leakage demonstration), see:</p> <ul> <li>Validation vs scikit-learn</li> </ul>"},{"location":"user_guide/overview.html#why-splitting-matters-leakage","title":"Why splitting matters (leakage)","text":"<p>Many preprocessing nodes learn from data (means, categories, bin edges, vocabularies\u2026). If you learn those from the full dataset and then evaluate on a test set, you leak information.</p> <p>The safe pattern is:</p> <ul> <li>Split first (or provide a <code>SplitDataset</code>).</li> <li>Fit preprocessing on <code>train</code> only.</li> <li>Reuse the learned <code>params</code> to transform <code>test</code> / new inference data.</li> </ul>"},{"location":"user_guide/overview.html#where-things-live","title":"Where things live","text":"<ul> <li><code>skyulf.preprocessing</code>: feature engineering nodes (imputation, encoding, scaling, \u2026)</li> <li><code>skyulf.modeling</code>: estimators (classification/regression + tuning)</li> <li><code>skyulf.data</code>: <code>SplitDataset</code> for safe train/test/validation flows</li> </ul>"},{"location":"user_guide/pipeline_quickstart.html","title":"Pipeline Quickstart","text":"<p>This guide shows a production-style workflow: split \u2192 fit \u2192 evaluate \u2192 predict \u2192 save/load.</p>"},{"location":"user_guide/pipeline_quickstart.html#what-data-shape-should-i-pass","title":"What data shape should I pass?","text":"<p><code>SkyulfPipeline.fit(...)</code> supports two common inputs:</p> <ol> <li> <p>A single <code>pd.DataFrame</code> that includes the target column.</p> <ul> <li>You pass <code>target_column=\"...\"</code> and Skyulf splits features/target internally.</li> <li>This is what the quickstart uses, because it\u2019s the simplest onboarding path.</li> </ul> </li> <li> <p>A <code>SplitDataset</code> (recommended when you already have train/test splits).</p> <ul> <li>Each split can be either a DataFrame (with the target column) or a tuple <code>(X, y)</code>.</li> </ul> </li> </ol> <p>You do not need to manually build <code>X</code> and <code>y</code> for the pipeline unless you want to.</p> <p>If you prefer a scikit-learn-style workflow (<code>X</code>/<code>y</code> + <code>train_test_split</code>), see:</p> <ul> <li>Validation vs scikit-learn (Proof)</li> </ul>"},{"location":"user_guide/pipeline_quickstart.html#complete-runnable-example","title":"Complete runnable example","text":"<p>This single snippet is intentionally end-to-end (no duplicated setup across steps).</p> <pre><code>from __future__ import annotations\n\nimport tempfile\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\n# 1) Define a pipeline config\n# Each preprocessing step is:\n#   {\"name\": \"...\", \"transformer\": \"TransformerType\", \"params\": {...}}\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"split\",\n            \"transformer\": \"TrainTestSplitter\",\n            \"params\": {\n                \"test_size\": 0.25,\n                \"validation_size\": 0.0,\n                \"random_state\": 42,\n                \"shuffle\": True,\n                \"stratify\": True,\n                \"target_column\": \"target\",\n            },\n        },\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"},\n        },\n    ],\n    \"modeling\": {\n        \"type\": \"random_forest_classifier\",\n        \"params\": {\"n_estimators\": 50, \"random_state\": 42},\n    },\n}\n\n# 2) Training data (includes the target column)\ndf = pd.DataFrame(\n    {\n        \"age\": [10, 20, None, 40, 50, 60, None, 80],\n        \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n        \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n    }\n)\n\n# 3) Fit (learn params on train split, apply to test split)\npipeline = SkyulfPipeline(config)\nmetrics = pipeline.fit(df, target_column=\"target\")\nprint(\"Metrics keys:\", list(metrics.keys()))\n\n# 4) Predict (feature-only dataframe)\nincoming = pd.DataFrame({\"age\": [25, None], \"city\": [\"A\", \"C\"]})\npreds = pipeline.predict(incoming)\nprint(\"Preds:\")\nprint(preds)\n\n# 5) Save / load\nwith tempfile.TemporaryDirectory() as tmp:\n    model_path = Path(tmp) / \"model.pkl\"\n    pipeline.save(model_path)\n    loaded = SkyulfPipeline.load(model_path)\n    preds2 = loaded.predict(incoming)\n\nprint(\"Preds after reload:\")\nprint(preds2)\n</code></pre>"},{"location":"user_guide/serialization.html","title":"Serialization","text":""},{"location":"user_guide/serialization.html#what-is-persisted","title":"What is persisted","text":"<p><code>SkyulfPipeline.save()</code> uses Python <code>pickle</code> to serialize the entire pipeline object.</p> <p>That includes:</p> <ul> <li>preprocessing fitted artifacts (per-step <code>params</code>)</li> <li>the trained model (sklearn estimator object)</li> </ul>"},{"location":"user_guide/serialization.html#practical-guidance","title":"Practical guidance","text":"<ul> <li>Prefer saving in environments where the same library versions are available.</li> <li>Some preprocessing nodes store sklearn objects inside <code>params</code> (e.g., KNN/Iterative imputers, OneHotEncoder).   Those are not JSON-serializable and require pickling.</li> </ul>"},{"location":"user_guide/serialization.html#load-and-use","title":"Load and use","text":"<pre><code>from __future__ import annotations\n\nimport tempfile\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\ndf = pd.DataFrame(\n  {\n    \"age\": [10, 20, None, 40, 50, 60, None, 80],\n    \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n    \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n  }\n)\n\nconfig = {\n  \"preprocessing\": [\n    {\n      \"name\": \"split\",\n      \"transformer\": \"TrainTestSplitter\",\n      \"params\": {\n        \"test_size\": 0.2,\n        \"validation_size\": 0.0,\n        \"random_state\": 42,\n        \"shuffle\": True,\n        \"stratify\": True,\n        \"target_column\": \"target\",\n      },\n    },\n    {\n      \"name\": \"impute\",\n      \"transformer\": \"SimpleImputer\",\n      \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n    },\n    {\n      \"name\": \"encode\",\n      \"transformer\": \"OneHotEncoder\",\n      \"params\": {\"columns\": [\"city\"], \"drop_original\": True},\n    },\n  ],\n  \"modeling\": {\n    \"type\": \"random_forest_classifier\",\n    \"params\": {\"n_estimators\": 50, \"random_state\": 42},\n  },\n}\n\nwith tempfile.TemporaryDirectory() as tmp:\n  model_path = Path(tmp) / \"model.pkl\"\n\n  pipeline = SkyulfPipeline(config)\n  _ = pipeline.fit(df, target_column=\"target\")\n  pipeline.save(model_path)\n\n  loaded = SkyulfPipeline.load(model_path)\n  new_df = pd.DataFrame({\"age\": [25, None], \"city\": [\"A\", \"C\"]})\n  preds = loaded.predict(new_df)\n\nprint(preds)\n</code></pre>"},{"location":"user_guide/splitdataset_and_leakage.html","title":"SplitDataset &amp; Leakage","text":""},{"location":"user_guide/splitdataset_and_leakage.html#why-splitdataset-exists","title":"Why <code>SplitDataset</code> exists","text":"<p>Many preprocessing nodes learn statistics from data (means, categories, bin edges, \u2026). If those statistics are computed on the full dataset and then evaluated on test data, you leak information.</p> <p><code>SplitDataset</code> is a container for:</p> <ul> <li><code>train</code></li> <li><code>test</code></li> <li>optional <code>validation</code></li> </ul> <p>Each split can be either:</p> <ul> <li>a <code>pd.DataFrame</code> (with the target column inside), or</li> <li>a tuple <code>(X: pd.DataFrame, y: pd.Series)</code>.</li> </ul>"},{"location":"user_guide/splitdataset_and_leakage.html#recommended-patterns","title":"Recommended patterns","text":""},{"location":"user_guide/splitdataset_and_leakage.html#pattern-a-split-in-preprocessing","title":"Pattern A: split in preprocessing","text":"<p>Use the <code>TrainTestSplitter</code> transformer early.</p> <pre><code>{\n  \"name\": \"split\",\n  \"transformer\": \"TrainTestSplitter\",\n  \"params\": {\"test_size\": 0.2, \"random_state\": 42, \"target_column\": \"target\"}\n}\n</code></pre>"},{"location":"user_guide/splitdataset_and_leakage.html#pattern-b-create-splitdataset-yourself","title":"Pattern B: create <code>SplitDataset</code> yourself","text":"<pre><code>from __future__ import annotations\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom skyulf.data.dataset import SplitDataset\nfrom skyulf.pipeline import SkyulfPipeline\n\ndf = pd.DataFrame(\n  {\n    \"age\": [10, 20, None, 40, 50, 60, None, 80],\n    \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n    \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n  }\n)\n\nconfig = {\n  \"preprocessing\": [\n    {\n      \"name\": \"impute\",\n      \"transformer\": \"SimpleImputer\",\n      \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n    },\n    {\n      \"name\": \"encode\",\n      \"transformer\": \"OneHotEncoder\",\n      \"params\": {\"columns\": [\"city\"], \"drop_original\": True},\n    },\n  ],\n  \"modeling\": {\n    \"type\": \"random_forest_classifier\",\n    \"params\": {\"n_estimators\": 50, \"random_state\": 42},\n  },\n}\ntrain, test = train_test_split(df, test_size=0.2, random_state=42)\n\ndataset = SplitDataset(train=train, test=test, validation=None)\n\npipeline = SkyulfPipeline(config)\nmetrics = pipeline.fit(dataset, target_column=\"target\")\n\nprint(metrics)\n</code></pre>"},{"location":"user_guide/splitdataset_and_leakage.html#notes-on-inference","title":"Notes on inference","text":"<p>At inference time, <code>FeatureEngineer.transform()</code> skips splitters and resampling steps. That ensures your <code>predict()</code> path remains deterministic.</p>"},{"location":"user_guide/step_by_step_no_config.html","title":"Step-by-Step (No Config)","text":"<p>This tutorial runs an end-to-end workflow without building a pipeline <code>config</code> dictionary. Instead, it uses Skyulf\u2019s low-level building blocks directly:</p> <ul> <li>preprocessing Calculators and Appliers</li> <li><code>SplitDataset</code> to avoid data leakage</li> <li>modeling via <code>StatefulEstimator</code></li> </ul> <p>Use this approach when you want maximum transparency, debugging control, or custom orchestration.</p>"},{"location":"user_guide/step_by_step_no_config.html#what-you-will-build","title":"What you will build","text":"<ol> <li>Split a DataFrame into train/test using <code>TrainTestSplitter</code></li> <li>Apply a few preprocessing steps (imputation + encoding)</li> <li>Train a model and generate predictions</li> </ol>"},{"location":"user_guide/step_by_step_no_config.html#runnable-example","title":"Runnable example","text":"<p>This single snippet is fully self-contained and can be pasted into a Python session as-is.</p> <pre><code>from __future__ import annotations\n\nimport pandas as pd\n\nfrom skyulf.data.dataset import SplitDataset\nfrom skyulf.modeling.base import StatefulEstimator\nfrom skyulf.modeling.classification import (\n    RandomForestClassifierApplier,\n    RandomForestClassifierCalculator,\n)\nfrom skyulf.preprocessing.encoding import OneHotEncoderApplier, OneHotEncoderCalculator\nfrom skyulf.preprocessing.imputation import SimpleImputerApplier, SimpleImputerCalculator\nfrom skyulf.preprocessing.split import (\n    FeatureTargetSplitApplier,\n    FeatureTargetSplitCalculator,\n    SplitApplier,\n    SplitCalculator,\n)\n\n# 0) Setup data\ndf = pd.DataFrame(\n    {\n        \"age\": [10, 20, None, 40, 50, None, 70, 80],\n        \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n        \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n    }\n)\n\n# 1) Split the dataset (no leakage)\nsplit_params = {\n    \"test_size\": 0.25,\n    \"validation_size\": 0.0,\n    \"random_state\": 42,\n    \"shuffle\": True,\n    \"stratify\": True,\n    \"target_column\": \"target\",\n}\n\ndataset = SplitApplier().apply(df, SplitCalculator().fit(df, split_params))\n\n# 2) Convert DataFrames to (X, y) tuples\nfts_params = {\"target_column\": \"target\"}\ndataset_xy = FeatureTargetSplitApplier().apply(\n    dataset, FeatureTargetSplitCalculator().fit(dataset, fts_params)\n)\n\nX_train, y_train = dataset_xy.train\nX_test, y_test = dataset_xy.test\n\n# 3) Fit preprocessing on train, apply to test\nimp_cfg = {\"columns\": [\"age\"], \"strategy\": \"mean\"}\nimp_params = SimpleImputerCalculator().fit((X_train, y_train), imp_cfg)\nX_train_imp, y_train = SimpleImputerApplier().apply((X_train, y_train), imp_params)\nX_test_imp, y_test = SimpleImputerApplier().apply((X_test, y_test), imp_params)\n\nohe_cfg = {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"}\nohe_params = OneHotEncoderCalculator().fit((X_train_imp, y_train), ohe_cfg)\nX_train_fe, y_train = OneHotEncoderApplier().apply((X_train_imp, y_train), ohe_params)\nX_test_fe, y_test = OneHotEncoderApplier().apply((X_test_imp, y_test), ohe_params)\n\n# 4) Train a model and predict\nestimator = StatefulEstimator(\n    calculator=RandomForestClassifierCalculator(),\n    applier=RandomForestClassifierApplier(),\n    node_id=\"rf_model\",\n)\n\ndataset_for_model = SplitDataset(\n    train=(X_train_fe, y_train),\n    test=(X_test_fe, y_test),\n    validation=None,\n)\n\nmodel_cfg = {\"params\": {\"n_estimators\": 50, \"random_state\": 42}}\npreds = estimator.fit_predict(dataset=dataset_for_model, target_column=\"target\", config=model_cfg)\n\nprint(\"Train preds:\")\nprint(preds[\"train\"].head())\nprint(\"Test preds:\")\nprint(preds.get(\"test\", pd.Series(dtype=float)).head())\n</code></pre>"},{"location":"user_guide/step_by_step_no_config.html#5-summary-what-to-remember","title":"5) Summary: what to remember","text":"<ul> <li>Split first (or provide a <code>SplitDataset</code>) to prevent leakage.</li> <li>Fit preprocessing on train, reuse the learned <code>params</code> for test/inference.</li> <li>Modeling can be driven directly through <code>StatefulEstimator</code> when you don\u2019t want a pipeline config.</li> </ul>"},{"location":"user_guide/troubleshooting.html","title":"Troubleshooting","text":""},{"location":"user_guide/troubleshooting.html#unknown-transformer-type","title":"\u201cUnknown transformer type\u201d","text":"<p>This means your step\u2019s <code>transformer</code> string does not match the <code>FeatureEngineer</code> dispatcher. Check spelling and casing.</p>"},{"location":"user_guide/troubleshooting.html#resampling-errors-about-non-numeric-columns","title":"Resampling errors about non-numeric columns","text":"<p>Oversampling/undersampling require numeric features. Apply an encoder first (e.g., OneHotEncoder / OrdinalEncoder) before resampling.</p>"},{"location":"user_guide/troubleshooting.html#pickle-loading-errors","title":"Pickle loading errors","text":"<p>If <code>SkyulfPipeline.load()</code> fails, ensure you are using compatible versions of:</p> <ul> <li>Python</li> <li>scikit-learn</li> <li>pandas</li> </ul> <p>If you need portability across environments, consider constraining versions.</p>"},{"location":"user_guide/validation_vs_sklearn.html","title":"Validation vs scikit-learn (Proof)","text":"<p>This page gives reproducible, runnable checks that:</p> <ul> <li>Skyulf supports the familiar scikit-learn workflow (build <code>X</code>/<code>y</code>, run <code>train_test_split</code>, fit on train, transform/predict on test).</li> <li>Skyulf avoids common forms of data leakage by learning preprocessing parameters from train only.</li> </ul> <p>Goal: show verifiable behavior, not claim bit-for-bit identical models.</p>"},{"location":"user_guide/validation_vs_sklearn.html#1-scikit-learn-style-workflow-xy-train_test_split","title":"1) scikit-learn-style workflow (X/y + train_test_split)","text":"<p>This mirrors the sklearn pattern:</p> <ul> <li>sklearn: <code>fit(X_train, y_train)</code> then <code>predict(X_test)</code></li> <li>Skyulf: pass <code>SplitDataset(train=(X_train, y_train), test=(X_test, y_test))</code></li> </ul> <pre><code>from __future__ import annotations\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom skyulf.data.dataset import SplitDataset\nfrom skyulf.pipeline import SkyulfPipeline\n\n# Synthetic classification data\nraw = pd.DataFrame(\n    {\n        \"age\": [10, 20, None, 40, 50, 60, None, 80],\n        \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n        \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n    }\n)\n\nX = raw.drop(columns=[\"target\"])\ny = raw[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\ndataset = SplitDataset(train=(X_train, y_train), test=(X_test, y_test), validation=None)\n\n# IMPORTANT: because we already split, we do not add TrainTestSplitter here.\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"},\n        },\n    ],\n    \"modeling\": {\"type\": \"random_forest_classifier\", \"params\": {\"n_estimators\": 50, \"random_state\": 42}},\n}\n\npipeline = SkyulfPipeline(config)\nmetrics = pipeline.fit(dataset, target_column=\"target\")  # target_column ignored for (X, y) tuples\n\npreds = pipeline.predict(X_test)\n\n# Proof-like checks\nassert len(preds) == len(X_test)\nassert preds.index.equals(X_test.index)\nprint(\"OK: Skyulf fit/predict with sklearn-style train/test split\")\nprint(\"Metrics keys:\", list(metrics.keys()))\n</code></pre>"},{"location":"user_guide/validation_vs_sklearn.html#1b-side-by-side-run-sklearn-pipeline-vs-skyulfpipeline","title":"1b) Side-by-side run: sklearn Pipeline vs SkyulfPipeline","text":"<p>This comparison proves both stacks can run the same shape of workflow on the same split. We do not assert equality of predictions (different defaults / encodings can legitimately differ).</p> <p>For a stronger numeric sanity check, we also compute and print test accuracy for both pipelines and the absolute difference.</p> <pre><code>from __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.impute import SimpleImputer as SkSimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline as SkPipeline\nfrom sklearn.preprocessing import OneHotEncoder as SkOneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom skyulf import SkyulfPipeline\nfrom skyulf.data.dataset import SplitDataset\n\nnp.random.seed(42)\n\n# Real dataset + extra categorical feature + missingness\nraw = load_breast_cancer(as_frame=True)\ndf = raw.frame.copy().rename(columns={\"target\": \"label\"})\n\ndf[\"radius_band\"] = pd.cut(\n    df[\"mean radius\"],\n    bins=[0, 12, 15, 100],\n    labels=[\"small\", \"medium\", \"large\"],\n    include_lowest=True,\n)\n\nmissing_idx = np.random.choice(df.index, size=25, replace=False)\ndf.loc[missing_idx, \"mean texture\"] = np.nan\n\ntarget_col = \"label\"\ncat_cols = [\"radius_band\"]\nnum_cols = [c for c in df.columns if c not in [target_col, *cat_cols]]\n\nX = df[num_cols + cat_cols]\ny = df[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# --- scikit-learn pipeline ---\nnumeric_features = num_cols\ncategorical_features = cat_cols\n\nnumeric_pipe = SkPipeline(\n    steps=[\n        (\"imputer\", SkSimpleImputer(strategy=\"mean\")),\n        (\"scaler\", StandardScaler()),\n    ]\n)\n\ncategorical_pipe = SkPipeline(\n    steps=[\n        (\"imputer\", SkSimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", SkOneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n    ]\n)\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipe, numeric_features),\n        (\"cat\", categorical_pipe, categorical_features),\n    ],\n    remainder=\"drop\",\n)\n\nsk_model = LogisticRegression(max_iter=1000, random_state=42)\nsk = SkPipeline(steps=[(\"preprocess\", preprocess), (\"model\", sk_model)])\nsk.fit(X_train, y_train)\nsk_preds = pd.Series(sk.predict(X_test), index=X_test.index)\nsk_acc = accuracy_score(y_test, sk_preds)\n\n# --- Skyulf pipeline ---\ntrain_df = X_train.copy()\ntrain_df[target_col] = y_train\ntest_df = X_test.copy()\ntest_df[target_col] = y_test\n\ndataset = SplitDataset(train=train_df, test=test_df, validation=None)\nskyulf_config = {\n    \"preprocessing\": [\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": num_cols},\n        },\n        {\n            \"name\": \"impute_cat\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"most_frequent\", \"columns\": cat_cols},\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": cat_cols, \"drop_original\": True, \"handle_unknown\": \"ignore\"},\n        },\n        {\n            \"name\": \"scale\",\n            \"transformer\": \"StandardScaler\",\n            \"params\": {\"columns\": num_cols},\n        },\n    ],\n    \"modeling\": {\"type\": \"logistic_regression\", \"params\": {\"max_iter\": 1000, \"random_state\": 42}},\n}\n\nsky = SkyulfPipeline(skyulf_config)\n_ = sky.fit(dataset, target_column=target_col)\nsky_preds = sky.predict(X_test)\nsky_acc = accuracy_score(y_test, sky_preds)\ndelta = abs(sk_acc - sky_acc)\n\nassert sk_preds.index.equals(X_test.index)\nassert sky_preds.index.equals(X_test.index)\n\n# Proof-like checks\nassert len(sk_preds) == len(X_test)\nassert sk_preds.index.equals(X_test.index)\nassert len(sky_preds) == len(X_test)\nassert sky_preds.index.equals(X_test.index)\n\nprint(\"OK: sklearn Pipeline and SkyulfPipeline both run\")\nprint(f\"sklearn test accuracy: {sk_acc:.4f}\")\nprint(f\"skyulf  test accuracy: {sky_acc:.4f}\")\nprint(f\"delta accuracy: {delta:.4f}\")\n\n# --- Classification metrics (side-by-side) ---\nsk_report = classification_report(y_test, sk_preds, output_dict=True, zero_division=0)\nsky_report = classification_report(y_test, sky_preds, output_dict=True, zero_division=0)\n\nsk_df = pd.DataFrame(sk_report).T\nsky_df = pd.DataFrame(sky_report).T\n\n# Keep a consistent row order: class labels first, then summary rows (if present)\nlabel_rows = [str(v) for v in sorted(pd.unique(y_test))]\nsummary_rows = [r for r in [\"accuracy\", \"macro avg\", \"weighted avg\"] if r in sk_df.index]\nrow_order = [r for r in label_rows if r in sk_df.index] + summary_rows\n\nsk_df = sk_df.loc[row_order]\nsky_df = sky_df.loc[row_order]\n\nside_by_side = pd.concat(\n    {\n        \"sklearn\": sk_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]],\n        \"skyulf\": sky_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]],\n    },\n    axis=1,\n)\n\nprint(\"\\nClassification report (side-by-side):\")\nprint(side_by_side.to_string())\n\nlabels = sorted(pd.unique(y_test))\ncm_sk = confusion_matrix(y_test, sk_preds, labels=labels)\ncm_sky = confusion_matrix(y_test, sky_preds, labels=labels)\n\ncm_index = [f\"true_{l}\" for l in labels]\ncm_cols = [f\"pred_{l}\" for l in labels]\n\ncm_sk_df = pd.DataFrame(cm_sk, index=cm_index, columns=cm_cols)\ncm_sky_df = pd.DataFrame(cm_sky, index=cm_index, columns=cm_cols)\n\nprint(\"\\nConfusion matrix (sklearn):\")\nprint(cm_sk_df.to_string())\nprint(\"\\nConfusion matrix (skyulf):\")\nprint(cm_sky_df.to_string())\n\nprint(\"sklearn preds head:\")\nprint(sk_preds.head())\nprint(\"skyulf preds head:\")\nprint(sky_preds.head())\n\nassert (sk_pred.values == sky_pred.values).all()\nprint(\"Predictions match exactly.\")\n</code></pre>"},{"location":"user_guide/validation_vs_sklearn.html#example-output-from-the-notebook","title":"Example output (from the notebook)","text":"<p>This is the exact output from one notebook run (same dataset, same random seed/split):</p> <p>Classification report (side-by-side):</p> class/avg sklearn precision sklearn recall sklearn f1-score sklearn support skyulf precision skyulf recall skyulf f1-score skyulf support 0 0.962963 0.981132 0.971963 53.000000 0.962963 0.981132 0.971963 53.000000 1 0.988764 0.977778 0.983240 90.000000 0.988764 0.977778 0.983240 90.000000 accuracy 0.979021 0.979021 0.979021 0.979021 0.979021 0.979021 0.979021 0.979021 macro avg 0.975864 0.979455 0.977601 143.000000 0.975864 0.979455 0.977601 143.000000 weighted avg 0.979201 0.979021 0.979060 143.000000 0.979201 0.979021 0.979060 143.000000 <p>Confusion matrix (sklearn):</p> pred_0 pred_1 true_0 52 1 true_1 2 88 <p>Confusion matrix (skyulf):</p> pred_0 pred_1 true_0 52 1 true_1 2 88"},{"location":"user_guide/validation_vs_sklearn.html#2-proof-of-leakage-prevention-train-only-learned-params","title":"2) Proof of leakage prevention (train-only learned params)","text":"<p>A common leakage bug is fitting preprocessing on the full dataset.</p> <p>Here we construct a dataset where train and test have very different distributions. If an imputer learns the mean from the full dataset, it will be pulled toward the test distribution.</p> <p>Skyulf\u2019s pattern learns from train only (Calculator) and applies to test (Applier).</p> <pre><code>from __future__ import annotations\n\nimport pandas as pd\n\nfrom skyulf.preprocessing.imputation import SimpleImputerCalculator\n\n# Train has small ages; test has huge ages.\nX_train = pd.DataFrame({\"age\": [1.0, 2.0, None, 2.0]})\ny_train = pd.Series([0, 1, 0, 1])\n\nX_test = pd.DataFrame({\"age\": [1000.0, None, 1200.0]})\ny_test = pd.Series([0, 1, 1])\n\ncfg = {\"strategy\": \"mean\", \"columns\": [\"age\"]}\n\n# What train-only mean should be (ignoring NaNs)\nexpected_train_mean = float(pd.Series([1.0, 2.0, 2.0]).mean())\n\nparams = SimpleImputerCalculator().fit((X_train, y_train), cfg)\nlearned_mean = float(params[\"fill_values\"][\"age\"])\n\n# Proof: learned mean equals train mean (not influenced by test)\nassert abs(learned_mean - expected_train_mean) &lt; 1e-12\n\n# For comparison only: full-data mean would be very different\nfull_mean = float(pd.concat([X_train[\"age\"], X_test[\"age\"]]).mean())\nassert abs(full_mean - expected_train_mean) &gt; 1.0\n\nprint(\"OK: SimpleImputer learns from train only\")\nprint(\"train_mean:\", expected_train_mean)\nprint(\"full_mean:\", full_mean)\nprint(\"learned_mean:\", learned_mean)\n</code></pre>"},{"location":"user_guide/validation_vs_sklearn.html#3-what-this-proves-and-what-it-doesnt","title":"3) What this proves (and what it doesn\u2019t)","text":"<ul> <li>Proves the API supports sklearn-style <code>X/y</code> workflows and produces aligned predictions.</li> <li>Proves at least one common leakage-sensitive node (<code>SimpleImputer</code>) learns its statistics from the provided training data.</li> </ul> <p>It does not claim Skyulf will produce identical predictions to an arbitrary sklearn pipeline, because:</p> <ul> <li>different defaults/hyperparameters,</li> <li>different encoding conventions,</li> <li>and different ordering of operations</li> </ul> <p>can all change results while still being correct.</p>"}]}