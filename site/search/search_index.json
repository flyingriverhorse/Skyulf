{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Skyulf","text":"<p>Skyulf is a self-hosted MLOps platform with:</p> <ul> <li>a FastAPI backend</li> <li>a React frontend</li> <li>a standalone Python ML library: skyulf-core</li> </ul>"},{"location":"index.html#architecture","title":"Architecture","text":"<p>Skyulf is built on a modern stack:</p> <ul> <li>Frontend: React + TypeScript + React Flow</li> <li>Backend: FastAPI + Celery + Redis</li> <li>Core Library: <code>skyulf-core</code> (Standalone Python package)</li> <li>Data Engine: Hybrid Polars (Ingestion) + Pandas (ML)</li> </ul> <p>See Architecture and Data Architecture for details.</p>"},{"location":"index.html#if-you-are-here-for-skyulf-core","title":"If you are here for skyulf-core","text":"<p>Start with:</p> <ul> <li>User Guide \u2192 Overview</li> <li>User Guide \u2192 Pipeline Quickstart</li> <li>Reference \u2192 Preprocessing Nodes / Modeling Nodes</li> </ul>"},{"location":"index.html#backend-quick-start","title":"Backend quick start","text":"<pre><code>pip install -r requirements-fastapi.txt\npython run_skyulf.py\n</code></pre> <p>Open:</p> <ul> <li>http://127.0.0.1:8000</li> </ul>"},{"location":"architecture.html","title":"Architecture","text":"<p>Skyulf is split into three pieces with strict boundaries:</p> <ol> <li>skyulf-core (this docs set focuses on this)</li> <li>A standalone Python ML library.</li> <li>Implements a strict Calculator \u2192 Applier pattern for every node.</li> <li>Depends on Pandas/Numpy/Scikit-Learn (and a few optional ML utilities).</li> <li>backend</li> <li>FastAPI + Celery orchestration layer.</li> <li>Handles ingestion, jobs, persistence, and exposes REST APIs.</li> <li>frontend</li> <li>React + TypeScript UI (ML canvas).</li> <li>Builds pipeline configs and talks to the backend.</li> </ol>"},{"location":"architecture.html#the-calculator-applier-pattern","title":"The Calculator \u2192 Applier Pattern","text":"<p>Skyulf-core separates learning from transformation:</p> <ul> <li>Calculator: <code>fit(data, config) -&gt; params</code></li> <li>Learns statistics / encoders / models.</li> <li>Returns a serializable <code>params</code> dictionary.</li> <li>Applier: <code>apply(data, params) -&gt; transformed_data</code></li> <li>Stateless transformer.</li> <li>Applies learned parameters.</li> </ul> <p>This makes pipelines easier to persist and safer to run in production:</p> <ul> <li>Learning happens on train.</li> <li>The learned state is explicit.</li> <li>Applying is pure and repeatable.</li> </ul>"},{"location":"architecture.html#hybrid-engine-polars-pandas","title":"Hybrid Engine (Polars &amp; Pandas)","text":"<p>Skyulf employs a Hybrid Engine architecture to maximize performance:</p> <ul> <li>Polars: Used for high-performance data ingestion (ETL) and stateless transformations (Scaling, Imputation, Encoding) where possible.</li> <li>Pandas/Numpy: Used for stateful learning (Calculators) and compatibility with Scikit-Learn models.</li> </ul> <p>The system automatically detects the input data type (<code>pd.DataFrame</code> or <code>pl.DataFrame</code>) and dispatches to the appropriate optimized path.</p>"},{"location":"architecture.html#node-registry","title":"Node Registry","text":"<p>Skyulf uses a Registry Pattern to decouple the pipeline orchestrator from specific node implementations.</p> <ul> <li>Registration: Nodes self-register using the <code>@NodeRegistry.register(\"NodeName\", ApplierClass)</code> decorator on the Calculator class.</li> <li>Discovery: The pipeline dynamically looks up the Calculator and Applier classes by name at runtime.</li> <li>Extensibility: New nodes can be added simply by creating a new file and decorating the class; no changes to <code>pipeline.py</code> are required.</li> </ul>"},{"location":"architecture.html#data-catalog","title":"Data Catalog","text":"<p>To decouple data loading from the execution engine, Skyulf uses a Data Catalog pattern.</p> <ul> <li>Interface: <code>DataCatalog</code> (in <code>skyulf-core</code>) defines the contract for loading data by identifier.</li> <li>Implementation: <code>FileSystemCatalog</code> (in <code>backend</code>) implements this interface to load files from the local filesystem.</li> <li>Usage: The <code>PipelineEngine</code> is injected with a catalog instance. Nodes request data by ID (or path), and the catalog handles the retrieval.</li> </ul>"},{"location":"architecture.html#pipeline-data-flow","title":"Pipeline Data Flow","text":"<p>At runtime, <code>SkyulfPipeline</code> orchestrates:</p> <ol> <li>Preprocessing: <code>FeatureEngineer</code></li> <li>Executes a list of steps (each step is a transformer).</li> <li>Some steps change the data structure (e.g., splitters) and are handled specially.</li> <li>Modeling: <code>StatefulEstimator</code></li> <li>Trains a model on the train split.</li> <li>Optionally evaluates on test/validation.</li> </ol> <p>High-level flow:</p> <pre><code>Raw DataFrame\n  \u2514\u2500 FeatureEngineer.fit_transform(...)  -&gt; DataFrame or SplitDataset\n        \u2514\u2500 (optionally) SplitDataset.train / test / validation\n              \u2514\u2500 StatefulEstimator.fit_predict(...) -&gt; predictions\n</code></pre>"},{"location":"architecture.html#avoiding-data-leakage","title":"Avoiding Data Leakage","text":"<p>If you split first (or provide a <code>SplitDataset</code>), calculators should learn only on the train split. This prevents leakage of statistics from test/validation.</p> <p>See the User Guide section \u201cSplitDataset &amp; Leakage\u201d for recommended patterns.</p>"},{"location":"data_architecture.html","title":"Skyulf Architecture &amp; Data Flow","text":""},{"location":"data_architecture.html#1-the-dual-engine-strategy-polars-pandas","title":"1. The Dual-Engine Strategy: Polars &amp; Pandas","text":"<p>Skyulf uses a hybrid approach to data processing to balance performance (Ingestion) with compatibility (Machine Learning).</p>"},{"location":"data_architecture.html#a-ingestion-preview-polars","title":"A. Ingestion &amp; Preview (Polars)","text":"<ul> <li>Engine: Polars</li> <li>Why: Polars is significantly faster than Pandas for reading large files (CSV, Parquet) and performing initial scans. It uses lazy evaluation and multi-threading.</li> <li>Where: <ul> <li><code>backend/data_ingestion/</code>: Reading uploaded files.</li> <li><code>backend/services/data_service.py</code>: Generating data previews and samples for the UI.</li> </ul> </li> <li>Format: Data is kept in Polars DataFrames or converted to Python dictionaries (<code>to_dicts()</code>) for JSON API responses.</li> </ul>"},{"location":"data_architecture.html#b-machine-learning-core-pandasnumpy","title":"B. Machine Learning Core (Pandas/Numpy)","text":"<ul> <li>Engine: Pandas &amp; Numpy</li> <li>Why: The vast majority of the Python ML ecosystem (Scikit-Learn, XGBoost, LightGBM) is built around Numpy arrays and Pandas DataFrames.</li> <li>Where: <ul> <li><code>skyulf-core/</code>: The actual ML pipeline execution.</li> <li><code>backend/ml_pipeline/execution/</code>: The orchestration layer that runs the core library.</li> </ul> </li> <li>Format: Data is converted to Pandas DataFrames before entering the <code>SkyulfPipeline</code>.</li> </ul>"},{"location":"data_architecture.html#c-the-bridge-apache-arrow","title":"C. The Bridge: Apache Arrow","text":"<ul> <li>Technology: Apache Arrow</li> <li>Role: Arrow is the in-memory columnar format that both Polars and Pandas (2.0+) support.</li> <li>Benefit: It allows for zero-copy (or near zero-copy) conversion between Polars and Pandas. When we load data with Polars and then hand it to Scikit-Learn, we aren't serializing/deserializing text; we are just passing memory pointers. This makes the \"switch\" extremely efficient.</li> </ul>"},{"location":"data_architecture.html#2-future-architecture-the-ai-hub","title":"2. Future Architecture: The AI Hub","text":"<p>Skyulf is evolving from a Tabular ML tool into a multi-modal AI Hub.</p>"},{"location":"data_architecture.html#the-node-abstraction","title":"The \"Node\" Abstraction","text":"<p>The core architecture (Graph -&gt; Nodes -&gt; Artifacts) is agnostic to the data type. *   Today: Nodes process <code>pd.DataFrame</code>. *   Tomorrow: Nodes will process <code>ImageBatch</code>, <code>TextCorpus</code>, or <code>HuggingFaceDataset</code>.</p>"},{"location":"data_architecture.html#planned-engines","title":"Planned Engines","text":"<p>We will introduce specialized engines alongside <code>PandasEngine</code> and <code>PolarsEngine</code>: 1.  TorchEngine: For Deep Learning workflows (PyTorch). 2.  HuggingFaceEngine: For NLP pipelines (Tokenizers, Transformers). 3.  LlamaIndexEngine: For RAG and LLM orchestration.</p>"},{"location":"data_architecture.html#artifact-store-evolution","title":"Artifact Store Evolution","text":"<p>The <code>ArtifactStore</code> will evolve to handle: *   Large Blobs: Storing images/audio directly or via S3 references. *   Model Weights: Managing <code>.pt</code>, <code>.onnx</code>, and <code>.gguf</code> files efficiently. *   Vector Indices: Storing FAISS/ChromaDB indices for RAG.</p> <p>See <code>ROADMAP.md</code> for the detailed timeline.</p>"},{"location":"performance.html","title":"Performance &amp; Scalability","text":"<p>Skyulf is designed to handle production-scale workloads efficiently. By leveraging a Hybrid Engine architecture, it automatically selects the best tool for the job: Polars for high-performance data transformation and Pandas/Scikit-Learn for compatibility with the vast ML ecosystem.</p>"},{"location":"performance.html#benchmarks","title":"Benchmarks","text":"<p>We regularly benchmark Skyulf to ensure it meets performance standards. Below are the results from our latest internal benchmarks comparing the Pandas-only path vs. the Polars-optimized path.</p>"},{"location":"performance.html#scenario-large-scale-transformation","title":"Scenario: Large Scale Transformation","text":"<ul> <li>Dataset: 2,000,000 rows, 20 columns (10 numeric, 10 categorical).</li> <li>Pipeline:<ol> <li>Imputation (Mean)</li> <li>Standard Scaling (10 columns)</li> <li>One-Hot Encoding (5 columns)</li> <li>Hash Encoding (5 columns)</li> </ol> </li> <li>Hardware: Standard Dev Environment</li> </ul> Engine Execution Time Speedup Pandas 11.91s 1.0x Polars 3.04s 3.91x \ud83d\ude80"},{"location":"performance.html#why-is-polars-faster","title":"Why is Polars Faster?","text":"<ol> <li>Parallelization: Polars executes operations in parallel across available CPU cores, whereas Pandas is largely single-threaded.</li> <li>Memory Efficiency: Polars uses Arrow memory format and optimizes memory usage, reducing overhead during large transformations.</li> <li>Lazy Evaluation: (Future Roadmap) While Skyulf currently uses Polars in eager mode for compatibility, the underlying engine allows for query optimization.</li> </ol>"},{"location":"performance.html#optimization-tips","title":"Optimization Tips","text":"<p>To get the most out of Skyulf's performance:</p> <ol> <li>Use Polars for Ingestion: When loading data in your backend or scripts, prefer <code>pl.read_parquet()</code> or <code>pl.read_csv()</code>. Skyulf will detect the Polars DataFrame and stay in the fast lane.</li> <li>Batch Processing: For massive datasets (larger than RAM), consider splitting your data into batches. Skyulf's <code>Applier</code> is stateless and thread-safe, making it ideal for parallel batch processing.</li> <li>Avoid \"Slow\" Nodes: Some Scikit-Learn transformers (like <code>IterativeImputer</code> or complex kernel approximations) are inherently computationally expensive and may bottleneck the pipeline regardless of the dataframe engine.</li> </ol>"},{"location":"contributing/writing_docs.html","title":"Writing Documentation","text":"<p>We use MkDocs with the Material for MkDocs theme to build our documentation.</p>"},{"location":"contributing/writing_docs.html#setup","title":"Setup","text":"<ol> <li> <p>Install Dependencies:     <code>bash     pip install mkdocs mkdocs-material mkdocstrings[python]</code></p> </li> <li> <p>Serve Locally:     <code>bash     mkdocs serve</code>     This will start a local server at <code>http://127.0.0.1:8000</code> that auto-reloads when you change files.</p> </li> </ol>"},{"location":"contributing/writing_docs.html#directory-structure","title":"Directory Structure","text":"<ul> <li><code>docs/</code>: Contains all markdown files.</li> <li><code>mkdocs.yml</code>: The main configuration file.</li> </ul>"},{"location":"contributing/writing_docs.html#adding-a-new-page","title":"Adding a New Page","text":"<ol> <li>Create a new <code>.md</code> file in <code>docs/</code>.</li> <li>Add the file to the <code>nav</code> section in <code>mkdocs.yml</code>.</li> </ol>"},{"location":"contributing/writing_docs.html#writing-api-documentation","title":"Writing API Documentation","text":"<p>We use <code>mkdocstrings</code> to auto-generate API docs from Python docstrings.</p> <p>To document a module, class, or function, use the <code>:::</code> directive:</p> <pre><code>::: core.my_module.MyClass\n</code></pre>"},{"location":"contributing/writing_docs.html#docstring-style","title":"Docstring Style","text":"<p>We follow the Google Style Guide for docstrings.</p> <pre><code>def my_function(arg1: int, arg2: str) -&gt; bool:\n    \"\"\"\n    Does something amazing.\n\n    Args:\n        arg1: The first argument.\n        arg2: The second argument.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"examples/leakage_proof.html","title":"Proof of Trust: Preventing Data Leakage with Skyulf","text":"<p>One of the biggest risks in Machine Learning is Data Leakage: when information from the test set (or the future) accidentally \"leaks\" into the training process. This creates models that look perfect during training but fail in production.</p> <p>Common sources of leakage:</p> <ol> <li>Imputation: Filling missing values in the Test set using the mean of the entire dataset (including Test).</li> <li>Scaling: Normalizing Test data using the min/max of the entire dataset.</li> <li>Target Encoding: Encoding categorical features using the target mean of the entire dataset.</li> </ol>"},{"location":"examples/leakage_proof.html#the-skyulf-guarantee","title":"The Skyulf Guarantee","text":"<p>Skyulf prevents this by design using the Calculator / Applier pattern.</p> <ul> <li>Calculator: Learns statistics only from the Training data.</li> <li>Applier: Applies those learned statistics to Test data blindly.</li> </ul> <p>This example proves this behavior using the Titanic dataset. We will:</p> <ol> <li>Load the dataset.</li> <li>Split it into Train/Test.</li> <li>Run a Skyulf Pipeline.</li> <li>Mathematically verify that the Test data was processed using only Training statistics.</li> </ol>"},{"location":"examples/leakage_proof.html#1-setup-and-data-loading","title":"1. Setup and Data Loading","text":"<pre><code>import sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom skyulf import SkyulfPipeline\nfrom skyulf.data.dataset import SplitDataset\n\n# Load Titanic Dataset\nprint(\"Loading Titanic dataset...\")\ntitanic = fetch_openml(\"titanic\", version=1, as_frame=True)\ndf = titanic.frame\n\n# Select relevant columns for demonstration\n# 'sex': Categorical (needs encoding)\n# 'age': Numeric with missing values (needs imputation)\n# 'fare': Numeric (needs scaling)\n# 'survived': Target\ncols = ['sex', 'age', 'fare', 'survived']\ndf = df[cols].copy()\n\n# Convert target to int\ndf['survived'] = df['survived'].astype(int)\n\nprint(f\"Dataset Shape: {df.shape}\")\nprint(df.head())\nprint(\"\\nMissing Values:\\n\", df.isnull().sum())\n</code></pre> <p>Output:</p> <pre><code>Loading Titanic dataset...\nDataset Shape: (1309, 4)\n      sex   age      fare  survived\n0  female  29.0  211.3375         1\n1    male   0.9151.5500         1\n2  female   2.0  151.5500         0\n3    male  30.0  151.5500         0\n4  female  25.0  151.5500         0\n\nMissing Values:\n sex           0\nage         263\nfare          1\nsurvived      0\ndtype: int64\n</code></pre>"},{"location":"examples/leakage_proof.html#2-split-data","title":"2. Split Data","text":"<p>We split BEFORE any processing to simulate a real-world scenario.</p> <pre><code>X = df.drop(columns=['survived'])\ny = df['survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create Skyulf Dataset\ndataset = SplitDataset(\n    train=pd.concat([X_train, y_train], axis=1),\n    test=pd.concat([X_test, y_test], axis=1)\n)\n\nprint(f\"Train Shape: {dataset.train.shape}\")\nprint(f\"Test Shape: {dataset.test.shape}\")\n</code></pre> <p>Output:</p> <pre><code>Train Shape: (916, 4)\nTest Shape: (393, 4)\n</code></pre>"},{"location":"examples/leakage_proof.html#3-define-pipeline","title":"3. Define Pipeline","text":"<p>We intentionally use methods that are prone to leakage if done wrong.</p> <pre><code>config = {\n    \"preprocessing\": [\n        # Imputation 1: Age (Train Mean)\n        {\n            \"name\": \"impute_age\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]}\n        },\n        # Imputation 2: Fare (Train Mean) - Added for robustness\n        {\n            \"name\": \"impute_fare\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"fare\"]}\n        },\n        # Scaling: Should use Train Mean/Std\n        {\n            \"name\": \"scale_fare\",\n            \"transformer\": \"StandardScaler\",\n            \"params\": {\"columns\": [\"fare\"]}\n        },\n        # Target Encoding: Should use Train Target Mean\n        # This is the most dangerous one! If it sees Test target, it's 100% leakage.\n        {\n            \"name\": \"encode_sex\",\n            \"transformer\": \"TargetEncoder\",\n            \"params\": {\"columns\": [\"sex\"], \"target_column\": \"survived\"}\n        }\n    ],\n    \"modeling\": {\n        \"type\": \"random_forest_classifier\",\n        \"params\": {\"n_estimators\": 10, \"random_state\": 42}\n    }\n}\n\npipeline = SkyulfPipeline(config)\n\n# Fit the pipeline\n# This runs fit() on Train and transform() on Test\nprint(\"Running pipeline...\")\nmetrics = pipeline.fit(dataset, target_column=\"survived\")\nprint(\"Pipeline execution complete.\")\n</code></pre> <p>Output:</p> <pre><code>Running pipeline...\nPipeline execution complete.\n</code></pre>"},{"location":"examples/leakage_proof.html#4-verification-1-imputation-leakage","title":"4. Verification 1: Imputation Leakage","text":"<p>Did we fill missing 'age' in Test with the Train mean?</p> <pre><code># Get the fitted imputer from the pipeline\nimputer_step = pipeline.feature_engineer.fitted_steps[0]\nassert imputer_step['name'] == 'impute_age'\n\n# The fitted transformer is stored in 'artifact'\nartifact = imputer_step['artifact']\nfill_values = artifact['fill_values']\n\n# Calculate Train Mean manually\ntrain_age_mean = X_train['age'].mean()\nprint(f\"Train Age Mean: {train_age_mean:.4f}\")\n\n# Check what the imputer learned\nlearned_mean = fill_values['age']\nprint(f\"Imputer Learned Mean: {learned_mean:.4f}\")\n\n# Verify\nnp.testing.assert_allclose(train_age_mean, learned_mean)\n\n# --- NEW: Explicitly check against Full Dataset Mean ---\nfull_age_mean = df['age'].mean()\nprint(f\"Full Dataset Age Mean: {full_age_mean:.4f}\")\n\n# Assert that learned mean is NOT the full mean (additional sanity check)\n# Note: In some datasets, train mean could equal full mean by chance.\nif abs(learned_mean - full_age_mean) &gt; 1e-4:\n    print(\"\u2705 Imputation Sanity Check Passed: Learned mean differs from Full Dataset mean.\")\nelse:\n    print(\"\u26a0\ufe0f Warning: Train mean equals Full mean (could be chance, but check split).\")\n\nprint(\"\u2705 Imputation Proof: The imputer learned the mean ONLY from the Training set.\")\n</code></pre> <p>Output:</p> <pre><code>Train Age Mean: 29.1023\nImputer Learned Mean: 29.1023\nFull Dataset Age Mean: 29.8811\n\u2705 Imputation Sanity Check Passed: Learned mean differs from Full Dataset mean.\n\u2705 Imputation Proof: The imputer learned the mean ONLY from the Training set.\n</code></pre>"},{"location":"examples/leakage_proof.html#5-verification-2-scaling-leakage","title":"5. Verification 2: Scaling Leakage","text":"<p>Did we scale 'fare' using Train Mean/Std?</p> <pre><code># Note: Index is 2 because we added impute_fare at index 1\nscaler_step = pipeline.feature_engineer.fitted_steps[2]\nassert scaler_step['name'] == 'scale_fare'\n\nartifact = scaler_step['artifact']\n\n# Calculate Train Stats manually (Exact)\n# We must use the imputed fare for manual calculation to match the pipeline's flow.\ntrain_fare_imputed = X_train[\"fare\"].fillna(X_train[\"fare\"].mean())\ntrain_fare_mean = train_fare_imputed.mean()\ntrain_fare_std  = train_fare_imputed.std(ddof=0) # Sklearn uses ddof=0 for std\n\nprint(f\"Train Fare Mean: {train_fare_mean:.4f}, Std: {train_fare_std:.4f}\")\n\n# Check what the scaler learned\ncolumns = artifact['columns']\nfare_idx = columns.index('fare')\n\nlearned_mean = artifact['mean'][fare_idx]\nlearned_scale = artifact['scale'][fare_idx]\nprint(f\"Scaler Learned Mean: {learned_mean:.4f}, Scale: {learned_scale:.4f}\")\n\n# Verify (Tight tolerance now possible)\nnp.testing.assert_allclose(train_fare_mean, learned_mean)\nnp.testing.assert_allclose(train_fare_std, learned_scale)\n\n# --- NEW: Explicitly check against Full Dataset Stats ---\nfull_fare_mean = df['fare'].mean()\nfull_fare_std = df['fare'].std(ddof=0)\nprint(f\"Full Dataset Fare Mean: {full_fare_mean:.4f}, Std: {full_fare_std:.4f}\")\n\n# Additional Sanity Checks\nif abs(learned_mean - full_fare_mean) &gt; 1e-4:\n    print(\"\u2705 Scaling Mean Sanity Check Passed: Learned mean differs from Full Dataset mean.\")\nelse:\n    print(\"\u26a0\ufe0f Warning: Train mean equals Full mean.\")\n\nif abs(learned_scale - full_fare_std) &gt; 1e-4:\n    print(\"\u2705 Scaling Std Sanity Check Passed: Learned std differs from Full Dataset std.\")\nelse:\n    print(\"\u26a0\ufe0f Warning: Train std equals Full std.\")\n\nprint(\"\u2705 Scaling Proof: The scaler learned stats ONLY from the Training set.\")\n</code></pre> <p>Output:</p> <pre><code>Train Fare Mean: 33.7092, Std: 52.7829\nScaler Learned Mean: 33.7092, Scale: 52.7829\nFull Dataset Fare Mean: 33.2955, Std: 51.7389\n\u2705 Scaling Mean Sanity Check Passed: Learned mean differs from Full Dataset mean.\n\u2705 Scaling Std Sanity Check Passed: Learned std differs from Full Dataset std.\n\u2705 Scaling Proof: The scaler learned stats ONLY from the Training set.\n</code></pre>"},{"location":"examples/leakage_proof.html#6-verification-3-target-encoding-leakage","title":"6. Verification 3: Target Encoding Leakage","text":"<p>Did we encode 'sex' using the Target Mean of the Training set?</p> <pre><code># Note: Index is 3\nencoder_step = pipeline.feature_engineer.fitted_steps[3]\nassert encoder_step['name'] == 'encode_sex'\n\nartifact = encoder_step['artifact']\nencoder = artifact['encoder_object']\n\n# Calculate Train Target Mean for 'sex' manually\ntrain_sex_means = pd.concat([X_train, y_train], axis=1).groupby('sex', observed=True)['survived'].mean()\nprint(\"Train Target Means:\\n\", train_sex_means)\n\n# Check what the encoder learned\ncategories = encoder.categories_[0]\nencodings = encoder.encodings_[0]\n\nprint(\"\\nEncoder Learned Means:\")\nfor cat, enc in zip(categories, encodings):\n    print(f\"  {cat}: {enc:.6f}\")\n\n# Verify\n# Note: Sklearn's TargetEncoder uses cross-fitting and shrinkage (smoothing), \n# so the learned encodings will NOT equal the raw conditional means of the training set.\n# However, they must be **invariant** to changes in the Test set.\n\nfull_sex_means = pd.concat([X, y], axis=1).groupby('sex', observed=True)['survived'].mean()\nprint(\"\\nFull Dataset Means (Leakage!):\\n\", full_sex_means)\n\n# Check 'male'\nmale_train_mean = train_sex_means['male']\nmale_full_mean = full_sex_means['male']\nmale_encoded = encodings[list(categories).index('male')]\n\nprint(f\"\\nComparison for 'male':\")\nprint(f\"  Train Mean: {male_train_mean:.6f}\")\nprint(f\"  Full Mean:  {male_full_mean:.6f}\")\nprint(f\"  Encoded:    {male_encoded:.6f}\")\n\nassert abs(male_encoded - male_full_mean) &gt; 1e-4, \"Leakage detected! Encoded value matches Full Mean.\"\nprint(\"\u2705 Target Encoding Proof: The encoder did NOT use the full dataset statistics.\")\n</code></pre> <p>Output:</p> <pre><code>Train Target Means:\n sex\nfemale    0.694444\nmale      0.179054\nName: survived, dtype: float64\n\nEncoder Learned Means:\n  female: 0.693502\n  male: 0.179250\n\nFull Dataset Means (Leakage!):\n sex\nfemale    0.727468\nmale      0.190985\nName: survived, dtype: float64\n\nComparison for 'male':\n  Train Mean: 0.179054\n  Full Mean:  0.190985\n  Encoded:    0.179250\n\u2705 Target Encoding Proof: The encoder did NOT use the full dataset statistics.\n</code></pre>"},{"location":"examples/leakage_proof.html#7-the-ultimate-test-the-poisoned-dataset","title":"7. The Ultimate Test: The \"Poisoned\" Dataset","text":"<p>To prove beyond doubt that the Test set is ignored during training, we will run an experiment:</p> <ol> <li>Take the original dataset.</li> <li>\"Poison\" the Test set with extreme outliers and flipped labels.</li> <li>Train a new pipeline on this poisoned dataset.</li> <li>Compare the learned parameters with the original pipeline.</li> </ol> <p>Hypothesis: If there is NO leakage, the learned parameters (Imputation Mean, Scaling Stats, Encodings) must be identical to the original run, because the Training set hasn't changed.</p> <pre><code># Create a Poisoned Dataset\n# We keep Train exactly the same, but corrupt Test.\nX_test_poisoned = X_test.copy()\ny_test_poisoned = y_test.copy()\n\n# 1. Poison 'age' (Imputation)\n# Set all test ages to a massive number. If leakage exists, the mean will skyrocket.\nX_test_poisoned['age'] = 10000.0 \n\n# 2. Poison 'fare' (Scaling)\n# Set all test fares to a massive number.\nX_test_poisoned['fare'] = 1000000.0 \n\n# 3. Poison 'survived' (Target Encoding)\n# Flip all labels: 0-&gt;1, 1-&gt;0. If leakage exists, encodings will flip.\ny_test_poisoned = 1 - y_test_poisoned\n\n# Create Skyulf Dataset with Poisoned Test\ndataset_poisoned = SplitDataset(\n    train=pd.concat([X_train, y_train], axis=1),\n    test=pd.concat([X_test_poisoned, y_test_poisoned], axis=1)\n)\n\nprint(\"Poisoned Test Stats:\")\nprint(f\"  Age Mean: {X_test_poisoned['age'].mean():.2f}\")\nprint(f\"  Fare Mean: {X_test_poisoned['fare'].mean():.2f}\")\nprint(f\"  Target Mean: {y_test_poisoned.mean():.2f}\")\n\n# Run Pipeline on Poisoned Data\npipeline_poisoned = SkyulfPipeline(config) # Same config\nprint(\"\\nRunning pipeline on Poisoned Dataset...\")\npipeline_poisoned.fit(dataset_poisoned, target_column=\"survived\")\n\n# Compare Artifacts\n\n# 1. Imputation (Age)\noriginal_age_mean = pipeline.feature_engineer.fitted_steps[0]['artifact']['fill_values']['age']\npoisoned_age_mean = pipeline_poisoned.feature_engineer.fitted_steps[0]['artifact']['fill_values']['age']\n\nprint(f\"\\nImputation Comparison:\")\nprint(f\"  Original: {original_age_mean:.4f}\")\nprint(f\"  Poisoned: {poisoned_age_mean:.4f}\")\nnp.testing.assert_allclose(original_age_mean, poisoned_age_mean)\nprint(\"\u2705 Imputation is unaffected by Test data.\")\n\n# 2. Scaling (Fare)\n# Note: Index is 2\noriginal_scaler = pipeline.feature_engineer.fitted_steps[2]['artifact']\npoisoned_scaler = pipeline_poisoned.feature_engineer.fitted_steps[2]['artifact']\n\noriginal_fare_mean = original_scaler['mean'][0] # fare is only col\npoisoned_fare_mean = poisoned_scaler['mean'][0]\noriginal_fare_scale = original_scaler['scale'][0]\npoisoned_fare_scale = poisoned_scaler['scale'][0]\n\nprint(f\"\\nScaling Comparison:\")\nprint(f\"  Original Mean: {original_fare_mean:.4f}, Scale: {original_fare_scale:.4f}\")\nprint(f\"  Poisoned Mean: {poisoned_fare_mean:.4f}, Scale: {poisoned_fare_scale:.4f}\")\n\nnp.testing.assert_allclose(original_fare_mean, poisoned_fare_mean)\nnp.testing.assert_allclose(original_fare_scale, poisoned_fare_scale)\nprint(\"\u2705 Scaling is unaffected by Test data.\")\n\n# 3. Target Encoding\n# Note: Index is 3\noriginal_encodings = pipeline.feature_engineer.fitted_steps[3]['artifact']['encoder_object'].encodings_[0]\npoisoned_encodings = pipeline_poisoned.feature_engineer.fitted_steps[3]['artifact']['encoder_object'].encodings_[0]\n\nprint(f\"\\nTarget Encoding Comparison (First 5 values):\")\nprint(f\"  Original: {original_encodings[:5]}\")\nprint(f\"  Poisoned: {poisoned_encodings[:5]}\")\nnp.testing.assert_allclose(original_encodings, poisoned_encodings)\nprint(\"\u2705 Target Encoding is unaffected by Test labels.\")\n\nprint(\"\\n\ud83c\udf89 FINAL VERDICT: The pipeline is LEAKAGE-RESISTANT by design.\")\n</code></pre> <p>Output:</p> <pre><code>Poisoned Test Stats:\n  Age Mean: 10000.00\n  Fare Mean: 1000000.00\n  Target Mean: 0.57\n\nRunning pipeline on Poisoned Dataset...\n\nImputation Comparison:\n  Original: 29.1023\n  Poisoned: 29.1023\n\u2705 Imputation is unaffected by Test data.\n\nScaling Comparison:\n  Original Mean: 33.7092, Scale: 52.7829\n  Poisoned Mean: 33.7092, Scale: 52.7829\n\u2705 Scaling is unaffected by Test data.\n\nTarget Encoding Comparison (First 5 values):\n  Original: [0.69350186 0.17924998]\n  Poisoned: [0.69350186 0.17924998]\n\u2705 Target Encoding is unaffected by Test labels.\n\n\ud83c\udf89 FINAL VERDICT: The pipeline is LEAKAGE-RESISTANT by design.\n</code></pre>"},{"location":"examples/leakage_proof.html#conclusion","title":"Conclusion","text":"<p>We have mathematically verified that:</p> <ol> <li>Imputation on Test data used the Train Mean.</li> <li>Scaling on Test data used the Train Mean/Std.</li> <li>Target Encoding used train-derived target statistics (smoothed/cross-fitted), and is invariant to test labels.</li> </ol> <p>This proves that Skyulf pipelines are leakage-free by design. The strict separation of <code>fit()</code> (Calculator) and <code>transform()</code> (Applier) ensures that no information from the Test set (or future data) can influence the model training. Under adversarial \u2018poisoned test\u2019 conditions, the learned preprocessing artifacts are invariant to test data and test labels, providing strong empirical evidence that the pipeline fits strictly on training data for these steps.</p>"},{"location":"examples/quickstart.html","title":"Quickstart Guide","text":"<p>This guide demonstrates how to create a simple end-to-end pipeline using Skyulf Core.</p>"},{"location":"examples/quickstart.html#1-setup","title":"1. Setup","text":"<p>First, import the necessary modules.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom skyulf import SkyulfPipeline\n</code></pre>"},{"location":"examples/quickstart.html#2-create-dummy-data","title":"2. Create Dummy Data","text":"<p>We'll create a synthetic dataset with some missing values and categorical features.</p> <pre><code>def create_dummy_data(n: int = 200) -&gt; pd.DataFrame:\n    np.random.seed(42)\n    df = pd.DataFrame({\n        'age': np.random.randint(18, 80, n),\n        'income': np.random.normal(50000, 15000, n),\n        'city': np.random.choice(['New York', 'London', 'Paris'], n),\n        'is_customer': np.random.choice([0, 1], n),\n    })\n    # Introduce missing values\n    df.loc[0:10, 'income'] = np.nan\n    return df\n\ndata = create_dummy_data()\nprint(data.head())\n</code></pre> <p>Output:</p> <pre><code>   age        income      city  is_customer\n0   56           NaN     Paris            0\n1   69           NaN  New York            1\n2   46           NaN     Paris            0\n3   32           NaN     Paris            0\n4   60           NaN  New York            0\n</code></pre>"},{"location":"examples/quickstart.html#3-define-pipeline-configuration","title":"3. Define Pipeline Configuration","text":"<p>Skyulf pipelines are defined using a JSON-compatible dictionary. This makes them easy to serialize and store.</p> <pre><code>config = {\n    'preprocessing': [\n        # 1. Split Data into Train/Test\n        {\n            'name': 'split_data',\n            'transformer': 'TrainTestSplitter',\n            'params': {\n                'test_size': 0.2,\n                'target_column': 'is_customer',\n            },\n        },\n        # 2. Impute Missing Income\n        {\n            'name': 'impute_income',\n            'transformer': 'SimpleImputer',\n            'params': {\n                'columns': ['income'],\n                'strategy': 'mean',\n            },\n        },\n        # 3. Encode City (Categorical)\n        {\n            'name': 'encode_city',\n            'transformer': 'OneHotEncoder',\n            'params': {'columns': ['city']},\n        },\n        # 4. Scale Numeric Features\n        {\n            'name': 'scale_features',\n            'transformer': 'StandardScaler',\n            'params': {'columns': ['age', 'income']},\n        },\n    ],\n    'modeling': {\n        'type': 'random_forest_classifier',\n        'params': {'n_estimators': 50, 'max_depth': 5},\n    },\n}\n</code></pre>"},{"location":"examples/quickstart.html#4-run-pipeline","title":"4. Run Pipeline","text":"<p>Initialize and fit the pipeline.</p> <pre><code>pipeline = SkyulfPipeline(config)\nmetrics = pipeline.fit(data, target_column='is_customer')\nprint(metrics)\n</code></pre> <p>Output:</p> <pre><code>{\n    'preprocessing': {...},\n    'modeling': {\n        'accuracy': 0.85,\n        'f1_score': 0.82,\n        ...\n    }\n}\n</code></pre>"},{"location":"examples/quickstart.html#5-save-and-load","title":"5. Save and Load","text":"<p>Pipelines can be saved to disk and reloaded for inference.</p> <pre><code>import os\n\nartifact_path = 'my_model.pkl'\npipeline.save(artifact_path)\n\n# Load back\nloaded = SkyulfPipeline.load(artifact_path)\n\n# Predict on new data\nnew_data = pd.DataFrame({\n    'age': [25, 40],\n    'income': [60000, np.nan],\n    'city': ['London', 'Paris'],\n})\n\npredictions = loaded.predict(new_data)\nprint(predictions)\n\n# Cleanup\nif os.path.exists(artifact_path):\n    os.remove(artifact_path)\n</code></pre> <p>Output:</p> <pre><code>0    0\n1    1\ndtype: int64\n</code></pre>"},{"location":"guides/adding_new_engine.html","title":"Adding a New Compute Engine (Dask, Spark, etc.)","text":"<p>Skyulf is designed to be engine-agnostic. While it currently supports Pandas and Polars, the architecture allows for adding distributed computing engines like Dask or Spark (via PySpark) without rewriting the entire codebase.</p>"},{"location":"guides/adding_new_engine.html#architecture-overview","title":"Architecture Overview","text":"<p>The core abstraction is the <code>EngineRegistry</code> and the <code>BaseEngine</code> interface.</p> <ol> <li>Engine Registry: <code>skyulf.engines.registry.EngineRegistry</code> manages available engines.</li> <li>Base Engine: <code>skyulf.engines.base.BaseEngine</code> defines the contract that all engines must fulfill.</li> <li>Nodes: <code>Calculator</code> and <code>Applier</code> classes use the engine to perform operations.</li> </ol>"},{"location":"guides/adding_new_engine.html#steps-to-add-a-new-engine","title":"Steps to Add a New Engine","text":""},{"location":"guides/adding_new_engine.html#1-implement-the-engine-class","title":"1. Implement the Engine Class","text":"<p>Create a new file in <code>skyulf-core/skyulf/engines/</code> (e.g., <code>dask_engine.py</code>).</p> <pre><code>from typing import Any, List, Union\nimport dask.dataframe as dd\nfrom skyulf.engines.base import BaseEngine\n\nclass DaskEngine(BaseEngine):\n    @property\n    def name(self) -&gt; str:\n        return \"dask\"\n\n    def is_dataframe(self, data: Any) -&gt; bool:\n        return isinstance(data, dd.DataFrame)\n\n    def is_series(self, data: Any) -&gt; bool:\n        return isinstance(data, dd.Series)\n\n    def get_columns(self, df: Any) -&gt; List[str]:\n        return list(df.columns)\n\n    def shape(self, df: Any) -&gt; tuple:\n        # Dask shape is lazy, might need compute() or delayed\n        return (len(df), len(df.columns))\n\n    # ... implement other required methods\n</code></pre>"},{"location":"guides/adding_new_engine.html#2-register-the-engine","title":"2. Register the Engine","text":"<p>In <code>skyulf-core/skyulf/engines/__init__.py</code> or <code>registry.py</code>, register your new engine.</p> <pre><code>from .dask_engine import DaskEngine\nfrom .registry import EngineRegistry\n\nEngineRegistry.register(DaskEngine())\n</code></pre>"},{"location":"guides/adding_new_engine.html#3-update-nodes-optional-but-recommended","title":"3. Update Nodes (Optional but Recommended)","text":"<p>Most existing nodes use <code>SklearnBridge</code> or generic Python logic. *   SklearnBridge: If you implement <code>to_numpy()</code> or <code>to_pandas()</code> in your engine (or if the dataframe supports it), many nodes will work out-of-the-box by converting data to local memory (Pandas/Numpy). *   Native Implementation: For true distributed performance, you should update critical nodes (like <code>Scaling</code>, <code>Join</code>, <code>Aggregation</code>) to use native Dask/Spark API.</p> <p>Example of a node supporting multiple engines:</p> <pre><code>class StandardScalerCalculator(BaseCalculator):\n    def fit(self, df, config):\n        engine = get_engine(df)\n\n        if engine.name == \"dask\":\n            # Use Dask native logic\n            mean = df.mean().compute()\n            std = df.std().compute()\n            return {\"mean\": mean, \"std\": std}\n\n        elif engine.name == \"polars\":\n            # Use Polars logic\n            ...\n\n        else:\n            # Fallback to Pandas/Sklearn\n            ...\n</code></pre>"},{"location":"guides/adding_new_engine.html#4-update-data-ingestion","title":"4. Update Data Ingestion","text":"<p>Ensure <code>DataService</code> or your ingestion layer can load data into the new format (e.g., <code>dd.read_csv</code>).</p>"},{"location":"guides/adding_new_engine.html#summary","title":"Summary","text":"<p>You do not need to rewrite everything. 1.  Infrastructure: Add the Engine implementation (One-time setup). 2.  Compatibility: Existing nodes will work via fallback (conversion to Pandas), assuming the data fits in memory. 3.  Optimization: Gradually optimize specific nodes to use the new engine's native capabilities for large-scale data.</p>"},{"location":"guides/getting_started.html","title":"Getting Started (skyulf-core)","text":"<p>This page is the fastest path to running <code>skyulf-core</code> locally.</p>"},{"location":"guides/getting_started.html#install","title":"Install","text":"<p>From the repository root:</p> <pre><code>pip install -e ./skyulf-core\n</code></pre>"},{"location":"guides/getting_started.html#minimal-end-to-end-example","title":"Minimal end-to-end example","text":"<p><code>SkyulfPipeline</code> expects a configuration with:</p> <ul> <li><code>preprocessing</code>: a list of steps</li> <li><code>modeling</code>: a single model config</li> </ul> <pre><code>import pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\ndf = pd.DataFrame(\n    {\n        \"age\": [10, 20, None, 40],\n        \"city\": [\"A\", \"B\", \"A\", \"C\"],\n        \"target\": [0, 1, 0, 1],\n    }\n)\n\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"impute_age\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"columns\": [\"age\"], \"strategy\": \"mean\"},\n        },\n        {\n            \"name\": \"encode_city\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": [\"city\"], \"drop_original\": True},\n        },\n    ],\n    \"modeling\": {\n        \"type\": \"logistic_regression\",\n        \"params\": {\"max_iter\": 1000},\n    },\n}\n\npipeline = SkyulfPipeline(config)\nmetrics = pipeline.fit(df, target_column=\"target\")\npreds = pipeline.predict(df.drop(columns=[\"target\"]))\n\nprint(metrics)\nprint(preds.head())\n</code></pre>"},{"location":"guides/getting_started.html#next-steps","title":"Next steps","text":"<ul> <li>Read the User Guide section \u201cPipeline Quickstart\u201d for train/test splits.</li> <li>Use the Reference section for supported preprocessing and modeling nodes.</li> </ul>"},{"location":"guides/recipes.html","title":"Recipes","text":"<p>This page contains practical patterns for common workflows.</p>"},{"location":"guides/recipes.html#recipe-split-preprocess-train","title":"Recipe: split, preprocess, train","text":"<p>If you want evaluation metrics, use a split step (or pass a <code>SplitDataset</code>).</p> <pre><code>from __future__ import annotations\n\nimport pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\n# In real usage you'd likely load a file:\n# df = pd.read_csv(\"your_data.csv\")\n\ndf = pd.DataFrame(\n    {\n        \"free_text\": [\n            \"  Hello   World  \",\n            \"Skyulf is GREAT! \",\n            \"  hello\\tworld \",\n            \"  ML pipelines   \",\n            \"Encode + scale\",\n            \" text cleaning  \",\n        ],\n        \"country\": [\"TR\", \"TR\", \"DE\", \"DE\", \"TR\", \"DE\"],\n        \"age\": [10, 20, 30, 40, 50, 60],\n        \"target\": [0, 1, 0, 1, 1, 0],\n    }\n)\n\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"split\",\n            \"transformer\": \"TrainTestSplitter\",\n            \"params\": {\n                \"test_size\": 0.34,\n                \"validation_size\": 0.0,\n                \"random_state\": 42,\n                \"shuffle\": True,\n                \"stratify\": True,\n                \"target_column\": \"target\",\n            },\n        },\n        {\n            \"name\": \"text_clean\",\n            \"transformer\": \"TextCleaning\",\n            \"params\": {\n                \"columns\": [\"free_text\"],\n                \"operations\": [\n                    {\"op\": \"trim\", \"mode\": \"both\"},\n                    {\"op\": \"case\", \"mode\": \"lower\"},\n                    {\"op\": \"regex\", \"mode\": \"collapse_whitespace\"},\n                ],\n            },\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\n                \"columns\": [\"country\", \"free_text\"],\n                \"drop_original\": True,\n                \"handle_unknown\": \"ignore\",\n            },\n        },\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n        },\n        {\n            \"name\": \"scale\",\n            \"transformer\": \"StandardScaler\",\n            \"params\": {\"auto_detect\": True},\n        },\n    ],\n    \"modeling\": {\"type\": \"random_forest_classifier\", \"params\": {\"n_estimators\": 200}},\n}\n\npipeline = SkyulfPipeline(config)\nreport = pipeline.fit(df, target_column=\"target\")\nprint(report.get(\"modeling\"))\n</code></pre>"},{"location":"guides/recipes.html#recipe-safe-inference","title":"Recipe: safe inference","text":"<p>At inference time you typically:</p> <ol> <li>load a persisted pipeline</li> <li>call <code>predict(df)</code> on a dataframe without the target column</li> </ol> <pre><code>from __future__ import annotations\n\nimport tempfile\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\ndf = pd.DataFrame(\n    {\n        \"age\": [10, 20, 30, 40, 50, 60],\n        \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\"],\n        \"target\": [0, 1, 0, 1, 1, 0],\n    }\n)\n\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"split\",\n            \"transformer\": \"TrainTestSplitter\",\n            \"params\": {\n                \"test_size\": 0.2,\n                \"validation_size\": 0.0,\n                \"random_state\": 42,\n                \"shuffle\": True,\n                \"stratify\": True,\n                \"target_column\": \"target\",\n            },\n        },\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": [\"city\"], \"drop_original\": True},\n        },\n    ],\n    \"modeling\": {\n        \"type\": \"random_forest_classifier\",\n        \"params\": {\"n_estimators\": 50, \"random_state\": 42},\n    },\n}\n\nwith tempfile.TemporaryDirectory() as tmp:\n    model_path = Path(tmp) / \"model.pkl\"\n\n    pipeline = SkyulfPipeline(config)\n    _ = pipeline.fit(df, target_column=\"target\")\n    pipeline.save(model_path)\n\n    loaded = SkyulfPipeline.load(model_path)\n\n    new_df = pd.DataFrame({\"age\": [25, 55], \"city\": [\"A\", \"B\"]})\n    preds = loaded.predict(new_df)\n\nprint(preds)\n</code></pre>"},{"location":"guides/recipes.html#recipe-use-a-single-component-debug","title":"Recipe: use a single component (debug)","text":"<p>For debugging, you can run one node directly.</p> <pre><code>import pandas as pd\n\nfrom skyulf.preprocessing.imputation import SimpleImputerApplier, SimpleImputerCalculator\n\ndf = pd.DataFrame({\"A\": [1, 2, None, 4]})\nconfig = {\"columns\": [\"A\"], \"strategy\": \"mean\"}\n\nparams = SimpleImputerCalculator().fit(df, config)\nout = SimpleImputerApplier().apply(df, params)\n\nprint(params)\nprint(out)\n</code></pre>"},{"location":"reference/modeling_nodes.html","title":"Modeling Nodes","text":"<p>This page documents modeling configuration for <code>SkyulfPipeline</code>.</p>"},{"location":"reference/modeling_nodes.html#common-config-shape","title":"Common config shape","text":"<p><code>SkyulfPipeline</code> expects a modeling block like:</p> <pre><code>{\n  \"type\": \"logistic_regression\",\n  \"node_id\": \"model_node\",  # optional\n  \"params\": { ... }          # optional; estimator hyperparameters\n}\n</code></pre> <p>The sklearn wrapper supports both:</p> <ul> <li>Nested params (preferred): <code>{ \"params\": {\"C\": 1.0} }</code></li> <li>Flat params (legacy): <code>{ \"C\": 1.0, \"type\": \"...\" }</code></li> </ul> <p>Example (RandomForestClassifier):</p> <pre><code>{\n  \"type\": \"random_forest_classifier\",\n  \"params\": {\"n_estimators\": 50, \"random_state\": 42}\n}\n</code></pre>"},{"location":"reference/modeling_nodes.html#classification","title":"Classification","text":""},{"location":"reference/modeling_nodes.html#logistic_regression","title":"logistic_regression","text":"<p>Backed by <code>sklearn.linear_model.LogisticRegression</code>.</p> <p>Defaults:</p> <ul> <li><code>max_iter=1000</code></li> <li><code>solver=lbfgs</code></li> <li><code>random_state=42</code></li> </ul> <p>Learned params:</p> <ul> <li>fitted sklearn estimator (stored in-memory and pickled when saving the pipeline)</li> </ul>"},{"location":"reference/modeling_nodes.html#random_forest_classifier","title":"random_forest_classifier","text":"<p>Backed by <code>sklearn.ensemble.RandomForestClassifier</code>.</p> <p>Defaults include:</p> <ul> <li><code>n_estimators=50</code>, <code>max_depth=10</code></li> <li><code>min_samples_split=5</code>, <code>min_samples_leaf=2</code></li> <li><code>n_jobs=-1</code>, <code>random_state=42</code></li> </ul> <p>Learned params:</p> <ul> <li>fitted sklearn estimator</li> </ul>"},{"location":"reference/modeling_nodes.html#regression","title":"Regression","text":""},{"location":"reference/modeling_nodes.html#ridge_regression","title":"ridge_regression","text":"<p>Backed by <code>sklearn.linear_model.Ridge</code>.</p> <p>Defaults:</p> <ul> <li><code>alpha=1.0</code>, <code>solver=auto</code>, <code>random_state=42</code></li> </ul>"},{"location":"reference/modeling_nodes.html#random_forest_regressor","title":"random_forest_regressor","text":"<p>Backed by <code>sklearn.ensemble.RandomForestRegressor</code>.</p> <p>Defaults include:</p> <ul> <li><code>n_estimators=50</code>, <code>max_depth=10</code></li> <li><code>min_samples_split=5</code>, <code>min_samples_leaf=2</code></li> <li><code>n_jobs=-1</code>, <code>random_state=42</code></li> </ul>"},{"location":"reference/modeling_nodes.html#hyperparameter-tuning","title":"Hyperparameter tuning","text":""},{"location":"reference/modeling_nodes.html#hyperparameter_tuner","title":"hyperparameter_tuner","text":"<p>This mode wraps a base model and performs search.</p> <p>Config:</p> <ul> <li><code>type</code>: <code>hyperparameter_tuner</code></li> <li><code>base_model</code>: dict with a supported base model type (e.g., logistic regression)</li> <li>tuning options such as:</li> <li><code>strategy</code>: <code>grid</code> | <code>random</code> | <code>halving_grid</code> | <code>halving_random</code> | <code>optuna</code> (availability depends on installed packages)</li> <li><code>search_space</code>: dict of parameter \u2192 list/range</li> <li><code>metric</code>: e.g., <code>accuracy</code>, <code>f1</code>, <code>roc_auc</code>, <code>rmse</code>, <code>r2</code></li> <li><code>cv_enabled</code>, <code>cv_type</code>, <code>cv_folds</code>, <code>random_state</code></li> </ul> <p>Learned params:</p> <ul> <li>a tuple <code>(best_model, tuning_result)</code> where <code>best_model</code> is a fitted estimator.</li> </ul>"},{"location":"reference/modeling_nodes.html#cross-validation","title":"Cross-validation","text":"<p><code>StatefulEstimator.cross_validate()</code> can perform CV on the train split and returns aggregated fold metrics.</p> <p>Note: <code>SkyulfPipeline</code> performs modeling through the same building blocks (a calculator + applier); <code>StatefulEstimator</code> is the lightweight wrapper exposed for low-level usage.</p>"},{"location":"reference/preprocessing_nodes.html","title":"Preprocessing Nodes","text":"<p>This page documents the preprocessing node types supported by <code>FeatureEngineer</code>.</p>"},{"location":"reference/preprocessing_nodes.html#step-schema","title":"Step schema","text":"<p>Every preprocessing step in the pipeline config uses:</p> <pre><code>{\n  \"name\": \"...\",\n  \"transformer\": \"TransformerType\",\n  \"params\": { ... }\n}\n</code></pre> <p>Where <code>params</code> is passed into the node\u2019s Calculator <code>fit()</code>.</p>"},{"location":"reference/preprocessing_nodes.html#splitters","title":"Splitters","text":"<p>Example step:</p> <pre><code>{\"name\": \"split\", \"transformer\": \"TrainTestSplitter\", \"params\": {\"test_size\": 0.2, \"random_state\": 42, \"target_column\": \"target\"}}\n</code></pre>"},{"location":"reference/preprocessing_nodes.html#traintestsplitter","title":"TrainTestSplitter","text":"<p>Splits a DataFrame (or <code>(X, y)</code> tuple) into <code>SplitDataset(train, test, validation)</code>.</p> <p>Config (<code>params</code>):</p> <ul> <li><code>test_size</code>: float (default 0.2)</li> <li><code>validation_size</code>: float (default 0.0)</li> <li><code>random_state</code>: int (default 42)</li> <li><code>shuffle</code>: bool (default True)</li> <li><code>stratify</code>: bool (default False)</li> <li><code>target_column</code>: str (required only when splitting a DataFrame and using stratify)</li> </ul> <p>Learned params: none (passes through config).</p>"},{"location":"reference/preprocessing_nodes.html#feature_target_split","title":"feature_target_split","text":"<p>Splits a DataFrame into <code>(X, y)</code> (or applies the split to each <code>SplitDataset</code> split).</p> <p>Config:</p> <ul> <li><code>target_column</code>: str (required)</li> </ul> <p>Learned params: none.</p>"},{"location":"reference/preprocessing_nodes.html#cleaning","title":"Cleaning","text":"<p>Example step:</p> <pre><code>{\n  \"name\": \"clean_text\",\n  \"transformer\": \"TextCleaning\",\n  \"params\": {\n    \"columns\": [\"free_text\"],\n    \"operations\": [\n      {\"op\": \"trim\", \"mode\": \"both\"},\n      {\"op\": \"case\", \"mode\": \"lower\"},\n      {\"op\": \"regex\", \"mode\": \"collapse_whitespace\"}\n    ]\n  }\n}\n</code></pre>"},{"location":"reference/preprocessing_nodes.html#textcleaning","title":"TextCleaning","text":"<p>Applies a list of string operations.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str] (optional; auto-detects text-like columns)</li> <li><code>operations</code>: list[dict]</li> <li><code>{ \"op\": \"trim\", \"mode\": \"both\"|\"leading\"|\"trailing\" }</code></li> <li><code>{ \"op\": \"case\", \"mode\": \"lower\"|\"upper\"|\"title\"|\"sentence\" }</code></li> <li><code>{ \"op\": \"remove_special\", \"mode\": \"keep_alphanumeric\"|\"keep_alphanumeric_space\"|\"letters_only\"|\"digits_only\", \"replacement\": \"\" }</code></li> <li><code>{ \"op\": \"regex\", \"mode\": \"collapse_whitespace\"|\"extract_digits\"|\"custom\", \"pattern\": \"...\", \"repl\": \"...\" }</code></li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>operations</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#valuereplacement","title":"ValueReplacement","text":"<p>Replaces values in selected columns.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li>Either:</li> <li><code>mapping</code>: dict (global mapping) or dict[col -&gt; mapping]</li> <li><code>to_replace</code> + <code>value</code></li> <li><code>replacements</code>: list of <code>{old, new}</code> (converted into a mapping)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>mapping</code>, <code>to_replace</code>, <code>value</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#aliasreplacement","title":"AliasReplacement","text":"<p>Normalizes common textual aliases (boolean/country/custom).</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str] (optional; auto-detects text-like columns)</li> <li><code>alias_type</code>: <code>boolean</code> | <code>country</code> | <code>custom</code> (also supports legacy <code>mode</code>)</li> <li><code>custom_map</code>: dict[str, str] (also supports legacy <code>custom_pairs</code>)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>alias_type</code>, <code>custom_map</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#invalidvaluereplacement","title":"InvalidValueReplacement","text":"<p>Replaces invalid numeric values.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>rule</code>: <code>negative</code> | <code>zero</code> | <code>negative_to_nan</code> | <code>custom_range</code> (also supports legacy <code>mode</code>)</li> <li><code>replacement</code>: any (default NaN)</li> <li><code>min_value</code> / <code>max_value</code>: used by <code>custom_range</code></li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>rule</code>, <code>replacement</code>, <code>min_value</code>, <code>max_value</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#drop-missing","title":"Drop &amp; Missing","text":""},{"location":"reference/preprocessing_nodes.html#deduplicate","title":"Deduplicate","text":"<p>Config:</p> <ul> <li><code>subset</code>: list[str] | None</li> <li><code>keep</code>: <code>first</code> | <code>last</code> | <code>none</code> (mapped to <code>False</code>)</li> </ul> <p>Learned params:</p> <ul> <li><code>subset</code>, <code>keep</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#dropmissingcolumns","title":"DropMissingColumns","text":"<p>Config:</p> <ul> <li><code>missing_threshold</code>: float (percentage; if &gt; 0, drops columns with missing% &gt;= threshold)</li> <li><code>columns</code>: list[str] (explicit columns to drop)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns_to_drop</code>, <code>threshold</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#dropmissingrows","title":"DropMissingRows","text":"<p>Config:</p> <ul> <li><code>subset</code>: list[str] | None</li> <li><code>how</code>: <code>any</code> | <code>all</code> (ignored if <code>threshold</code> provided)</li> <li><code>threshold</code>: int | None (min non-null values)</li> </ul> <p>Learned params:</p> <ul> <li><code>subset</code>, <code>how</code>, <code>threshold</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#missingindicator","title":"MissingIndicator","text":"<p>Adds <code>{col}_missing</code> indicator columns.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str] (optional; defaults to all columns with any missing values)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#imputation","title":"Imputation","text":"<p>Example step:</p> <pre><code>{\"name\": \"impute\", \"transformer\": \"SimpleImputer\", \"params\": {\"strategy\": \"median\", \"columns\": [\"age\"]}}\n</code></pre>"},{"location":"reference/preprocessing_nodes.html#simpleimputer","title":"SimpleImputer","text":"<p>Config:</p> <ul> <li><code>strategy</code>: <code>mean</code> | <code>median</code> | <code>most_frequent</code> | <code>constant</code> (also accepts <code>mode</code>)</li> <li><code>columns</code>: list[str] (optional; numeric auto-detection for mean/median)</li> <li><code>fill_value</code>: any (used for <code>constant</code>)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>strategy</code></li> <li><code>fill_values</code>: dict[col -&gt; value]</li> <li><code>missing_counts</code>: dict[col -&gt; count]</li> <li><code>total_missing</code>: int</li> </ul>"},{"location":"reference/preprocessing_nodes.html#knnimputer","title":"KNNImputer","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str] (numeric)</li> <li><code>n_neighbors</code>: int (default 5)</li> <li><code>weights</code>: <code>uniform</code> | <code>distance</code></li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>imputer_object</code> (sklearn object; pickled in pipeline)</li> <li><code>n_neighbors</code>, <code>weights</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#iterativeimputer","title":"IterativeImputer","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str] (numeric)</li> <li><code>max_iter</code>: int (default 10)</li> <li><code>estimator</code>: <code>BayesianRidge</code> | <code>DecisionTree</code> | <code>ExtraTrees</code> | <code>KNeighbors</code></li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>imputer_object</code> (sklearn object; pickled in pipeline)</li> <li><code>estimator</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#encoding","title":"Encoding","text":"<p>Example step:</p> <pre><code>{\"name\": \"encode\", \"transformer\": \"OneHotEncoder\", \"params\": {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"}}\n</code></pre>"},{"location":"reference/preprocessing_nodes.html#onehotencoder","title":"OneHotEncoder","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str] (optional; auto-detects categorical columns)</li> <li><code>drop_first</code>: bool (default False)</li> <li><code>max_categories</code>: int (default 20)</li> <li><code>handle_unknown</code>: <code>ignore</code> | <code>error</code> (default ignore)</li> <li><code>drop_original</code>: bool (default True)</li> <li><code>include_missing</code>: bool (default False)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>encoder_object</code> (sklearn OneHotEncoder)</li> <li><code>feature_names</code>: list[str]</li> <li><code>drop_original</code>, <code>include_missing</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#dummyencoder","title":"DummyEncoder","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>drop_first</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>categories</code>: dict[col -&gt; list[str]]</li> <li><code>drop_first</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#ordinalencoder","title":"OrdinalEncoder","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>handle_unknown</code>: str (default <code>use_encoded_value</code>)</li> <li><code>unknown_value</code>: int/float (default -1)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>encoder_object</code> (sklearn OrdinalEncoder)</li> <li><code>categories_count</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#labelencoder","title":"LabelEncoder","text":"<p>Encodes either target or selected feature columns.</p> <p>Config:</p> <ul> <li><code>columns</code>: optional list[str]</li> <li>if omitted, encodes the provided target <code>y</code></li> <li>if provided, encodes those feature columns (and also target if included)</li> </ul> <p>Learned params:</p> <ul> <li><code>encoders</code>: dict[col or \"target\" -&gt; sklearn LabelEncoder]</li> <li><code>classes_count</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#targetencoder","title":"TargetEncoder","text":"<p>Requires a target series (<code>y</code>).</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>smooth</code>: <code>auto</code> or numeric</li> <li><code>target_type</code>: <code>auto</code> or explicit type</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code></li> <li><code>encoder_object</code> (sklearn TargetEncoder)</li> </ul>"},{"location":"reference/preprocessing_nodes.html#hashencoder","title":"HashEncoder","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>n_features</code>: int (default 10)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>n_features</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#scaling","title":"Scaling","text":"<p>All scaling nodes accept <code>columns</code> (optional; numeric auto-detect) and return learned numeric arrays.</p>"},{"location":"reference/preprocessing_nodes.html#standardscaler","title":"StandardScaler","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>with_mean</code>: bool (default True)</li> <li><code>with_std</code>: bool (default True)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>mean</code>, <code>scale</code>, <code>var</code>, <code>with_mean</code>, <code>with_std</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#minmaxscaler","title":"MinMaxScaler","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>feature_range</code>: tuple (default (0, 1))</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>min</code>, <code>scale</code>, <code>data_min</code>, <code>data_max</code>, <code>feature_range</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#robustscaler","title":"RobustScaler","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>quantile_range</code>: tuple (default (25.0, 75.0))</li> <li><code>with_centering</code>: bool (default True)</li> <li><code>with_scaling</code>: bool (default True)</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>center</code>, <code>scale</code>, <code>quantile_range</code>, <code>with_centering</code>, <code>with_scaling</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#maxabsscaler","title":"MaxAbsScaler","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>scale</code>, <code>max_abs</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#outliers","title":"Outliers","text":""},{"location":"reference/preprocessing_nodes.html#iqr","title":"IQR","text":"<p>Filters rows outside per-column IQR bounds.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>multiplier</code>: float (default 1.5)</li> </ul> <p>Learned params:</p> <ul> <li><code>bounds</code>: dict[col -&gt; {lower, upper}]</li> <li><code>warnings</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#zscore","title":"ZScore","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>threshold</code>: float (default 3.0)</li> </ul> <p>Learned params:</p> <ul> <li><code>stats</code>: dict[col -&gt; {mean, std}]</li> <li><code>threshold</code>, <code>warnings</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#winsorize","title":"Winsorize","text":"<p>Clips values into per-column percentile bounds.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>lower_percentile</code>: float (default 5.0)</li> <li><code>upper_percentile</code>: float (default 95.0)</li> </ul> <p>Learned params:</p> <ul> <li><code>bounds</code>: dict[col -&gt; {lower, upper}]</li> </ul>"},{"location":"reference/preprocessing_nodes.html#manualbounds","title":"ManualBounds","text":"<p>Filters rows outside user-provided bounds.</p> <p>Config:</p> <ul> <li><code>bounds</code>: dict[col -&gt; {lower, upper}]</li> </ul> <p>Learned params:</p> <ul> <li><code>bounds</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#ellipticenvelope","title":"EllipticEnvelope","text":"<p>Learns a per-column EllipticEnvelope model and filters outliers.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>contamination</code>: float (default 0.01)</li> </ul> <p>Learned params:</p> <ul> <li><code>models</code>: dict[col -&gt; sklearn model]</li> <li><code>contamination</code>, <code>warnings</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#transformations","title":"Transformations","text":""},{"location":"reference/preprocessing_nodes.html#powertransformer","title":"PowerTransformer","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>method</code>: <code>yeo-johnson</code> | <code>box-cox</code></li> <li><code>standardize</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>lambdas</code>, <code>method</code>, <code>standardize</code>, <code>scaler_params</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#simpletransformation","title":"SimpleTransformation","text":"<p>Config:</p> <ul> <li><code>transformations</code>: list of <code>{column, method, clip_threshold?}</code></li> <li>methods include <code>log</code>, <code>square_root</code>, <code>cube_root</code>, <code>reciprocal</code>, <code>square</code>, <code>exponential</code></li> </ul> <p>Learned params:</p> <ul> <li><code>transformations</code> (passes through)</li> </ul>"},{"location":"reference/preprocessing_nodes.html#generaltransformation","title":"GeneralTransformation","text":"<p>Config:</p> <ul> <li><code>transformations</code>: list of <code>{column, method, clip_threshold?}</code></li> <li>methods include power transforms (<code>box-cox</code>, <code>yeo-johnson</code>) and the simple methods</li> </ul> <p>Learned params:</p> <ul> <li><code>transformations</code> with fitted <code>lambdas</code>/<code>scaler_params</code> where applicable</li> </ul>"},{"location":"reference/preprocessing_nodes.html#bucketing-binning","title":"Bucketing (Binning)","text":""},{"location":"reference/preprocessing_nodes.html#generalbinning","title":"GeneralBinning","text":"<p>Creates binned features with configurable strategies.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str] (numeric)</li> <li><code>strategy</code>: <code>equal_width</code> | <code>equal_frequency</code> | <code>kmeans</code> | <code>custom</code> | <code>kbins</code></li> <li><code>n_bins</code> and strategy-specific keys:</li> <li><code>equal_width_bins</code>, <code>equal_frequency_bins</code>, <code>duplicates</code></li> <li><code>kbins_n_bins</code>, <code>kbins_strategy</code></li> <li><code>custom_bins</code>: dict[col -&gt; edges]</li> <li><code>custom_labels</code>: dict[col -&gt; labels]</li> <li>output formatting:</li> <li><code>output_suffix</code>, <code>drop_original</code>, <code>label_format</code>, <code>missing_strategy</code>, <code>missing_label</code>, <code>include_lowest</code>, <code>precision</code></li> </ul> <p>Learned params:</p> <ul> <li><code>bin_edges</code> (dict[col -&gt; edges])</li> <li>output formatting settings</li> </ul>"},{"location":"reference/preprocessing_nodes.html#custombinning","title":"CustomBinning","text":"<p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>bins</code>: list[float] (shared edges)</li> <li>plus output formatting keys (same as GeneralBinning)</li> </ul> <p>Learned params:</p> <ul> <li><code>bin_edges</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#kbinsdiscretizer","title":"KBinsDiscretizer","text":"<p>Wrapper around <code>GeneralBinning</code> with a KBins-style interface.</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>n_bins</code>: int</li> <li><code>strategy</code>: <code>uniform</code> | <code>quantile</code> | <code>kmeans</code></li> </ul> <p>Learned params:</p> <ul> <li><code>bin_edges</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#casting","title":"Casting","text":""},{"location":"reference/preprocessing_nodes.html#casting_1","title":"Casting","text":"<p>Config:</p> <ul> <li>Either:</li> <li><code>column_types</code>: dict[col -&gt; dtype]</li> <li>or <code>columns</code> + <code>target_type</code></li> <li><code>coerce_on_error</code>: bool (default True)</li> </ul> <p>Learned params:</p> <ul> <li><code>type_map</code>, <code>coerce_on_error</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#feature-generation","title":"Feature Generation","text":""},{"location":"reference/preprocessing_nodes.html#polynomialfeatures","title":"PolynomialFeatures","text":"<p>(Alias: <code>PolynomialFeaturesNode</code>)</p> <p>Config:</p> <ul> <li><code>columns</code>: list[str]</li> <li><code>auto_detect</code>: bool</li> <li><code>degree</code>: int</li> <li><code>interaction_only</code>: bool</li> <li><code>include_bias</code>: bool</li> <li><code>include_input_features</code>: bool</li> <li><code>output_prefix</code>: str</li> </ul> <p>Learned params:</p> <ul> <li><code>columns</code>, <code>degree</code>, <code>interaction_only</code>, <code>include_bias</code>, <code>include_input_features</code>, <code>output_prefix</code>, <code>feature_names</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#featuregeneration","title":"FeatureGeneration","text":"<p>(Aliases: <code>FeatureMath</code>, <code>FeatureGenerationNode</code>)</p> <p>Config:</p> <ul> <li><code>operations</code>: list[dict]</li> <li><code>epsilon</code>: float (default 1e-9)</li> <li><code>allow_overwrite</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>operations</code>, <code>epsilon</code>, <code>allow_overwrite</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#feature-selection","title":"Feature Selection","text":""},{"location":"reference/preprocessing_nodes.html#variancethreshold","title":"VarianceThreshold","text":"<p>Config:</p> <ul> <li><code>threshold</code>: float (default 0.0)</li> <li><code>columns</code>: list[str] (optional; numeric)</li> <li><code>drop_columns</code>: bool (default True)</li> </ul> <p>Learned params:</p> <ul> <li><code>candidate_columns</code>, <code>selected_columns</code>, <code>variances</code>, <code>threshold</code>, <code>drop_columns</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#correlationthreshold","title":"CorrelationThreshold","text":"<p>Config:</p> <ul> <li><code>threshold</code>: float (default 0.95)</li> <li><code>correlation_method</code>: <code>pearson</code> | <code>spearman</code> | <code>kendall</code> (default pearson)</li> <li><code>columns</code>: list[str] (numeric)</li> <li><code>drop_columns</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>columns_to_drop</code>, <code>threshold</code>, <code>method</code>, <code>drop_columns</code></li> </ul>"},{"location":"reference/preprocessing_nodes.html#univariateselection","title":"UnivariateSelection","text":"<p>Config:</p> <ul> <li><code>target_column</code>: str (if <code>y</code> not passed as tuple)</li> <li><code>problem_type</code>: <code>auto</code> | <code>classification</code> | <code>regression</code></li> <li><code>method</code>: <code>select_k_best</code> | <code>select_percentile</code> | <code>select_fpr</code> | <code>select_fdr</code> | <code>select_fwe</code> | <code>generic_univariate_select</code></li> <li>selector parameters (depending on method): <code>k</code>, <code>percentile</code>, <code>alpha</code>, <code>mode</code>, <code>param</code></li> <li>scoring: <code>score_func</code> (e.g., <code>f_classif</code>, <code>mutual_info_classif</code>, \u2026)</li> <li><code>drop_columns</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>selected_columns</code>, <code>candidate_columns</code>, <code>scores</code>, <code>pvalues</code> (when available), plus selector config</li> </ul>"},{"location":"reference/preprocessing_nodes.html#modelbasedselection","title":"ModelBasedSelection","text":"<p>Config:</p> <ul> <li><code>target_column</code>: str</li> <li><code>problem_type</code>: <code>auto</code> | <code>classification</code> | <code>regression</code></li> <li><code>method</code>: <code>select_from_model</code> | <code>rfe</code></li> <li><code>estimator</code>: <code>auto</code> | <code>logistic_regression</code> | <code>random_forest</code> | <code>linear_regression</code></li> <li>For select_from_model: <code>threshold</code>, <code>max_features</code></li> <li>For RFE: <code>n_features_to_select</code>, <code>step</code></li> <li><code>drop_columns</code>: bool</li> </ul> <p>Learned params:</p> <ul> <li><code>selected_columns</code>, <code>candidate_columns</code>, and method-specific metadata</li> </ul>"},{"location":"reference/preprocessing_nodes.html#feature_selection","title":"feature_selection","text":"<p>A higher-level facade node that dispatches to the selection implementations.</p>"},{"location":"reference/preprocessing_nodes.html#resampling","title":"Resampling","text":""},{"location":"reference/preprocessing_nodes.html#oversampling","title":"Oversampling","text":"<p>Config:</p> <ul> <li><code>method</code>: <code>smote</code> | <code>adasyn</code> | <code>borderline_smote</code> | <code>svm_smote</code> | <code>kmeans_smote</code> | <code>smote_tomek</code></li> <li><code>target_column</code>: required if <code>y</code> is not provided as tuple</li> <li><code>sampling_strategy</code>: <code>auto</code> or dict</li> <li><code>random_state</code>: int</li> <li>method-specific keys: <code>k_neighbors</code>, <code>m_neighbors</code>, <code>kind</code>, <code>out_step</code>, <code>cluster_balance_threshold</code>, <code>density_exponent</code>, <code>n_jobs</code></li> </ul> <p>Learned params: none (passes through config).</p>"},{"location":"reference/preprocessing_nodes.html#undersampling","title":"Undersampling","text":"<p>Config:</p> <ul> <li><code>method</code>: <code>random_under_sampling</code> | <code>nearmiss</code> | <code>tomek_links</code> | <code>edited_nearest_neighbours</code></li> <li><code>target_column</code>: required if <code>y</code> not provided as tuple</li> <li><code>sampling_strategy</code>, <code>random_state</code>, <code>replacement</code>, <code>version</code>, <code>n_neighbors</code>, <code>kind_sel</code>, <code>n_jobs</code></li> </ul> <p>Learned params: none.</p>"},{"location":"reference/preprocessing_nodes.html#inspection","title":"Inspection","text":""},{"location":"reference/preprocessing_nodes.html#datasetprofile","title":"DatasetProfile","text":"<p>Captures basic dataset stats without modifying data.</p> <p>Config: none.</p> <p>Learned params:</p> <ul> <li><code>profile</code>: rows/columns/dtypes/missing/numeric_stats</li> </ul>"},{"location":"reference/preprocessing_nodes.html#datasnapshot","title":"DataSnapshot","text":"<p>Captures the first N rows without modifying data.</p> <p>Config:</p> <ul> <li><code>n_rows</code>: int (default 5)</li> </ul> <p>Learned params:</p> <ul> <li><code>snapshot</code>: list[dict]</li> </ul>"},{"location":"reference/api/pipeline.html","title":"API: Pipeline","text":""},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline","title":"<code>skyulf.pipeline.SkyulfPipeline</code>","text":"<p>End-to-end ML Pipeline.</p> <p>Encapsulates: 1. Feature Engineering (Preprocessing) 2. Modeling (Training/Inference)</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>class SkyulfPipeline:\n    \"\"\"\n    End-to-end ML Pipeline.\n\n    Encapsulates:\n    1. Feature Engineering (Preprocessing)\n    2. Modeling (Training/Inference)\n    \"\"\"\n\n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        Initialize the pipeline.\n\n        Args:\n            config: Pipeline configuration dictionary.\n                    Must contain 'preprocessing' (list) and 'modeling' (dict).\n        \"\"\"\n        self.config = config\n        self.preprocessing_steps = config.get(\"preprocessing\", [])\n        self.modeling_config = config.get(\"modeling\", {})\n\n        self.feature_engineer = FeatureEngineer(self.preprocessing_steps)\n        self.model_estimator: Optional[StatefulEstimator] = None\n\n        # Initialize model estimator if config is present\n        if self.modeling_config:\n            self._init_model_estimator()\n\n    def _init_model_estimator(self):\n        \"\"\"Initialize the StatefulEstimator based on config.\"\"\"\n        model_type = self.modeling_config.get(\"type\")\n        node_id = self.modeling_config.get(\"node_id\", \"model_node\")\n\n        calculator: Optional[BaseModelCalculator] = None\n        applier: Optional[BaseModelApplier] = None\n\n        # Try Registry first\n        try:\n            calculator = NodeRegistry.get_calculator(model_type)()\n            applier = NodeRegistry.get_applier(model_type)()\n        except ValueError:\n            pass\n\n        if calculator is None:\n            # Map model types to classes\n            if model_type == \"logistic_regression\":\n                calculator = LogisticRegressionCalculator()\n                applier = LogisticRegressionApplier()\n            elif model_type == \"random_forest_classifier\":\n                calculator = RandomForestClassifierCalculator()\n                applier = RandomForestClassifierApplier()\n            elif model_type == \"ridge_regression\":\n                calculator = RidgeRegressionCalculator()\n                applier = RidgeRegressionApplier()\n            elif model_type == \"random_forest_regressor\":\n                calculator = RandomForestRegressorCalculator()\n                applier = RandomForestRegressorApplier()\n            elif model_type == \"hyperparameter_tuner\":\n                # Tuner wraps another model\n                base_model_config = self.modeling_config.get(\"base_model\", {})\n                base_model_type = base_model_config.get(\"type\")\n\n                base_calc: Optional[BaseModelCalculator] = None\n                base_applier: Optional[BaseModelApplier] = None\n\n                # Try Registry for base model\n                try:\n                    base_calc = NodeRegistry.get_calculator(base_model_type)()\n                    base_applier = NodeRegistry.get_applier(base_model_type)()\n                except ValueError:\n                    pass\n\n                if base_calc is None:\n                    if base_model_type == \"logistic_regression\":\n                        base_calc = LogisticRegressionCalculator()\n                        base_applier = LogisticRegressionApplier()\n                    elif base_model_type == \"random_forest_classifier\":\n                        base_calc = RandomForestClassifierCalculator()\n                        base_applier = RandomForestClassifierApplier()\n                    elif base_model_type == \"ridge_regression\":\n                        base_calc = RidgeRegressionCalculator()\n                        base_applier = RidgeRegressionApplier()\n                    elif base_model_type == \"random_forest_regressor\":\n                        base_calc = RandomForestRegressorCalculator()\n                        base_applier = RandomForestRegressorApplier()\n\n                if base_calc and base_applier:\n                    calculator = TunerCalculator(base_calc)\n                    applier = TunerApplier(base_applier)\n                else:\n                    raise ValueError(\n                        f\"Unknown base model type for tuner: {base_model_type}\"\n                    )\n\n        if calculator is None:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n        self.model_estimator = StatefulEstimator(\n            node_id=node_id, calculator=calculator, applier=applier\n        )\n\n    def fit(\n        self, data: Union[pd.DataFrame, SkyulfDataFrame, SplitDataset], target_column: str\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Fit the pipeline.\n\n        Args:\n            data: Input data (DataFrame or SplitDataset).\n            target_column: Name of the target column.\n\n        Returns:\n            Dictionary containing execution metrics.\n        \"\"\"\n        metrics = {}\n\n        # 1. Feature Engineering\n        logger.info(\"Starting Feature Engineering...\")\n        transformed_data, fe_metrics = self.feature_engineer.fit_transform(data)\n        metrics[\"preprocessing\"] = fe_metrics\n\n        # 2. Modeling\n        if self.model_estimator:\n            logger.info(\"Starting Model Training...\")\n\n            # Ensure transformed_data is SplitDataset for modeling\n            if isinstance(transformed_data, SplitDataset):\n                dataset = transformed_data\n            else:\n                # If we only have a DataFrame, we can't really evaluate properly without a split\n                # But we can fit on it.\n                # Ideally, the user should provide a SplitDataset or use a Splitter node in preprocessing.\n                # If preprocessing didn't split, we wrap it.\n                engine = get_engine(transformed_data)\n                empty_df = engine.create_dataframe({})\n                dataset = SplitDataset(\n                    train=transformed_data, test=empty_df, validation=None\n                )\n\n            # Fit the model\n            # Note: fit_predict updates self.model_estimator.model in-memory\n            _ = self.model_estimator.fit_predict(\n                dataset=dataset,\n                target_column=target_column,\n                config=self.modeling_config,\n            )\n\n            # Evaluate\n            # We can run evaluation if we have test/validation sets\n            try:\n                eval_report = self.model_estimator.evaluate(\n                    dataset=dataset, target_column=target_column\n                )\n                metrics[\"modeling\"] = eval_report\n            except Exception as e:\n                logger.warning(f\"Evaluation failed: {e}\")\n                metrics[\"modeling_error\"] = str(e)\n\n        return metrics\n\n    def predict(self, data: Union[pd.DataFrame, SkyulfDataFrame]) -&gt; Any:\n        \"\"\"\n        Generate predictions.\n\n        Args:\n            data: Input DataFrame.\n\n        Returns:\n            Series of predictions.\n        \"\"\"\n        # 1. Feature Engineering (Transform only)\n        transformed_data = self.feature_engineer.transform(data)\n\n        # 2. Modeling\n        if self.model_estimator and self.model_estimator.model is not None:\n            return self.model_estimator.applier.predict(\n                transformed_data, self.model_estimator.model\n            )\n        else:\n            raise ValueError(\"Pipeline not fitted or no model configured.\")\n\n    def save(self, path: str):\n        \"\"\"Save the pipeline to a file.\"\"\"\n        # We can use pickle to save the whole object since we removed external dependencies\n        with open(path, \"wb\") as f:\n            pickle.dump(self, f)\n\n    @classmethod\n    def load(cls, path: str) -&gt; \"SkyulfPipeline\":\n        \"\"\"Load the pipeline from a file.\"\"\"\n        with open(path, \"rb\") as f:\n            return pickle.load(f)  # type: ignore\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>Pipeline configuration dictionary.     Must contain 'preprocessing' (list) and 'modeling' (dict).</p> required Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>def __init__(self, config: Dict[str, Any]):\n    \"\"\"\n    Initialize the pipeline.\n\n    Args:\n        config: Pipeline configuration dictionary.\n                Must contain 'preprocessing' (list) and 'modeling' (dict).\n    \"\"\"\n    self.config = config\n    self.preprocessing_steps = config.get(\"preprocessing\", [])\n    self.modeling_config = config.get(\"modeling\", {})\n\n    self.feature_engineer = FeatureEngineer(self.preprocessing_steps)\n    self.model_estimator: Optional[StatefulEstimator] = None\n\n    # Initialize model estimator if config is present\n    if self.modeling_config:\n        self._init_model_estimator()\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.fit","title":"<code>fit(data, target_column)</code>","text":"<p>Fit the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, SkyulfDataFrame, SplitDataset]</code> <p>Input data (DataFrame or SplitDataset).</p> required <code>target_column</code> <code>str</code> <p>Name of the target column.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing execution metrics.</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>def fit(\n    self, data: Union[pd.DataFrame, SkyulfDataFrame, SplitDataset], target_column: str\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Fit the pipeline.\n\n    Args:\n        data: Input data (DataFrame or SplitDataset).\n        target_column: Name of the target column.\n\n    Returns:\n        Dictionary containing execution metrics.\n    \"\"\"\n    metrics = {}\n\n    # 1. Feature Engineering\n    logger.info(\"Starting Feature Engineering...\")\n    transformed_data, fe_metrics = self.feature_engineer.fit_transform(data)\n    metrics[\"preprocessing\"] = fe_metrics\n\n    # 2. Modeling\n    if self.model_estimator:\n        logger.info(\"Starting Model Training...\")\n\n        # Ensure transformed_data is SplitDataset for modeling\n        if isinstance(transformed_data, SplitDataset):\n            dataset = transformed_data\n        else:\n            # If we only have a DataFrame, we can't really evaluate properly without a split\n            # But we can fit on it.\n            # Ideally, the user should provide a SplitDataset or use a Splitter node in preprocessing.\n            # If preprocessing didn't split, we wrap it.\n            engine = get_engine(transformed_data)\n            empty_df = engine.create_dataframe({})\n            dataset = SplitDataset(\n                train=transformed_data, test=empty_df, validation=None\n            )\n\n        # Fit the model\n        # Note: fit_predict updates self.model_estimator.model in-memory\n        _ = self.model_estimator.fit_predict(\n            dataset=dataset,\n            target_column=target_column,\n            config=self.modeling_config,\n        )\n\n        # Evaluate\n        # We can run evaluation if we have test/validation sets\n        try:\n            eval_report = self.model_estimator.evaluate(\n                dataset=dataset, target_column=target_column\n            )\n            metrics[\"modeling\"] = eval_report\n        except Exception as e:\n            logger.warning(f\"Evaluation failed: {e}\")\n            metrics[\"modeling_error\"] = str(e)\n\n    return metrics\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load the pipeline from a file.</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; \"SkyulfPipeline\":\n    \"\"\"Load the pipeline from a file.\"\"\"\n    with open(path, \"rb\") as f:\n        return pickle.load(f)  # type: ignore\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.predict","title":"<code>predict(data)</code>","text":"<p>Generate predictions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, SkyulfDataFrame]</code> <p>Input DataFrame.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Series of predictions.</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>def predict(self, data: Union[pd.DataFrame, SkyulfDataFrame]) -&gt; Any:\n    \"\"\"\n    Generate predictions.\n\n    Args:\n        data: Input DataFrame.\n\n    Returns:\n        Series of predictions.\n    \"\"\"\n    # 1. Feature Engineering (Transform only)\n    transformed_data = self.feature_engineer.transform(data)\n\n    # 2. Modeling\n    if self.model_estimator and self.model_estimator.model is not None:\n        return self.model_estimator.applier.predict(\n            transformed_data, self.model_estimator.model\n        )\n    else:\n        raise ValueError(\"Pipeline not fitted or no model configured.\")\n</code></pre>"},{"location":"reference/api/pipeline.html#skyulf.pipeline.SkyulfPipeline.save","title":"<code>save(path)</code>","text":"<p>Save the pipeline to a file.</p> Source code in <code>skyulf-core/skyulf/pipeline.py</code> <pre><code>def save(self, path: str):\n    \"\"\"Save the pipeline to a file.\"\"\"\n    # We can use pickle to save the whole object since we removed external dependencies\n    with open(path, \"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"reference/api/data/dataset.html","title":"API: SplitDataset","text":""},{"location":"reference/api/data/dataset.html#skyulf.data.dataset.SplitDataset","title":"<code>skyulf.data.dataset.SplitDataset</code>  <code>dataclass</code>","text":"Source code in <code>skyulf-core/skyulf/data/dataset.py</code> <pre><code>@dataclass\nclass SplitDataset:\n    train: Union[SkyulfDataFrame, Tuple[SkyulfDataFrame, Any]]\n    test: Union[SkyulfDataFrame, Tuple[SkyulfDataFrame, Any]]\n    validation: Optional[Union[SkyulfDataFrame, Tuple[SkyulfDataFrame, Any]]] = None\n\n    def copy(self) -&gt; \"SplitDataset\":\n        def copy_data(data):\n            if isinstance(data, tuple):\n                # Handle target copy safely (Series/Array/List)\n                y = data[1]\n                y_copy = y.copy() if hasattr(y, \"copy\") else (y.clone() if hasattr(y, \"clone\") else y)\n\n                X = data[0]\n                X_copy = X.copy() if hasattr(X, \"copy\") else (X.clone() if hasattr(X, \"clone\") else X)\n\n                return (X_copy, y_copy)\n\n            if hasattr(data, \"copy\"):\n                return data.copy()\n            if hasattr(data, \"clone\"):\n                return data.clone()\n            return data\n\n        return SplitDataset(\n            train=copy_data(self.train),\n            test=copy_data(self.test),\n            validation=(\n                copy_data(self.validation) if self.validation is not None else None\n            ),\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html","title":"API: modeling","text":""},{"location":"reference/api/modeling/index.html#skyulf.modeling","title":"<code>skyulf.modeling</code>","text":"<p>Modeling module for Skyulf.</p>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelApplier","title":"<code>BaseModelApplier</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class BaseModelApplier(ABC):\n    @abstractmethod\n    def predict(self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any) -&gt; Union[pd.Series, Any]:\n        \"\"\"\n        Generates predictions.\n        \"\"\"\n        pass\n\n    def predict_proba(\n        self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any\n    ) -&gt; Optional[Union[pd.DataFrame, SkyulfDataFrame]]:\n        \"\"\"\n        Generates prediction probabilities if supported.\n        Returns DataFrame where columns are classes.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelApplier.predict","title":"<code>predict(df, model_artifact)</code>  <code>abstractmethod</code>","text":"<p>Generates predictions.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>@abstractmethod\ndef predict(self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any) -&gt; Union[pd.Series, Any]:\n    \"\"\"\n    Generates predictions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelApplier.predict_proba","title":"<code>predict_proba(df, model_artifact)</code>","text":"<p>Generates prediction probabilities if supported. Returns DataFrame where columns are classes.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def predict_proba(\n    self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any\n) -&gt; Optional[Union[pd.DataFrame, SkyulfDataFrame]]:\n    \"\"\"\n    Generates prediction probabilities if supported.\n    Returns DataFrame where columns are classes.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelCalculator","title":"<code>BaseModelCalculator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class BaseModelCalculator(ABC):\n    @property\n    @abstractmethod\n    def problem_type(self) -&gt; str:\n        \"\"\"Returns 'classification' or 'regression'.\"\"\"\n        pass\n\n    @property\n    def default_params(self) -&gt; Dict[str, Any]:\n        \"\"\"Default hyperparameters for the model.\"\"\"\n        return {}\n\n    @abstractmethod\n    def fit(\n        self,\n        X: Union[pd.DataFrame, SkyulfDataFrame],\n        y: Union[pd.Series, Any],\n        config: Dict[str, Any],\n        progress_callback: Optional[Callable[..., None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        validation_data: Optional[tuple[Union[pd.DataFrame, SkyulfDataFrame], Union[pd.Series, Any]]] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Trains the model. Returns the model object (serializable).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelCalculator.default_params","title":"<code>default_params</code>  <code>property</code>","text":"<p>Default hyperparameters for the model.</p>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelCalculator.problem_type","title":"<code>problem_type</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns 'classification' or 'regression'.</p>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.BaseModelCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>  <code>abstractmethod</code>","text":"<p>Trains the model. Returns the model object (serializable).</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>@abstractmethod\ndef fit(\n    self,\n    X: Union[pd.DataFrame, SkyulfDataFrame],\n    y: Union[pd.Series, Any],\n    config: Dict[str, Any],\n    progress_callback: Optional[Callable[..., None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    validation_data: Optional[tuple[Union[pd.DataFrame, SkyulfDataFrame], Union[pd.Series, Any]]] = None,\n) -&gt; Any:\n    \"\"\"\n    Trains the model. Returns the model object (serializable).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.HyperparameterField","title":"<code>HyperparameterField</code>  <code>dataclass</code>","text":"<p>Describe a single tunable hyperparameter.</p> Source code in <code>skyulf-core/skyulf/modeling/hyperparameters.py</code> <pre><code>@dataclass\nclass HyperparameterField:\n    \"\"\"Describe a single tunable hyperparameter.\"\"\"\n\n    name: str\n    label: str\n    type: str  # \"number\", \"select\", \"boolean\"\n    default: Any\n    description: str = \"\"\n    min: Optional[float] = None\n    max: Optional[float] = None\n    step: Optional[float] = None\n    options: Optional[List[Dict[str, Any]]] = (\n        None  # For 'select' type: [{\"label\": \"L1\", \"value\": \"l1\"}]\n    )\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return asdict(self)\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.LogisticRegressionApplier","title":"<code>LogisticRegressionApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Logistic Regression Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class LogisticRegressionApplier(SklearnApplier):\n    \"\"\"Logistic Regression Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.LogisticRegressionCalculator","title":"<code>LogisticRegressionCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Logistic Regression Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>@NodeRegistry.register(\"logistic_regression\", LogisticRegressionApplier)\nclass LogisticRegressionCalculator(SklearnCalculator):\n    \"\"\"Logistic Regression Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=LogisticRegression,\n            default_params={\n                \"max_iter\": 1000,\n                \"solver\": \"lbfgs\",\n                \"random_state\": 42,\n            },\n            problem_type=\"classification\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RandomForestClassifierApplier","title":"<code>RandomForestClassifierApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Random Forest Classifier Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class RandomForestClassifierApplier(SklearnApplier):\n    \"\"\"Random Forest Classifier Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RandomForestClassifierCalculator","title":"<code>RandomForestClassifierCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Random Forest Classifier Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>@NodeRegistry.register(\"random_forest_classifier\", RandomForestClassifierApplier)\nclass RandomForestClassifierCalculator(SklearnCalculator):\n    \"\"\"Random Forest Classifier Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=RandomForestClassifier,\n            default_params={\n                \"n_estimators\": 50,\n                \"max_depth\": 10,\n                \"min_samples_split\": 5,\n                \"min_samples_leaf\": 2,\n                \"n_jobs\": -1,\n                \"random_state\": 42,\n            },\n            problem_type=\"classification\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RandomForestRegressorApplier","title":"<code>RandomForestRegressorApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Random Forest Regressor Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RandomForestRegressorApplier(SklearnApplier):\n    \"\"\"Random Forest Regressor Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RandomForestRegressorCalculator","title":"<code>RandomForestRegressorCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Random Forest Regressor Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>@NodeRegistry.register(\"random_forest_regressor\", RandomForestRegressorApplier)\nclass RandomForestRegressorCalculator(SklearnCalculator):\n    \"\"\"Random Forest Regressor Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=RandomForestRegressor,\n            default_params={\n                \"n_estimators\": 50,\n                \"max_depth\": 10,\n                \"min_samples_split\": 5,\n                \"min_samples_leaf\": 2,\n                \"n_jobs\": -1,\n                \"random_state\": 42,\n            },\n            problem_type=\"regression\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RidgeRegressionApplier","title":"<code>RidgeRegressionApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Ridge Regression Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RidgeRegressionApplier(SklearnApplier):\n    \"\"\"Ridge Regression Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.RidgeRegressionCalculator","title":"<code>RidgeRegressionCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Ridge Regression Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>@NodeRegistry.register(\"ridge_regression\", RidgeRegressionApplier)\nclass RidgeRegressionCalculator(SklearnCalculator):\n    \"\"\"Ridge Regression Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=Ridge,\n            default_params={\n                \"alpha\": 1.0,\n                \"solver\": \"auto\",\n                \"random_state\": 42,\n            },\n            problem_type=\"regression\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.SklearnApplier","title":"<code>SklearnApplier</code>","text":"<p>               Bases: <code>BaseModelApplier</code></p> <p>Base applier for Scikit-Learn models.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>class SklearnApplier(BaseModelApplier):\n    \"\"\"Base applier for Scikit-Learn models.\"\"\"\n\n    def predict(self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any) -&gt; pd.Series:\n        # Convert to Numpy\n        X_np, _ = SklearnBridge.to_sklearn(df)\n\n        preds = model_artifact.predict(X_np)\n\n        # Return as Pandas Series for consistency\n        # If input was Pandas, try to preserve index\n        index = None\n        if hasattr(df, \"index\"):\n            index = df.index\n        elif hasattr(df, \"to_pandas\"):\n             # If it's a wrapper or Polars, we might lose index unless we convert\n             # For now, default index is acceptable for predictions\n             pass\n\n        return pd.Series(preds, index=index)\n\n    def predict_proba(\n        self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any\n    ) -&gt; Optional[pd.DataFrame]:\n        if not hasattr(model_artifact, \"predict_proba\"):\n            return None\n\n        X_np, _ = SklearnBridge.to_sklearn(df)\n        probs = model_artifact.predict_proba(X_np)\n\n        # Return as DataFrame\n        index = None\n        if hasattr(df, \"index\"):\n            index = df.index\n\n        # Column names usually 0, 1, etc. or classes_\n        columns = None\n        if hasattr(model_artifact, \"classes_\"):\n            columns = model_artifact.classes_\n\n        return pd.DataFrame(probs, index=index, columns=columns)\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.SklearnCalculator","title":"<code>SklearnCalculator</code>","text":"<p>               Bases: <code>BaseModelCalculator</code></p> <p>Base calculator for Scikit-Learn models.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>class SklearnCalculator(BaseModelCalculator):\n    \"\"\"Base calculator for Scikit-Learn models.\"\"\"\n\n    def __init__(\n        self,\n        model_class: Type[BaseEstimator],\n        default_params: Dict[str, Any],\n        problem_type: str,\n    ):\n        self.model_class = model_class\n        self._default_params = default_params\n        self._problem_type = problem_type\n\n    @property\n    def default_params(self) -&gt; Dict[str, Any]:\n        return self._default_params\n\n    @property\n    def problem_type(self) -&gt; str:\n        return self._problem_type\n\n    def fit(\n        self,\n        X: Union[pd.DataFrame, SkyulfDataFrame],\n        y: Union[pd.Series, Any],\n        config: Dict[str, Any],\n        progress_callback=None,\n        log_callback=None,\n        validation_data=None,\n    ) -&gt; Any:\n        \"\"\"Fit the Scikit-Learn model.\"\"\"\n        # 1. Merge Config with Defaults\n        params = self.default_params.copy()\n        if config:\n            # We support two configuration structures:\n            # 1. Nested: {'params': {'C': 1.0, ...}} - Preferred\n            # 2. Flat: {'C': 1.0, 'type': '...', ...} - Legacy/Simple support\n\n            # Check for explicit 'params' dictionary first\n            overrides = config.get(\"params\", {})\n\n            # If 'params' key exists but is None or empty, check if there are other keys at top level\n            # that might be params. But be careful not to mix them.\n            # If config has 'params', we assume it's the source of truth.\n\n            if not overrides and \"params\" not in config:\n                # Fallback to flat config if 'params' key is completely missing\n                reserved_keys = {\n                    \"type\",\n                    \"target_column\",\n                    \"node_id\",\n                    \"step_type\",\n                    \"inputs\",\n                }\n                overrides = {\n                    k: v\n                    for k, v in config.items()\n                    if k not in reserved_keys and not isinstance(v, dict)\n                }\n\n            if overrides:\n                params.update(overrides)\n\n        msg = f\"Initializing {self.model_class.__name__} with params: {params}\"\n        logger.info(msg)\n        if log_callback:\n            log_callback(msg)\n\n        # 2. Instantiate Model\n        model = self.model_class(**params)\n\n        # 3. Fit\n        # Convert to Numpy using Bridge (handles Polars/Pandas/Wrappers)\n        X_np, y_np = SklearnBridge.to_sklearn((X, y))\n\n        model.fit(X_np, y_np)\n\n        return model\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.SklearnCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>","text":"<p>Fit the Scikit-Learn model.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>def fit(\n    self,\n    X: Union[pd.DataFrame, SkyulfDataFrame],\n    y: Union[pd.Series, Any],\n    config: Dict[str, Any],\n    progress_callback=None,\n    log_callback=None,\n    validation_data=None,\n) -&gt; Any:\n    \"\"\"Fit the Scikit-Learn model.\"\"\"\n    # 1. Merge Config with Defaults\n    params = self.default_params.copy()\n    if config:\n        # We support two configuration structures:\n        # 1. Nested: {'params': {'C': 1.0, ...}} - Preferred\n        # 2. Flat: {'C': 1.0, 'type': '...', ...} - Legacy/Simple support\n\n        # Check for explicit 'params' dictionary first\n        overrides = config.get(\"params\", {})\n\n        # If 'params' key exists but is None or empty, check if there are other keys at top level\n        # that might be params. But be careful not to mix them.\n        # If config has 'params', we assume it's the source of truth.\n\n        if not overrides and \"params\" not in config:\n            # Fallback to flat config if 'params' key is completely missing\n            reserved_keys = {\n                \"type\",\n                \"target_column\",\n                \"node_id\",\n                \"step_type\",\n                \"inputs\",\n            }\n            overrides = {\n                k: v\n                for k, v in config.items()\n                if k not in reserved_keys and not isinstance(v, dict)\n            }\n\n        if overrides:\n            params.update(overrides)\n\n    msg = f\"Initializing {self.model_class.__name__} with params: {params}\"\n    logger.info(msg)\n    if log_callback:\n        log_callback(msg)\n\n    # 2. Instantiate Model\n    model = self.model_class(**params)\n\n    # 3. Fit\n    # Convert to Numpy using Bridge (handles Polars/Pandas/Wrappers)\n    X_np, y_np = SklearnBridge.to_sklearn((X, y))\n\n    model.fit(X_np, y_np)\n\n    return model\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator","title":"<code>StatefulEstimator</code>","text":"Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class StatefulEstimator:\n    def __init__(\n        self, calculator: BaseModelCalculator, applier: BaseModelApplier, node_id: str\n    ):\n        self.calculator = calculator\n        self.applier = applier\n        self.node_id = node_id\n        self.model = None  # In-memory model storage\n\n    def _extract_xy(\n        self, data: Any, target_column: str\n    ) -&gt; tuple[Any, Any]:\n        \"\"\"Helper to extract X and y from DataFrame or Tuple.\"\"\"\n        if isinstance(data, tuple) and len(data) == 2:\n            return data[0], data[1]\n\n        engine = get_engine(data)\n\n        if engine.name == \"polars\":\n            if target_column not in data.columns:\n                raise ValueError(f\"Target column '{target_column}' not found in data\")\n            X = data.drop([target_column])\n            y = data.select(target_column).to_series()\n            return X, y\n\n        # Pandas / Default\n        # Check for DataFrame-like\n        if hasattr(data, \"columns\"):\n            if target_column not in data.columns:\n                raise ValueError(f\"Target column '{target_column}' not found in data\")\n\n            # Fallback for pure Pandas\n            if isinstance(data, pd.DataFrame):\n                return data.drop(columns=[target_column]), data[target_column]\n\n        raise ValueError(f\"Unexpected data type: {type(data)}\")\n\n    def cross_validate(\n        self,\n        dataset: SplitDataset,\n        target_column: str,\n        config: Dict[str, Any],\n        n_folds: int = 5,\n        cv_type: str = \"k_fold\",\n        shuffle: bool = True,\n        random_state: int = 42,\n        progress_callback: Optional[Callable[[int, int], None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Performs cross-validation on the training split.\n        \"\"\"\n        # Import here to avoid circular dependency if any\n        from .cross_validation import perform_cross_validation\n\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n        return perform_cross_validation(\n            calculator=self.calculator,\n            applier=self.applier,\n            X=X_train,\n            y=y_train,\n            config=config,\n            n_folds=n_folds,\n            cv_type=cv_type,\n            shuffle=shuffle,\n            random_state=random_state,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n        )\n\n    def fit_predict(\n        self,\n        dataset: Union[SplitDataset, pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        target_column: str,\n        config: Dict[str, Any],\n        progress_callback: Optional[Callable[[int, int], None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        job_id: str = \"unknown\",\n    ) -&gt; Dict[str, pd.Series]:\n        \"\"\"\n        Fits the model on training data and returns predictions for all splits.\n        \"\"\"\n        # Handle raw DataFrame or Tuple input by wrapping it in a dummy SplitDataset\n        if isinstance(dataset, pd.DataFrame):\n            dataset = SplitDataset(train=dataset, test=pd.DataFrame(), validation=None)\n        elif isinstance(dataset, tuple):\n            # Check if it's (train_df, test_df) or (X, y)\n            elem0 = dataset[0]\n            if isinstance(elem0, pd.DataFrame) and target_column in elem0.columns:\n                # It's (train_df, test_df)\n                train_df, test_df = dataset\n                dataset = SplitDataset(train=train_df, test=test_df, validation=None)  # type: ignore\n            else:\n                # Fallback: Treat input as training data (e.g. X, y tuple) and initialize empty test set.\n                msg = (\n                    \"WARNING: No test set provided. Using entire input as training data. \"\n                    \"Ensure data was split BEFORE preprocessing to avoid data leakage.\"\n                )\n                logger.warning(msg)\n                if log_callback:\n                    log_callback(msg)\n\n                dataset = SplitDataset(\n                    train=dataset, test=pd.DataFrame(), validation=None\n                )\n\n        # 1. Prepare Data\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n        validation_data = None\n        if dataset.validation is not None:\n            X_val, y_val = self._extract_xy(dataset.validation, target_column)\n            validation_data = (X_val, y_val)\n\n        # 2. Train Model\n        self.model = self.calculator.fit(\n            X_train,\n            y_train,\n            config,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n            validation_data=validation_data,\n        )\n\n        # 3. Predict on all splits\n        predictions = {}\n\n        # Train Predictions\n        predictions[\"train\"] = self.applier.predict(X_train, self.model)\n\n        # Test Predictions\n        is_test_empty = False\n        test_df = None\n        if isinstance(dataset.test, tuple):\n            test_df = dataset.test[0]\n        else:\n            test_df = dataset.test\n\n        if hasattr(test_df, \"empty\"):\n            is_test_empty = test_df.empty\n        else:\n            # Polars\n            is_test_empty = test_df.is_empty()\n\n        if not is_test_empty:\n            if isinstance(dataset.test, tuple):\n                X_test, _ = dataset.test\n            else:\n                if target_column in dataset.test.columns:\n                    try:\n                        X_test = dataset.test.drop(columns=[target_column])\n                    except TypeError:\n                        # Polars\n                        X_test = dataset.test.drop([target_column])\n                else:\n                    X_test = dataset.test\n            predictions[\"test\"] = self.applier.predict(X_test, self.model)\n\n        # Validation Predictions\n        if dataset.validation is not None:\n            if isinstance(dataset.validation, tuple):\n                X_val, _ = dataset.validation\n            else:\n                if target_column in dataset.validation.columns:\n                    X_val = dataset.validation.drop(columns=[target_column])\n                else:\n                    X_val = dataset.validation\n            predictions[\"validation\"] = self.applier.predict(X_val, self.model)\n\n        return predictions\n\n    def refit(\n        self,\n        dataset: SplitDataset,\n        target_column: str,\n        config: Dict[str, Any],\n        job_id: str = \"unknown\",\n    ) -&gt; None:\n        \"\"\"\n        Refits the model on Train + Validation data and updates the artifact.\n        \"\"\"\n        if dataset.validation is None:\n            # Fallback to normal fit if no validation set\n            self.fit_predict(dataset, target_column, config, job_id=job_id)\n            return\n\n        # 1. Prepare Combined Data\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n        X_val, y_val = self._extract_xy(dataset.validation, target_column)\n\n        X_combined = pd.concat([X_train, X_val], axis=0)\n        y_combined = pd.concat([y_train, y_val], axis=0)\n\n        # 2. Train Model\n        self.model = self.calculator.fit(X_combined, y_combined, config)\n\n    def evaluate(  # noqa: C901\n        self, dataset: SplitDataset, target_column: str, job_id: str = \"unknown\"\n    ) -&gt; Any:\n        \"\"\"\n        Evaluates the model on all splits and returns a detailed report.\n        \"\"\"\n        # Import here to avoid circular dependency\n        from .evaluation.classification import evaluate_classification_model\n        from .evaluation.regression import evaluate_regression_model\n\n        if self.model is None:\n            raise ValueError(\n                \"Model has not been trained yet. Call fit_predict() first.\"\n            )\n\n        problem_type = self.calculator.problem_type\n\n        splits_payload = {}\n\n        # Container for raw predictions\n        evaluation_data = {\n            \"job_id\": job_id,\n            \"node_id\": self.node_id,\n            \"problem_type\": problem_type,\n            \"splits\": {},\n        }\n\n        # Helper to evaluate a single split\n        def evaluate_split(split_name: str, data: Any):\n            if isinstance(data, tuple):\n                X, y = data\n            elif isinstance(data, pd.DataFrame):\n                if target_column not in data.columns:\n                    return None  # Cannot evaluate without target\n                X = data.drop(columns=[target_column])\n                y = data[target_column]\n            else:\n                return None\n\n            y_pred = self.applier.predict(X, self.model)\n\n            # Try to get probabilities for classification\n            y_proba = None\n            if problem_type == \"classification\":\n                y_proba_df = self.applier.predict_proba(X, self.model)\n                if y_proba_df is not None:\n                    y_proba = {\n                        \"classes\": y_proba_df.columns.tolist(),\n                        \"values\": y_proba_df.values.tolist(),\n                    }\n\n            split_data = {\n                \"y_true\": y.tolist() if hasattr(y, \"tolist\") else list(y),\n                \"y_pred\": (\n                    y_pred.tolist() if hasattr(y_pred, \"tolist\") else list(y_pred)\n                ),\n            }\n\n            if y_proba:\n                split_data[\"y_proba\"] = y_proba\n\n            evaluation_data[\"splits\"][split_name] = split_data\n\n            # Unpack model if it's a tuple (from Tuner)\n            model_to_evaluate = self.model\n            if isinstance(self.model, tuple) and len(self.model) == 2:\n                # Check if first element looks like a model (has fit/predict)\n                # or if it's just a convention from TunerCalculator\n                model_to_evaluate = self.model[0]\n\n            if problem_type == \"classification\":\n                return evaluate_classification_model(\n                    model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n                )\n            elif problem_type == \"regression\":\n                return evaluate_regression_model(\n                    model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n                )\n            else:\n                raise ValueError(f\"Unknown problem type: {problem_type}\")\n\n        # 2. Evaluate Train\n        splits_payload[\"train\"] = evaluate_split(\"train\", dataset.train)\n\n        # 3. Evaluate Test\n        has_test = False\n        if isinstance(dataset.test, pd.DataFrame):\n            has_test = not dataset.test.empty\n        elif isinstance(dataset.test, tuple):\n            has_test = len(dataset.test) == 2 and len(dataset.test[0]) &gt; 0\n\n        if has_test:\n            splits_payload[\"test\"] = evaluate_split(\"test\", dataset.test)\n\n        # 4. Evaluate Validation\n        if dataset.validation is not None:\n            has_val = False\n            if isinstance(dataset.validation, pd.DataFrame):\n                has_val = not dataset.validation.empty\n            elif isinstance(dataset.validation, tuple):\n                has_val = (\n                    len(dataset.validation) == 2 and len(dataset.validation[0]) &gt; 0\n                )\n\n            if has_val:\n                splits_payload[\"validation\"] = evaluate_split(\n                    \"validation\", dataset.validation\n                )\n\n        # Return report object (simplified for now, assuming schema matches)\n        return {\n            \"problem_type\": problem_type,\n            \"splits\": splits_payload,\n            \"raw_data\": evaluation_data,\n        }\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator.cross_validate","title":"<code>cross_validate(dataset, target_column, config, n_folds=5, cv_type='k_fold', shuffle=True, random_state=42, progress_callback=None, log_callback=None)</code>","text":"<p>Performs cross-validation on the training split.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def cross_validate(\n    self,\n    dataset: SplitDataset,\n    target_column: str,\n    config: Dict[str, Any],\n    n_folds: int = 5,\n    cv_type: str = \"k_fold\",\n    shuffle: bool = True,\n    random_state: int = 42,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Performs cross-validation on the training split.\n    \"\"\"\n    # Import here to avoid circular dependency if any\n    from .cross_validation import perform_cross_validation\n\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n    return perform_cross_validation(\n        calculator=self.calculator,\n        applier=self.applier,\n        X=X_train,\n        y=y_train,\n        config=config,\n        n_folds=n_folds,\n        cv_type=cv_type,\n        shuffle=shuffle,\n        random_state=random_state,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n    )\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator.evaluate","title":"<code>evaluate(dataset, target_column, job_id='unknown')</code>","text":"<p>Evaluates the model on all splits and returns a detailed report.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def evaluate(  # noqa: C901\n    self, dataset: SplitDataset, target_column: str, job_id: str = \"unknown\"\n) -&gt; Any:\n    \"\"\"\n    Evaluates the model on all splits and returns a detailed report.\n    \"\"\"\n    # Import here to avoid circular dependency\n    from .evaluation.classification import evaluate_classification_model\n    from .evaluation.regression import evaluate_regression_model\n\n    if self.model is None:\n        raise ValueError(\n            \"Model has not been trained yet. Call fit_predict() first.\"\n        )\n\n    problem_type = self.calculator.problem_type\n\n    splits_payload = {}\n\n    # Container for raw predictions\n    evaluation_data = {\n        \"job_id\": job_id,\n        \"node_id\": self.node_id,\n        \"problem_type\": problem_type,\n        \"splits\": {},\n    }\n\n    # Helper to evaluate a single split\n    def evaluate_split(split_name: str, data: Any):\n        if isinstance(data, tuple):\n            X, y = data\n        elif isinstance(data, pd.DataFrame):\n            if target_column not in data.columns:\n                return None  # Cannot evaluate without target\n            X = data.drop(columns=[target_column])\n            y = data[target_column]\n        else:\n            return None\n\n        y_pred = self.applier.predict(X, self.model)\n\n        # Try to get probabilities for classification\n        y_proba = None\n        if problem_type == \"classification\":\n            y_proba_df = self.applier.predict_proba(X, self.model)\n            if y_proba_df is not None:\n                y_proba = {\n                    \"classes\": y_proba_df.columns.tolist(),\n                    \"values\": y_proba_df.values.tolist(),\n                }\n\n        split_data = {\n            \"y_true\": y.tolist() if hasattr(y, \"tolist\") else list(y),\n            \"y_pred\": (\n                y_pred.tolist() if hasattr(y_pred, \"tolist\") else list(y_pred)\n            ),\n        }\n\n        if y_proba:\n            split_data[\"y_proba\"] = y_proba\n\n        evaluation_data[\"splits\"][split_name] = split_data\n\n        # Unpack model if it's a tuple (from Tuner)\n        model_to_evaluate = self.model\n        if isinstance(self.model, tuple) and len(self.model) == 2:\n            # Check if first element looks like a model (has fit/predict)\n            # or if it's just a convention from TunerCalculator\n            model_to_evaluate = self.model[0]\n\n        if problem_type == \"classification\":\n            return evaluate_classification_model(\n                model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n            )\n        elif problem_type == \"regression\":\n            return evaluate_regression_model(\n                model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n            )\n        else:\n            raise ValueError(f\"Unknown problem type: {problem_type}\")\n\n    # 2. Evaluate Train\n    splits_payload[\"train\"] = evaluate_split(\"train\", dataset.train)\n\n    # 3. Evaluate Test\n    has_test = False\n    if isinstance(dataset.test, pd.DataFrame):\n        has_test = not dataset.test.empty\n    elif isinstance(dataset.test, tuple):\n        has_test = len(dataset.test) == 2 and len(dataset.test[0]) &gt; 0\n\n    if has_test:\n        splits_payload[\"test\"] = evaluate_split(\"test\", dataset.test)\n\n    # 4. Evaluate Validation\n    if dataset.validation is not None:\n        has_val = False\n        if isinstance(dataset.validation, pd.DataFrame):\n            has_val = not dataset.validation.empty\n        elif isinstance(dataset.validation, tuple):\n            has_val = (\n                len(dataset.validation) == 2 and len(dataset.validation[0]) &gt; 0\n            )\n\n        if has_val:\n            splits_payload[\"validation\"] = evaluate_split(\n                \"validation\", dataset.validation\n            )\n\n    # Return report object (simplified for now, assuming schema matches)\n    return {\n        \"problem_type\": problem_type,\n        \"splits\": splits_payload,\n        \"raw_data\": evaluation_data,\n    }\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator.fit_predict","title":"<code>fit_predict(dataset, target_column, config, progress_callback=None, log_callback=None, job_id='unknown')</code>","text":"<p>Fits the model on training data and returns predictions for all splits.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def fit_predict(\n    self,\n    dataset: Union[SplitDataset, pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n    target_column: str,\n    config: Dict[str, Any],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    job_id: str = \"unknown\",\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"\n    Fits the model on training data and returns predictions for all splits.\n    \"\"\"\n    # Handle raw DataFrame or Tuple input by wrapping it in a dummy SplitDataset\n    if isinstance(dataset, pd.DataFrame):\n        dataset = SplitDataset(train=dataset, test=pd.DataFrame(), validation=None)\n    elif isinstance(dataset, tuple):\n        # Check if it's (train_df, test_df) or (X, y)\n        elem0 = dataset[0]\n        if isinstance(elem0, pd.DataFrame) and target_column in elem0.columns:\n            # It's (train_df, test_df)\n            train_df, test_df = dataset\n            dataset = SplitDataset(train=train_df, test=test_df, validation=None)  # type: ignore\n        else:\n            # Fallback: Treat input as training data (e.g. X, y tuple) and initialize empty test set.\n            msg = (\n                \"WARNING: No test set provided. Using entire input as training data. \"\n                \"Ensure data was split BEFORE preprocessing to avoid data leakage.\"\n            )\n            logger.warning(msg)\n            if log_callback:\n                log_callback(msg)\n\n            dataset = SplitDataset(\n                train=dataset, test=pd.DataFrame(), validation=None\n            )\n\n    # 1. Prepare Data\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n    validation_data = None\n    if dataset.validation is not None:\n        X_val, y_val = self._extract_xy(dataset.validation, target_column)\n        validation_data = (X_val, y_val)\n\n    # 2. Train Model\n    self.model = self.calculator.fit(\n        X_train,\n        y_train,\n        config,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n        validation_data=validation_data,\n    )\n\n    # 3. Predict on all splits\n    predictions = {}\n\n    # Train Predictions\n    predictions[\"train\"] = self.applier.predict(X_train, self.model)\n\n    # Test Predictions\n    is_test_empty = False\n    test_df = None\n    if isinstance(dataset.test, tuple):\n        test_df = dataset.test[0]\n    else:\n        test_df = dataset.test\n\n    if hasattr(test_df, \"empty\"):\n        is_test_empty = test_df.empty\n    else:\n        # Polars\n        is_test_empty = test_df.is_empty()\n\n    if not is_test_empty:\n        if isinstance(dataset.test, tuple):\n            X_test, _ = dataset.test\n        else:\n            if target_column in dataset.test.columns:\n                try:\n                    X_test = dataset.test.drop(columns=[target_column])\n                except TypeError:\n                    # Polars\n                    X_test = dataset.test.drop([target_column])\n            else:\n                X_test = dataset.test\n        predictions[\"test\"] = self.applier.predict(X_test, self.model)\n\n    # Validation Predictions\n    if dataset.validation is not None:\n        if isinstance(dataset.validation, tuple):\n            X_val, _ = dataset.validation\n        else:\n            if target_column in dataset.validation.columns:\n                X_val = dataset.validation.drop(columns=[target_column])\n            else:\n                X_val = dataset.validation\n        predictions[\"validation\"] = self.applier.predict(X_val, self.model)\n\n    return predictions\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.StatefulEstimator.refit","title":"<code>refit(dataset, target_column, config, job_id='unknown')</code>","text":"<p>Refits the model on Train + Validation data and updates the artifact.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def refit(\n    self,\n    dataset: SplitDataset,\n    target_column: str,\n    config: Dict[str, Any],\n    job_id: str = \"unknown\",\n) -&gt; None:\n    \"\"\"\n    Refits the model on Train + Validation data and updates the artifact.\n    \"\"\"\n    if dataset.validation is None:\n        # Fallback to normal fit if no validation set\n        self.fit_predict(dataset, target_column, config, job_id=job_id)\n        return\n\n    # 1. Prepare Combined Data\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n    X_val, y_val = self._extract_xy(dataset.validation, target_column)\n\n    X_combined = pd.concat([X_train, X_val], axis=0)\n    y_combined = pd.concat([y_train, y_val], axis=0)\n\n    # 2. Train Model\n    self.model = self.calculator.fit(X_combined, y_combined, config)\n</code></pre>"},{"location":"reference/api/modeling/index.html#skyulf.modeling.perform_cross_validation","title":"<code>perform_cross_validation(calculator, applier, X, y, config, n_folds=5, cv_type='k_fold', shuffle=True, random_state=42, progress_callback=None, log_callback=None)</code>","text":"<p>Performs K-Fold cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>calculator</code> <code>BaseModelCalculator</code> <p>The model calculator (fit logic).</p> required <code>applier</code> <code>BaseModelApplier</code> <p>The model applier (predict logic).</p> required <code>X</code> <code>Union[DataFrame, SkyulfDataFrame]</code> <p>Features.</p> required <code>y</code> <code>Union[Series, Any]</code> <p>Target.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>Model configuration.</p> required <code>n_folds</code> <code>int</code> <p>Number of folds.</p> <code>5</code> <code>cv_type</code> <code>str</code> <p>Type of CV.</p> <code>'k_fold'</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle data before splitting (for KFold/Stratified).</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Random seed for shuffling.</p> <code>42</code> <code>progress_callback</code> <code>Optional[Callable[[int, int], None]]</code> <p>Optional callback(current_fold, total_folds).</p> <code>None</code> <code>log_callback</code> <code>Optional[Callable[[str], None]]</code> <p>Optional callback for logging messages.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing aggregated metrics and per-fold details.</p> Source code in <code>skyulf-core/skyulf/modeling/cross_validation.py</code> <pre><code>def perform_cross_validation(\n    calculator: BaseModelCalculator,\n    applier: BaseModelApplier,\n    X: Union[pd.DataFrame, SkyulfDataFrame],\n    y: Union[pd.Series, Any],\n    config: Dict[str, Any],\n    n_folds: int = 5,\n    cv_type: str = \"k_fold\",  # k_fold, stratified_k_fold, time_series_split, shuffle_split\n    shuffle: bool = True,\n    random_state: int = 42,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Performs K-Fold cross-validation.\n\n    Args:\n        calculator: The model calculator (fit logic).\n        applier: The model applier (predict logic).\n        X: Features.\n        y: Target.\n        config: Model configuration.\n        n_folds: Number of folds.\n        cv_type: Type of CV.\n        shuffle: Whether to shuffle data before splitting (for KFold/Stratified).\n        random_state: Random seed for shuffling.\n        progress_callback: Optional callback(current_fold, total_folds).\n        log_callback: Optional callback for logging messages.\n\n    Returns:\n        Dict containing aggregated metrics and per-fold details.\n    \"\"\"\n\n    problem_type = calculator.problem_type\n\n    if log_callback:\n        log_callback(f\"Starting Cross-Validation (Folds: {n_folds}, Type: {cv_type})\")\n\n    # 1. Setup Splitter\n    if cv_type == \"time_series_split\":\n        splitter = TimeSeriesSplit(n_splits=n_folds)\n    elif cv_type == \"shuffle_split\":\n        splitter = ShuffleSplit(\n            n_splits=n_folds, test_size=0.2, random_state=random_state\n        )\n    elif cv_type == \"stratified_k_fold\" and problem_type == \"classification\":\n        splitter = StratifiedKFold(\n            n_splits=n_folds,\n            shuffle=shuffle,\n            random_state=random_state if shuffle else None,\n        )\n    else:\n        # Default to KFold\n        splitter = KFold(\n            n_splits=n_folds,\n            shuffle=shuffle,\n            random_state=random_state if shuffle else None,\n        )\n\n    fold_results = []\n\n    # Ensure numpy for splitting using the Bridge\n    X_arr, y_arr = SklearnBridge.to_sklearn((X, y))\n\n    # 2. Iterate Folds\n    for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X_arr, y_arr)):\n        if progress_callback:\n            progress_callback(fold_idx + 1, n_folds)\n\n        if log_callback:\n            log_callback(f\"Processing Fold {fold_idx + 1}/{n_folds}...\")\n\n        # Split Data\n        # We slice the original X/y to preserve their type (Pandas/Polars) for the calculator\n        # Polars supports slicing with numpy arrays via __getitem__\n        # Pandas supports slicing via iloc\n\n        if hasattr(X, \"iloc\"):\n            X_train_fold = X.iloc[train_idx]\n            X_val_fold = X.iloc[val_idx]\n        else:\n            # Polars or other\n            X_train_fold = X[train_idx]\n            X_val_fold = X[val_idx]\n\n        if hasattr(y, \"iloc\"):\n            y_train_fold = y.iloc[train_idx]\n            y_val_fold = y.iloc[val_idx]\n        else:\n            # Polars Series or numpy array\n            y_train_fold = y[train_idx]\n            y_val_fold = y[val_idx]\n\n        # Fit\n        model_artifact = calculator.fit(X_train_fold, y_train_fold, config)\n\n        # Evaluate\n        if problem_type == \"classification\":\n            metrics = calculate_classification_metrics(\n                model_artifact, X_val_fold, y_val_fold\n            )\n        else:\n            metrics = calculate_regression_metrics(\n                model_artifact, X_val_fold, y_val_fold\n            )\n\n        if log_callback:\n            # Log a key metric for the fold\n            key_metric = \"accuracy\" if problem_type == \"classification\" else \"r2\"\n            score = metrics.get(key_metric, 0.0)\n            log_callback(f\"Fold {fold_idx + 1} completed. {key_metric}: {score:.4f}\")\n\n        fold_results.append(\n            {\n                \"fold\": fold_idx + 1,\n                \"metrics\": sanitize_metrics(metrics),\n                # We could store predictions here if needed, but might be too heavy\n            }\n        )\n\n    # 3. Aggregate\n    fold_metrics = [cast(Dict[str, float], r[\"metrics\"]) for r in fold_results]\n    aggregated = _aggregate_metrics(fold_metrics)\n\n    if log_callback:\n        log_callback(f\"Cross-Validation Completed. Aggregated Metrics: {aggregated}\")\n\n    return {\n        \"aggregated_metrics\": aggregated,\n        \"folds\": fold_results,\n        \"cv_config\": {\n            \"n_folds\": n_folds,\n            \"cv_type\": cv_type,\n            \"shuffle\": shuffle,\n            \"random_state\": random_state,\n        },\n    }\n</code></pre>"},{"location":"reference/api/modeling/base.html","title":"API: modeling.base","text":""},{"location":"reference/api/modeling/base.html#skyulf.modeling.base","title":"<code>skyulf.modeling.base</code>","text":""},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelApplier","title":"<code>BaseModelApplier</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class BaseModelApplier(ABC):\n    @abstractmethod\n    def predict(self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any) -&gt; Union[pd.Series, Any]:\n        \"\"\"\n        Generates predictions.\n        \"\"\"\n        pass\n\n    def predict_proba(\n        self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any\n    ) -&gt; Optional[Union[pd.DataFrame, SkyulfDataFrame]]:\n        \"\"\"\n        Generates prediction probabilities if supported.\n        Returns DataFrame where columns are classes.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelApplier.predict","title":"<code>predict(df, model_artifact)</code>  <code>abstractmethod</code>","text":"<p>Generates predictions.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>@abstractmethod\ndef predict(self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any) -&gt; Union[pd.Series, Any]:\n    \"\"\"\n    Generates predictions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelApplier.predict_proba","title":"<code>predict_proba(df, model_artifact)</code>","text":"<p>Generates prediction probabilities if supported. Returns DataFrame where columns are classes.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def predict_proba(\n    self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any\n) -&gt; Optional[Union[pd.DataFrame, SkyulfDataFrame]]:\n    \"\"\"\n    Generates prediction probabilities if supported.\n    Returns DataFrame where columns are classes.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelCalculator","title":"<code>BaseModelCalculator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class BaseModelCalculator(ABC):\n    @property\n    @abstractmethod\n    def problem_type(self) -&gt; str:\n        \"\"\"Returns 'classification' or 'regression'.\"\"\"\n        pass\n\n    @property\n    def default_params(self) -&gt; Dict[str, Any]:\n        \"\"\"Default hyperparameters for the model.\"\"\"\n        return {}\n\n    @abstractmethod\n    def fit(\n        self,\n        X: Union[pd.DataFrame, SkyulfDataFrame],\n        y: Union[pd.Series, Any],\n        config: Dict[str, Any],\n        progress_callback: Optional[Callable[..., None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        validation_data: Optional[tuple[Union[pd.DataFrame, SkyulfDataFrame], Union[pd.Series, Any]]] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Trains the model. Returns the model object (serializable).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelCalculator.default_params","title":"<code>default_params</code>  <code>property</code>","text":"<p>Default hyperparameters for the model.</p>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelCalculator.problem_type","title":"<code>problem_type</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns 'classification' or 'regression'.</p>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.BaseModelCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>  <code>abstractmethod</code>","text":"<p>Trains the model. Returns the model object (serializable).</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>@abstractmethod\ndef fit(\n    self,\n    X: Union[pd.DataFrame, SkyulfDataFrame],\n    y: Union[pd.Series, Any],\n    config: Dict[str, Any],\n    progress_callback: Optional[Callable[..., None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    validation_data: Optional[tuple[Union[pd.DataFrame, SkyulfDataFrame], Union[pd.Series, Any]]] = None,\n) -&gt; Any:\n    \"\"\"\n    Trains the model. Returns the model object (serializable).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator","title":"<code>StatefulEstimator</code>","text":"Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>class StatefulEstimator:\n    def __init__(\n        self, calculator: BaseModelCalculator, applier: BaseModelApplier, node_id: str\n    ):\n        self.calculator = calculator\n        self.applier = applier\n        self.node_id = node_id\n        self.model = None  # In-memory model storage\n\n    def _extract_xy(\n        self, data: Any, target_column: str\n    ) -&gt; tuple[Any, Any]:\n        \"\"\"Helper to extract X and y from DataFrame or Tuple.\"\"\"\n        if isinstance(data, tuple) and len(data) == 2:\n            return data[0], data[1]\n\n        engine = get_engine(data)\n\n        if engine.name == \"polars\":\n            if target_column not in data.columns:\n                raise ValueError(f\"Target column '{target_column}' not found in data\")\n            X = data.drop([target_column])\n            y = data.select(target_column).to_series()\n            return X, y\n\n        # Pandas / Default\n        # Check for DataFrame-like\n        if hasattr(data, \"columns\"):\n            if target_column not in data.columns:\n                raise ValueError(f\"Target column '{target_column}' not found in data\")\n\n            # Fallback for pure Pandas\n            if isinstance(data, pd.DataFrame):\n                return data.drop(columns=[target_column]), data[target_column]\n\n        raise ValueError(f\"Unexpected data type: {type(data)}\")\n\n    def cross_validate(\n        self,\n        dataset: SplitDataset,\n        target_column: str,\n        config: Dict[str, Any],\n        n_folds: int = 5,\n        cv_type: str = \"k_fold\",\n        shuffle: bool = True,\n        random_state: int = 42,\n        progress_callback: Optional[Callable[[int, int], None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Performs cross-validation on the training split.\n        \"\"\"\n        # Import here to avoid circular dependency if any\n        from .cross_validation import perform_cross_validation\n\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n        return perform_cross_validation(\n            calculator=self.calculator,\n            applier=self.applier,\n            X=X_train,\n            y=y_train,\n            config=config,\n            n_folds=n_folds,\n            cv_type=cv_type,\n            shuffle=shuffle,\n            random_state=random_state,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n        )\n\n    def fit_predict(\n        self,\n        dataset: Union[SplitDataset, pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        target_column: str,\n        config: Dict[str, Any],\n        progress_callback: Optional[Callable[[int, int], None]] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        job_id: str = \"unknown\",\n    ) -&gt; Dict[str, pd.Series]:\n        \"\"\"\n        Fits the model on training data and returns predictions for all splits.\n        \"\"\"\n        # Handle raw DataFrame or Tuple input by wrapping it in a dummy SplitDataset\n        if isinstance(dataset, pd.DataFrame):\n            dataset = SplitDataset(train=dataset, test=pd.DataFrame(), validation=None)\n        elif isinstance(dataset, tuple):\n            # Check if it's (train_df, test_df) or (X, y)\n            elem0 = dataset[0]\n            if isinstance(elem0, pd.DataFrame) and target_column in elem0.columns:\n                # It's (train_df, test_df)\n                train_df, test_df = dataset\n                dataset = SplitDataset(train=train_df, test=test_df, validation=None)  # type: ignore\n            else:\n                # Fallback: Treat input as training data (e.g. X, y tuple) and initialize empty test set.\n                msg = (\n                    \"WARNING: No test set provided. Using entire input as training data. \"\n                    \"Ensure data was split BEFORE preprocessing to avoid data leakage.\"\n                )\n                logger.warning(msg)\n                if log_callback:\n                    log_callback(msg)\n\n                dataset = SplitDataset(\n                    train=dataset, test=pd.DataFrame(), validation=None\n                )\n\n        # 1. Prepare Data\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n        validation_data = None\n        if dataset.validation is not None:\n            X_val, y_val = self._extract_xy(dataset.validation, target_column)\n            validation_data = (X_val, y_val)\n\n        # 2. Train Model\n        self.model = self.calculator.fit(\n            X_train,\n            y_train,\n            config,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n            validation_data=validation_data,\n        )\n\n        # 3. Predict on all splits\n        predictions = {}\n\n        # Train Predictions\n        predictions[\"train\"] = self.applier.predict(X_train, self.model)\n\n        # Test Predictions\n        is_test_empty = False\n        test_df = None\n        if isinstance(dataset.test, tuple):\n            test_df = dataset.test[0]\n        else:\n            test_df = dataset.test\n\n        if hasattr(test_df, \"empty\"):\n            is_test_empty = test_df.empty\n        else:\n            # Polars\n            is_test_empty = test_df.is_empty()\n\n        if not is_test_empty:\n            if isinstance(dataset.test, tuple):\n                X_test, _ = dataset.test\n            else:\n                if target_column in dataset.test.columns:\n                    try:\n                        X_test = dataset.test.drop(columns=[target_column])\n                    except TypeError:\n                        # Polars\n                        X_test = dataset.test.drop([target_column])\n                else:\n                    X_test = dataset.test\n            predictions[\"test\"] = self.applier.predict(X_test, self.model)\n\n        # Validation Predictions\n        if dataset.validation is not None:\n            if isinstance(dataset.validation, tuple):\n                X_val, _ = dataset.validation\n            else:\n                if target_column in dataset.validation.columns:\n                    X_val = dataset.validation.drop(columns=[target_column])\n                else:\n                    X_val = dataset.validation\n            predictions[\"validation\"] = self.applier.predict(X_val, self.model)\n\n        return predictions\n\n    def refit(\n        self,\n        dataset: SplitDataset,\n        target_column: str,\n        config: Dict[str, Any],\n        job_id: str = \"unknown\",\n    ) -&gt; None:\n        \"\"\"\n        Refits the model on Train + Validation data and updates the artifact.\n        \"\"\"\n        if dataset.validation is None:\n            # Fallback to normal fit if no validation set\n            self.fit_predict(dataset, target_column, config, job_id=job_id)\n            return\n\n        # 1. Prepare Combined Data\n        X_train, y_train = self._extract_xy(dataset.train, target_column)\n        X_val, y_val = self._extract_xy(dataset.validation, target_column)\n\n        X_combined = pd.concat([X_train, X_val], axis=0)\n        y_combined = pd.concat([y_train, y_val], axis=0)\n\n        # 2. Train Model\n        self.model = self.calculator.fit(X_combined, y_combined, config)\n\n    def evaluate(  # noqa: C901\n        self, dataset: SplitDataset, target_column: str, job_id: str = \"unknown\"\n    ) -&gt; Any:\n        \"\"\"\n        Evaluates the model on all splits and returns a detailed report.\n        \"\"\"\n        # Import here to avoid circular dependency\n        from .evaluation.classification import evaluate_classification_model\n        from .evaluation.regression import evaluate_regression_model\n\n        if self.model is None:\n            raise ValueError(\n                \"Model has not been trained yet. Call fit_predict() first.\"\n            )\n\n        problem_type = self.calculator.problem_type\n\n        splits_payload = {}\n\n        # Container for raw predictions\n        evaluation_data = {\n            \"job_id\": job_id,\n            \"node_id\": self.node_id,\n            \"problem_type\": problem_type,\n            \"splits\": {},\n        }\n\n        # Helper to evaluate a single split\n        def evaluate_split(split_name: str, data: Any):\n            if isinstance(data, tuple):\n                X, y = data\n            elif isinstance(data, pd.DataFrame):\n                if target_column not in data.columns:\n                    return None  # Cannot evaluate without target\n                X = data.drop(columns=[target_column])\n                y = data[target_column]\n            else:\n                return None\n\n            y_pred = self.applier.predict(X, self.model)\n\n            # Try to get probabilities for classification\n            y_proba = None\n            if problem_type == \"classification\":\n                y_proba_df = self.applier.predict_proba(X, self.model)\n                if y_proba_df is not None:\n                    y_proba = {\n                        \"classes\": y_proba_df.columns.tolist(),\n                        \"values\": y_proba_df.values.tolist(),\n                    }\n\n            split_data = {\n                \"y_true\": y.tolist() if hasattr(y, \"tolist\") else list(y),\n                \"y_pred\": (\n                    y_pred.tolist() if hasattr(y_pred, \"tolist\") else list(y_pred)\n                ),\n            }\n\n            if y_proba:\n                split_data[\"y_proba\"] = y_proba\n\n            evaluation_data[\"splits\"][split_name] = split_data\n\n            # Unpack model if it's a tuple (from Tuner)\n            model_to_evaluate = self.model\n            if isinstance(self.model, tuple) and len(self.model) == 2:\n                # Check if first element looks like a model (has fit/predict)\n                # or if it's just a convention from TunerCalculator\n                model_to_evaluate = self.model[0]\n\n            if problem_type == \"classification\":\n                return evaluate_classification_model(\n                    model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n                )\n            elif problem_type == \"regression\":\n                return evaluate_regression_model(\n                    model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n                )\n            else:\n                raise ValueError(f\"Unknown problem type: {problem_type}\")\n\n        # 2. Evaluate Train\n        splits_payload[\"train\"] = evaluate_split(\"train\", dataset.train)\n\n        # 3. Evaluate Test\n        has_test = False\n        if isinstance(dataset.test, pd.DataFrame):\n            has_test = not dataset.test.empty\n        elif isinstance(dataset.test, tuple):\n            has_test = len(dataset.test) == 2 and len(dataset.test[0]) &gt; 0\n\n        if has_test:\n            splits_payload[\"test\"] = evaluate_split(\"test\", dataset.test)\n\n        # 4. Evaluate Validation\n        if dataset.validation is not None:\n            has_val = False\n            if isinstance(dataset.validation, pd.DataFrame):\n                has_val = not dataset.validation.empty\n            elif isinstance(dataset.validation, tuple):\n                has_val = (\n                    len(dataset.validation) == 2 and len(dataset.validation[0]) &gt; 0\n                )\n\n            if has_val:\n                splits_payload[\"validation\"] = evaluate_split(\n                    \"validation\", dataset.validation\n                )\n\n        # Return report object (simplified for now, assuming schema matches)\n        return {\n            \"problem_type\": problem_type,\n            \"splits\": splits_payload,\n            \"raw_data\": evaluation_data,\n        }\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator.cross_validate","title":"<code>cross_validate(dataset, target_column, config, n_folds=5, cv_type='k_fold', shuffle=True, random_state=42, progress_callback=None, log_callback=None)</code>","text":"<p>Performs cross-validation on the training split.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def cross_validate(\n    self,\n    dataset: SplitDataset,\n    target_column: str,\n    config: Dict[str, Any],\n    n_folds: int = 5,\n    cv_type: str = \"k_fold\",\n    shuffle: bool = True,\n    random_state: int = 42,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Performs cross-validation on the training split.\n    \"\"\"\n    # Import here to avoid circular dependency if any\n    from .cross_validation import perform_cross_validation\n\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n    return perform_cross_validation(\n        calculator=self.calculator,\n        applier=self.applier,\n        X=X_train,\n        y=y_train,\n        config=config,\n        n_folds=n_folds,\n        cv_type=cv_type,\n        shuffle=shuffle,\n        random_state=random_state,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n    )\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator.evaluate","title":"<code>evaluate(dataset, target_column, job_id='unknown')</code>","text":"<p>Evaluates the model on all splits and returns a detailed report.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def evaluate(  # noqa: C901\n    self, dataset: SplitDataset, target_column: str, job_id: str = \"unknown\"\n) -&gt; Any:\n    \"\"\"\n    Evaluates the model on all splits and returns a detailed report.\n    \"\"\"\n    # Import here to avoid circular dependency\n    from .evaluation.classification import evaluate_classification_model\n    from .evaluation.regression import evaluate_regression_model\n\n    if self.model is None:\n        raise ValueError(\n            \"Model has not been trained yet. Call fit_predict() first.\"\n        )\n\n    problem_type = self.calculator.problem_type\n\n    splits_payload = {}\n\n    # Container for raw predictions\n    evaluation_data = {\n        \"job_id\": job_id,\n        \"node_id\": self.node_id,\n        \"problem_type\": problem_type,\n        \"splits\": {},\n    }\n\n    # Helper to evaluate a single split\n    def evaluate_split(split_name: str, data: Any):\n        if isinstance(data, tuple):\n            X, y = data\n        elif isinstance(data, pd.DataFrame):\n            if target_column not in data.columns:\n                return None  # Cannot evaluate without target\n            X = data.drop(columns=[target_column])\n            y = data[target_column]\n        else:\n            return None\n\n        y_pred = self.applier.predict(X, self.model)\n\n        # Try to get probabilities for classification\n        y_proba = None\n        if problem_type == \"classification\":\n            y_proba_df = self.applier.predict_proba(X, self.model)\n            if y_proba_df is not None:\n                y_proba = {\n                    \"classes\": y_proba_df.columns.tolist(),\n                    \"values\": y_proba_df.values.tolist(),\n                }\n\n        split_data = {\n            \"y_true\": y.tolist() if hasattr(y, \"tolist\") else list(y),\n            \"y_pred\": (\n                y_pred.tolist() if hasattr(y_pred, \"tolist\") else list(y_pred)\n            ),\n        }\n\n        if y_proba:\n            split_data[\"y_proba\"] = y_proba\n\n        evaluation_data[\"splits\"][split_name] = split_data\n\n        # Unpack model if it's a tuple (from Tuner)\n        model_to_evaluate = self.model\n        if isinstance(self.model, tuple) and len(self.model) == 2:\n            # Check if first element looks like a model (has fit/predict)\n            # or if it's just a convention from TunerCalculator\n            model_to_evaluate = self.model[0]\n\n        if problem_type == \"classification\":\n            return evaluate_classification_model(\n                model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n            )\n        elif problem_type == \"regression\":\n            return evaluate_regression_model(\n                model=model_to_evaluate, dataset_name=split_name, X_test=X, y_test=y\n            )\n        else:\n            raise ValueError(f\"Unknown problem type: {problem_type}\")\n\n    # 2. Evaluate Train\n    splits_payload[\"train\"] = evaluate_split(\"train\", dataset.train)\n\n    # 3. Evaluate Test\n    has_test = False\n    if isinstance(dataset.test, pd.DataFrame):\n        has_test = not dataset.test.empty\n    elif isinstance(dataset.test, tuple):\n        has_test = len(dataset.test) == 2 and len(dataset.test[0]) &gt; 0\n\n    if has_test:\n        splits_payload[\"test\"] = evaluate_split(\"test\", dataset.test)\n\n    # 4. Evaluate Validation\n    if dataset.validation is not None:\n        has_val = False\n        if isinstance(dataset.validation, pd.DataFrame):\n            has_val = not dataset.validation.empty\n        elif isinstance(dataset.validation, tuple):\n            has_val = (\n                len(dataset.validation) == 2 and len(dataset.validation[0]) &gt; 0\n            )\n\n        if has_val:\n            splits_payload[\"validation\"] = evaluate_split(\n                \"validation\", dataset.validation\n            )\n\n    # Return report object (simplified for now, assuming schema matches)\n    return {\n        \"problem_type\": problem_type,\n        \"splits\": splits_payload,\n        \"raw_data\": evaluation_data,\n    }\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator.fit_predict","title":"<code>fit_predict(dataset, target_column, config, progress_callback=None, log_callback=None, job_id='unknown')</code>","text":"<p>Fits the model on training data and returns predictions for all splits.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def fit_predict(\n    self,\n    dataset: Union[SplitDataset, pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n    target_column: str,\n    config: Dict[str, Any],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    job_id: str = \"unknown\",\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"\n    Fits the model on training data and returns predictions for all splits.\n    \"\"\"\n    # Handle raw DataFrame or Tuple input by wrapping it in a dummy SplitDataset\n    if isinstance(dataset, pd.DataFrame):\n        dataset = SplitDataset(train=dataset, test=pd.DataFrame(), validation=None)\n    elif isinstance(dataset, tuple):\n        # Check if it's (train_df, test_df) or (X, y)\n        elem0 = dataset[0]\n        if isinstance(elem0, pd.DataFrame) and target_column in elem0.columns:\n            # It's (train_df, test_df)\n            train_df, test_df = dataset\n            dataset = SplitDataset(train=train_df, test=test_df, validation=None)  # type: ignore\n        else:\n            # Fallback: Treat input as training data (e.g. X, y tuple) and initialize empty test set.\n            msg = (\n                \"WARNING: No test set provided. Using entire input as training data. \"\n                \"Ensure data was split BEFORE preprocessing to avoid data leakage.\"\n            )\n            logger.warning(msg)\n            if log_callback:\n                log_callback(msg)\n\n            dataset = SplitDataset(\n                train=dataset, test=pd.DataFrame(), validation=None\n            )\n\n    # 1. Prepare Data\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n\n    validation_data = None\n    if dataset.validation is not None:\n        X_val, y_val = self._extract_xy(dataset.validation, target_column)\n        validation_data = (X_val, y_val)\n\n    # 2. Train Model\n    self.model = self.calculator.fit(\n        X_train,\n        y_train,\n        config,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n        validation_data=validation_data,\n    )\n\n    # 3. Predict on all splits\n    predictions = {}\n\n    # Train Predictions\n    predictions[\"train\"] = self.applier.predict(X_train, self.model)\n\n    # Test Predictions\n    is_test_empty = False\n    test_df = None\n    if isinstance(dataset.test, tuple):\n        test_df = dataset.test[0]\n    else:\n        test_df = dataset.test\n\n    if hasattr(test_df, \"empty\"):\n        is_test_empty = test_df.empty\n    else:\n        # Polars\n        is_test_empty = test_df.is_empty()\n\n    if not is_test_empty:\n        if isinstance(dataset.test, tuple):\n            X_test, _ = dataset.test\n        else:\n            if target_column in dataset.test.columns:\n                try:\n                    X_test = dataset.test.drop(columns=[target_column])\n                except TypeError:\n                    # Polars\n                    X_test = dataset.test.drop([target_column])\n            else:\n                X_test = dataset.test\n        predictions[\"test\"] = self.applier.predict(X_test, self.model)\n\n    # Validation Predictions\n    if dataset.validation is not None:\n        if isinstance(dataset.validation, tuple):\n            X_val, _ = dataset.validation\n        else:\n            if target_column in dataset.validation.columns:\n                X_val = dataset.validation.drop(columns=[target_column])\n            else:\n                X_val = dataset.validation\n        predictions[\"validation\"] = self.applier.predict(X_val, self.model)\n\n    return predictions\n</code></pre>"},{"location":"reference/api/modeling/base.html#skyulf.modeling.base.StatefulEstimator.refit","title":"<code>refit(dataset, target_column, config, job_id='unknown')</code>","text":"<p>Refits the model on Train + Validation data and updates the artifact.</p> Source code in <code>skyulf-core/skyulf/modeling/base.py</code> <pre><code>def refit(\n    self,\n    dataset: SplitDataset,\n    target_column: str,\n    config: Dict[str, Any],\n    job_id: str = \"unknown\",\n) -&gt; None:\n    \"\"\"\n    Refits the model on Train + Validation data and updates the artifact.\n    \"\"\"\n    if dataset.validation is None:\n        # Fallback to normal fit if no validation set\n        self.fit_predict(dataset, target_column, config, job_id=job_id)\n        return\n\n    # 1. Prepare Combined Data\n    X_train, y_train = self._extract_xy(dataset.train, target_column)\n    X_val, y_val = self._extract_xy(dataset.validation, target_column)\n\n    X_combined = pd.concat([X_train, X_val], axis=0)\n    y_combined = pd.concat([y_train, y_val], axis=0)\n\n    # 2. Train Model\n    self.model = self.calculator.fit(X_combined, y_combined, config)\n</code></pre>"},{"location":"reference/api/modeling/classification.html","title":"API: modeling.classification","text":""},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification","title":"<code>skyulf.modeling.classification</code>","text":"<p>Classification models.</p>"},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification.LogisticRegressionApplier","title":"<code>LogisticRegressionApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Logistic Regression Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class LogisticRegressionApplier(SklearnApplier):\n    \"\"\"Logistic Regression Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification.LogisticRegressionCalculator","title":"<code>LogisticRegressionCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Logistic Regression Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>@NodeRegistry.register(\"logistic_regression\", LogisticRegressionApplier)\nclass LogisticRegressionCalculator(SklearnCalculator):\n    \"\"\"Logistic Regression Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=LogisticRegression,\n            default_params={\n                \"max_iter\": 1000,\n                \"solver\": \"lbfgs\",\n                \"random_state\": 42,\n            },\n            problem_type=\"classification\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification.RandomForestClassifierApplier","title":"<code>RandomForestClassifierApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Random Forest Classifier Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>class RandomForestClassifierApplier(SklearnApplier):\n    \"\"\"Random Forest Classifier Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/classification.html#skyulf.modeling.classification.RandomForestClassifierCalculator","title":"<code>RandomForestClassifierCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Random Forest Classifier Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/classification.py</code> <pre><code>@NodeRegistry.register(\"random_forest_classifier\", RandomForestClassifierApplier)\nclass RandomForestClassifierCalculator(SklearnCalculator):\n    \"\"\"Random Forest Classifier Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=RandomForestClassifier,\n            default_params={\n                \"n_estimators\": 50,\n                \"max_depth\": 10,\n                \"min_samples_split\": 5,\n                \"min_samples_leaf\": 2,\n                \"n_jobs\": -1,\n                \"random_state\": 42,\n            },\n            problem_type=\"classification\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/cross_validation.html","title":"API: modeling.cross_validation","text":""},{"location":"reference/api/modeling/cross_validation.html#skyulf.modeling.cross_validation","title":"<code>skyulf.modeling.cross_validation</code>","text":"<p>Cross-validation logic for V2 modeling.</p>"},{"location":"reference/api/modeling/cross_validation.html#skyulf.modeling.cross_validation.perform_cross_validation","title":"<code>perform_cross_validation(calculator, applier, X, y, config, n_folds=5, cv_type='k_fold', shuffle=True, random_state=42, progress_callback=None, log_callback=None)</code>","text":"<p>Performs K-Fold cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>calculator</code> <code>BaseModelCalculator</code> <p>The model calculator (fit logic).</p> required <code>applier</code> <code>BaseModelApplier</code> <p>The model applier (predict logic).</p> required <code>X</code> <code>Union[DataFrame, SkyulfDataFrame]</code> <p>Features.</p> required <code>y</code> <code>Union[Series, Any]</code> <p>Target.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>Model configuration.</p> required <code>n_folds</code> <code>int</code> <p>Number of folds.</p> <code>5</code> <code>cv_type</code> <code>str</code> <p>Type of CV.</p> <code>'k_fold'</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle data before splitting (for KFold/Stratified).</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Random seed for shuffling.</p> <code>42</code> <code>progress_callback</code> <code>Optional[Callable[[int, int], None]]</code> <p>Optional callback(current_fold, total_folds).</p> <code>None</code> <code>log_callback</code> <code>Optional[Callable[[str], None]]</code> <p>Optional callback for logging messages.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing aggregated metrics and per-fold details.</p> Source code in <code>skyulf-core/skyulf/modeling/cross_validation.py</code> <pre><code>def perform_cross_validation(\n    calculator: BaseModelCalculator,\n    applier: BaseModelApplier,\n    X: Union[pd.DataFrame, SkyulfDataFrame],\n    y: Union[pd.Series, Any],\n    config: Dict[str, Any],\n    n_folds: int = 5,\n    cv_type: str = \"k_fold\",  # k_fold, stratified_k_fold, time_series_split, shuffle_split\n    shuffle: bool = True,\n    random_state: int = 42,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Performs K-Fold cross-validation.\n\n    Args:\n        calculator: The model calculator (fit logic).\n        applier: The model applier (predict logic).\n        X: Features.\n        y: Target.\n        config: Model configuration.\n        n_folds: Number of folds.\n        cv_type: Type of CV.\n        shuffle: Whether to shuffle data before splitting (for KFold/Stratified).\n        random_state: Random seed for shuffling.\n        progress_callback: Optional callback(current_fold, total_folds).\n        log_callback: Optional callback for logging messages.\n\n    Returns:\n        Dict containing aggregated metrics and per-fold details.\n    \"\"\"\n\n    problem_type = calculator.problem_type\n\n    if log_callback:\n        log_callback(f\"Starting Cross-Validation (Folds: {n_folds}, Type: {cv_type})\")\n\n    # 1. Setup Splitter\n    if cv_type == \"time_series_split\":\n        splitter = TimeSeriesSplit(n_splits=n_folds)\n    elif cv_type == \"shuffle_split\":\n        splitter = ShuffleSplit(\n            n_splits=n_folds, test_size=0.2, random_state=random_state\n        )\n    elif cv_type == \"stratified_k_fold\" and problem_type == \"classification\":\n        splitter = StratifiedKFold(\n            n_splits=n_folds,\n            shuffle=shuffle,\n            random_state=random_state if shuffle else None,\n        )\n    else:\n        # Default to KFold\n        splitter = KFold(\n            n_splits=n_folds,\n            shuffle=shuffle,\n            random_state=random_state if shuffle else None,\n        )\n\n    fold_results = []\n\n    # Ensure numpy for splitting using the Bridge\n    X_arr, y_arr = SklearnBridge.to_sklearn((X, y))\n\n    # 2. Iterate Folds\n    for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X_arr, y_arr)):\n        if progress_callback:\n            progress_callback(fold_idx + 1, n_folds)\n\n        if log_callback:\n            log_callback(f\"Processing Fold {fold_idx + 1}/{n_folds}...\")\n\n        # Split Data\n        # We slice the original X/y to preserve their type (Pandas/Polars) for the calculator\n        # Polars supports slicing with numpy arrays via __getitem__\n        # Pandas supports slicing via iloc\n\n        if hasattr(X, \"iloc\"):\n            X_train_fold = X.iloc[train_idx]\n            X_val_fold = X.iloc[val_idx]\n        else:\n            # Polars or other\n            X_train_fold = X[train_idx]\n            X_val_fold = X[val_idx]\n\n        if hasattr(y, \"iloc\"):\n            y_train_fold = y.iloc[train_idx]\n            y_val_fold = y.iloc[val_idx]\n        else:\n            # Polars Series or numpy array\n            y_train_fold = y[train_idx]\n            y_val_fold = y[val_idx]\n\n        # Fit\n        model_artifact = calculator.fit(X_train_fold, y_train_fold, config)\n\n        # Evaluate\n        if problem_type == \"classification\":\n            metrics = calculate_classification_metrics(\n                model_artifact, X_val_fold, y_val_fold\n            )\n        else:\n            metrics = calculate_regression_metrics(\n                model_artifact, X_val_fold, y_val_fold\n            )\n\n        if log_callback:\n            # Log a key metric for the fold\n            key_metric = \"accuracy\" if problem_type == \"classification\" else \"r2\"\n            score = metrics.get(key_metric, 0.0)\n            log_callback(f\"Fold {fold_idx + 1} completed. {key_metric}: {score:.4f}\")\n\n        fold_results.append(\n            {\n                \"fold\": fold_idx + 1,\n                \"metrics\": sanitize_metrics(metrics),\n                # We could store predictions here if needed, but might be too heavy\n            }\n        )\n\n    # 3. Aggregate\n    fold_metrics = [cast(Dict[str, float], r[\"metrics\"]) for r in fold_results]\n    aggregated = _aggregate_metrics(fold_metrics)\n\n    if log_callback:\n        log_callback(f\"Cross-Validation Completed. Aggregated Metrics: {aggregated}\")\n\n    return {\n        \"aggregated_metrics\": aggregated,\n        \"folds\": fold_results,\n        \"cv_config\": {\n            \"n_folds\": n_folds,\n            \"cv_type\": cv_type,\n            \"shuffle\": shuffle,\n            \"random_state\": random_state,\n        },\n    }\n</code></pre>"},{"location":"reference/api/modeling/regression.html","title":"API: modeling.regression","text":""},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression","title":"<code>skyulf.modeling.regression</code>","text":"<p>Regression models.</p>"},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression.RandomForestRegressorApplier","title":"<code>RandomForestRegressorApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Random Forest Regressor Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RandomForestRegressorApplier(SklearnApplier):\n    \"\"\"Random Forest Regressor Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression.RandomForestRegressorCalculator","title":"<code>RandomForestRegressorCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Random Forest Regressor Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>@NodeRegistry.register(\"random_forest_regressor\", RandomForestRegressorApplier)\nclass RandomForestRegressorCalculator(SklearnCalculator):\n    \"\"\"Random Forest Regressor Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=RandomForestRegressor,\n            default_params={\n                \"n_estimators\": 50,\n                \"max_depth\": 10,\n                \"min_samples_split\": 5,\n                \"min_samples_leaf\": 2,\n                \"n_jobs\": -1,\n                \"random_state\": 42,\n            },\n            problem_type=\"regression\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression.RidgeRegressionApplier","title":"<code>RidgeRegressionApplier</code>","text":"<p>               Bases: <code>SklearnApplier</code></p> <p>Ridge Regression Applier.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>class RidgeRegressionApplier(SklearnApplier):\n    \"\"\"Ridge Regression Applier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/api/modeling/regression.html#skyulf.modeling.regression.RidgeRegressionCalculator","title":"<code>RidgeRegressionCalculator</code>","text":"<p>               Bases: <code>SklearnCalculator</code></p> <p>Ridge Regression Calculator.</p> Source code in <code>skyulf-core/skyulf/modeling/regression.py</code> <pre><code>@NodeRegistry.register(\"ridge_regression\", RidgeRegressionApplier)\nclass RidgeRegressionCalculator(SklearnCalculator):\n    \"\"\"Ridge Regression Calculator.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            model_class=Ridge,\n            default_params={\n                \"alpha\": 1.0,\n                \"solver\": \"auto\",\n                \"random_state\": 42,\n            },\n            problem_type=\"regression\",\n        )\n</code></pre>"},{"location":"reference/api/modeling/sklearn_wrapper.html","title":"API: modeling.sklearn_wrapper","text":""},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper","title":"<code>skyulf.modeling.sklearn_wrapper</code>","text":"<p>Wrapper for Scikit-Learn models.</p>"},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper.SklearnApplier","title":"<code>SklearnApplier</code>","text":"<p>               Bases: <code>BaseModelApplier</code></p> <p>Base applier for Scikit-Learn models.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>class SklearnApplier(BaseModelApplier):\n    \"\"\"Base applier for Scikit-Learn models.\"\"\"\n\n    def predict(self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any) -&gt; pd.Series:\n        # Convert to Numpy\n        X_np, _ = SklearnBridge.to_sklearn(df)\n\n        preds = model_artifact.predict(X_np)\n\n        # Return as Pandas Series for consistency\n        # If input was Pandas, try to preserve index\n        index = None\n        if hasattr(df, \"index\"):\n            index = df.index\n        elif hasattr(df, \"to_pandas\"):\n             # If it's a wrapper or Polars, we might lose index unless we convert\n             # For now, default index is acceptable for predictions\n             pass\n\n        return pd.Series(preds, index=index)\n\n    def predict_proba(\n        self, df: Union[pd.DataFrame, SkyulfDataFrame], model_artifact: Any\n    ) -&gt; Optional[pd.DataFrame]:\n        if not hasattr(model_artifact, \"predict_proba\"):\n            return None\n\n        X_np, _ = SklearnBridge.to_sklearn(df)\n        probs = model_artifact.predict_proba(X_np)\n\n        # Return as DataFrame\n        index = None\n        if hasattr(df, \"index\"):\n            index = df.index\n\n        # Column names usually 0, 1, etc. or classes_\n        columns = None\n        if hasattr(model_artifact, \"classes_\"):\n            columns = model_artifact.classes_\n\n        return pd.DataFrame(probs, index=index, columns=columns)\n</code></pre>"},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper.SklearnCalculator","title":"<code>SklearnCalculator</code>","text":"<p>               Bases: <code>BaseModelCalculator</code></p> <p>Base calculator for Scikit-Learn models.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>class SklearnCalculator(BaseModelCalculator):\n    \"\"\"Base calculator for Scikit-Learn models.\"\"\"\n\n    def __init__(\n        self,\n        model_class: Type[BaseEstimator],\n        default_params: Dict[str, Any],\n        problem_type: str,\n    ):\n        self.model_class = model_class\n        self._default_params = default_params\n        self._problem_type = problem_type\n\n    @property\n    def default_params(self) -&gt; Dict[str, Any]:\n        return self._default_params\n\n    @property\n    def problem_type(self) -&gt; str:\n        return self._problem_type\n\n    def fit(\n        self,\n        X: Union[pd.DataFrame, SkyulfDataFrame],\n        y: Union[pd.Series, Any],\n        config: Dict[str, Any],\n        progress_callback=None,\n        log_callback=None,\n        validation_data=None,\n    ) -&gt; Any:\n        \"\"\"Fit the Scikit-Learn model.\"\"\"\n        # 1. Merge Config with Defaults\n        params = self.default_params.copy()\n        if config:\n            # We support two configuration structures:\n            # 1. Nested: {'params': {'C': 1.0, ...}} - Preferred\n            # 2. Flat: {'C': 1.0, 'type': '...', ...} - Legacy/Simple support\n\n            # Check for explicit 'params' dictionary first\n            overrides = config.get(\"params\", {})\n\n            # If 'params' key exists but is None or empty, check if there are other keys at top level\n            # that might be params. But be careful not to mix them.\n            # If config has 'params', we assume it's the source of truth.\n\n            if not overrides and \"params\" not in config:\n                # Fallback to flat config if 'params' key is completely missing\n                reserved_keys = {\n                    \"type\",\n                    \"target_column\",\n                    \"node_id\",\n                    \"step_type\",\n                    \"inputs\",\n                }\n                overrides = {\n                    k: v\n                    for k, v in config.items()\n                    if k not in reserved_keys and not isinstance(v, dict)\n                }\n\n            if overrides:\n                params.update(overrides)\n\n        msg = f\"Initializing {self.model_class.__name__} with params: {params}\"\n        logger.info(msg)\n        if log_callback:\n            log_callback(msg)\n\n        # 2. Instantiate Model\n        model = self.model_class(**params)\n\n        # 3. Fit\n        # Convert to Numpy using Bridge (handles Polars/Pandas/Wrappers)\n        X_np, y_np = SklearnBridge.to_sklearn((X, y))\n\n        model.fit(X_np, y_np)\n\n        return model\n</code></pre>"},{"location":"reference/api/modeling/sklearn_wrapper.html#skyulf.modeling.sklearn_wrapper.SklearnCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>","text":"<p>Fit the Scikit-Learn model.</p> Source code in <code>skyulf-core/skyulf/modeling/sklearn_wrapper.py</code> <pre><code>def fit(\n    self,\n    X: Union[pd.DataFrame, SkyulfDataFrame],\n    y: Union[pd.Series, Any],\n    config: Dict[str, Any],\n    progress_callback=None,\n    log_callback=None,\n    validation_data=None,\n) -&gt; Any:\n    \"\"\"Fit the Scikit-Learn model.\"\"\"\n    # 1. Merge Config with Defaults\n    params = self.default_params.copy()\n    if config:\n        # We support two configuration structures:\n        # 1. Nested: {'params': {'C': 1.0, ...}} - Preferred\n        # 2. Flat: {'C': 1.0, 'type': '...', ...} - Legacy/Simple support\n\n        # Check for explicit 'params' dictionary first\n        overrides = config.get(\"params\", {})\n\n        # If 'params' key exists but is None or empty, check if there are other keys at top level\n        # that might be params. But be careful not to mix them.\n        # If config has 'params', we assume it's the source of truth.\n\n        if not overrides and \"params\" not in config:\n            # Fallback to flat config if 'params' key is completely missing\n            reserved_keys = {\n                \"type\",\n                \"target_column\",\n                \"node_id\",\n                \"step_type\",\n                \"inputs\",\n            }\n            overrides = {\n                k: v\n                for k, v in config.items()\n                if k not in reserved_keys and not isinstance(v, dict)\n            }\n\n        if overrides:\n            params.update(overrides)\n\n    msg = f\"Initializing {self.model_class.__name__} with params: {params}\"\n    logger.info(msg)\n    if log_callback:\n        log_callback(msg)\n\n    # 2. Instantiate Model\n    model = self.model_class(**params)\n\n    # 3. Fit\n    # Convert to Numpy using Bridge (handles Polars/Pandas/Wrappers)\n    X_np, y_np = SklearnBridge.to_sklearn((X, y))\n\n    model.fit(X_np, y_np)\n\n    return model\n</code></pre>"},{"location":"reference/api/modeling/tuning_tuner.html","title":"API: modeling.tuning.tuner","text":""},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner","title":"<code>skyulf.modeling.tuning.tuner</code>","text":"<p>Hyperparameter Tuner implementation.</p>"},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner.TunerApplier","title":"<code>TunerApplier</code>","text":"<p>               Bases: <code>BaseModelApplier</code></p> <p>Applier for TunerCalculator. Wraps the base model applier to provide predictions using the refitted best model.</p> Source code in <code>skyulf-core/skyulf/modeling/tuning/tuner.py</code> <pre><code>class TunerApplier(BaseModelApplier):\n    \"\"\"\n    Applier for TunerCalculator.\n    Wraps the base model applier to provide predictions using the refitted best model.\n    \"\"\"\n\n    def __init__(self, base_applier: BaseModelApplier):\n        self.base_applier = base_applier\n\n    def predict(self, df: pd.DataFrame, model_artifact: Any) -&gt; pd.Series:\n        # model_artifact is (fitted_model, tuning_result)\n        if isinstance(model_artifact, tuple) and len(model_artifact) == 2:\n            model, _ = model_artifact\n            return self.base_applier.predict(df, model)\n        # Fallback if artifact is just the result (legacy)\n        return pd.Series(np.nan, index=df.index)\n\n    def predict_proba(\n        self, df: pd.DataFrame, model_artifact: Any\n    ) -&gt; Optional[pd.DataFrame]:\n        if isinstance(model_artifact, tuple) and len(model_artifact) == 2:\n            model, _ = model_artifact\n            return self.base_applier.predict_proba(df, model)\n        return None\n</code></pre>"},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner.TunerCalculator","title":"<code>TunerCalculator</code>","text":"<p>               Bases: <code>BaseModelCalculator</code></p> <p>Calculator for hyperparameter tuning.</p> Source code in <code>skyulf-core/skyulf/modeling/tuning/tuner.py</code> <pre><code>class TunerCalculator(BaseModelCalculator):\n    \"\"\"Calculator for hyperparameter tuning.\"\"\"\n\n    def __init__(self, model_calculator: BaseModelCalculator):\n        self.model_calculator = model_calculator\n\n    @property\n    def problem_type(self) -&gt; str:\n        return self.model_calculator.problem_type\n\n    def _clean_search_space(self, search_space: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Recursively cleans the search space.\n        - Converts \"none\" string to None.\n        \"\"\"\n        cleaned: Dict[str, Any] = {}\n        for k, v in search_space.items():\n            if isinstance(v, list):\n                cleaned[k] = [None if x == \"none\" else x for x in v]\n            elif isinstance(v, dict):\n                cleaned[k] = self._clean_search_space(v)\n            else:\n                cleaned[k] = None if v == \"none\" else v\n        return cleaned\n\n    def fit(\n        self,\n        X: Union[pd.DataFrame, SkyulfDataFrame],\n        y: Union[pd.Series, Any],\n        config: Dict[str, Any],\n        progress_callback: Optional[\n            Callable[[int, int, Optional[float], Optional[Dict]], None]\n        ] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        validation_data: Optional[tuple[Union[pd.DataFrame, SkyulfDataFrame], Union[pd.Series, Any]]] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Fits the tuner (runs tuning).\n        Adapts the generic fit interface to the specific tune method.\n        \"\"\"\n        # Convert config dict to TuningConfig\n        if isinstance(config, TuningConfig):\n            tuning_config = config\n        else:\n            # Extract valid keys for TuningConfig\n            valid_keys = TuningConfig.__annotations__.keys()\n            filtered_config = {k: v for k, v in config.items() if k in valid_keys}\n            tuning_config = TuningConfig(**filtered_config)  # type: ignore\n\n        # Convert data to Numpy for tuning\n        X_np, y_np = SklearnBridge.to_sklearn((X, y))\n\n        validation_data_np = None\n        if validation_data:\n            X_val, y_val = validation_data\n            X_val_np, y_val_np = SklearnBridge.to_sklearn((X_val, y_val))\n            validation_data_np = (X_val_np, y_val_np)\n\n        tuning_result = self.tune(\n            X_np,\n            y_np,\n            tuning_config,\n            progress_callback=progress_callback,\n            log_callback=log_callback,\n            validation_data=validation_data_np,\n        )\n\n        # Refit the best model on the full dataset\n        best_params = tuning_result.best_params\n        final_params = {**self.model_calculator.default_params, **best_params}\n\n        # Ensure random_state is passed if available in config and not in params\n        if \"random_state\" not in final_params and hasattr(\n            tuning_config, \"random_state\"\n        ):\n            final_params[\"random_state\"] = tuning_config.random_state\n\n        if log_callback:\n            log_callback(f\"Refitting best model with params: {final_params}\")\n\n        # Mypy doesn't know that model_calculator has model_class because it's typed as BaseModelCalculator\n        # We can cast it or ignore it.\n        model_cls = getattr(self.model_calculator, \"model_class\", None)\n        if not model_cls:\n            raise ValueError(\"Model calculator does not have a model_class attribute\")\n\n        model = model_cls(**final_params)\n        model.fit(X_np, y_np)\n\n        return (model, tuning_result)\n\n    def tune(  # noqa: C901\n        self,\n        X: Any,\n        y: Any,\n        config: TuningConfig,\n        progress_callback: Optional[\n            Callable[[int, int, Optional[float], Optional[Dict]], None]\n        ] = None,\n        log_callback: Optional[Callable[[str], None]] = None,\n        validation_data: Optional[tuple[Any, Any]] = None,\n    ) -&gt; TuningResult:\n        \"\"\"\n        Runs hyperparameter tuning.\n        \"\"\"\n        # 1. Prepare Estimator\n        # We need a base estimator. Since our Calculator wraps the class,\n        # we need to instantiate the underlying sklearn model with default params.\n        # Assuming model_calculator is SklearnCalculator\n        if not hasattr(self.model_calculator, \"model_class\"):\n            raise ValueError(\"Tuner currently only supports SklearnCalculator\")\n\n        base_estimator = self.model_calculator.model_class(\n            **self.model_calculator.default_params\n        )\n\n        # 2. Prepare Splitter\n        # If validation data is provided, use PredefinedSplit to train on X and validate on validation_data\n        # Otherwise use CV\n\n        X_for_search = X\n        y_for_search = y\n\n        if validation_data is not None:\n            from sklearn.model_selection import PredefinedSplit\n\n            X_val, y_val = validation_data\n\n            # Concatenate Train and Val (Numpy arrays)\n            X_for_search = np.concatenate([X, X_val], axis=0)\n            y_for_search = np.concatenate([y, y_val], axis=0)\n\n            # Create test_fold array: -1 for train, 0 for val\n            # -1 means \"never in test set\" (so always in training set)\n            # 0 means \"in test set for fold 0\"\n            test_fold = np.concatenate([np.full(len(X), -1), np.full(len(X_val), 0)])\n\n            cv = PredefinedSplit(test_fold)\n        else:\n            if not config.cv_enabled:\n                # Single split validation (20% holdout)\n                cv = ShuffleSplit(\n                    n_splits=1, test_size=0.2, random_state=config.random_state\n                )\n            elif config.cv_type == \"time_series_split\":\n                cv = TimeSeriesSplit(n_splits=config.cv_folds)\n            elif config.cv_type == \"shuffle_split\":\n                cv = ShuffleSplit(\n                    n_splits=config.cv_folds,\n                    test_size=0.2,\n                    random_state=config.random_state,\n                )\n            elif (\n                config.cv_type == \"stratified_k_fold\"\n                and self.model_calculator.problem_type == \"classification\"\n            ):\n                cv = StratifiedKFold(\n                    n_splits=config.cv_folds,\n                    shuffle=True,\n                    random_state=config.random_state,\n                )\n            else:\n                # Default to KFold (also fallback for stratified if regression)\n                cv = KFold(\n                    n_splits=config.cv_folds,\n                    shuffle=True,\n                    random_state=config.random_state,\n                )\n\n        # 3. Select Search Strategy\n        searcher = None\n\n        # Handle multiclass metrics and map user-friendly names\n        metric = config.metric\n\n        # Map common user-friendly metrics to sklearn scoring strings\n        metric_map = {\n            \"mse\": \"neg_mean_squared_error\",\n            \"mae\": \"neg_mean_absolute_error\",\n            \"rmse\": \"neg_root_mean_squared_error\",\n            \"r2\": \"r2\",\n            \"accuracy\": \"accuracy\",\n            \"f1\": \"f1\",\n            \"precision\": \"precision\",\n            \"recall\": \"recall\",\n            \"roc_auc\": \"roc_auc\",\n        }\n\n        if metric in metric_map:\n            metric = metric_map[metric]\n\n        if self.model_calculator.problem_type == \"classification\":\n            # Check if target is multiclass\n            is_multiclass = False\n            if isinstance(y, pd.Series):\n                is_multiclass = y.nunique() &gt; 2\n            elif isinstance(y, np.ndarray):\n                is_multiclass = len(np.unique(y)) &gt; 2\n\n            # If multiclass and metric is binary-default, switch to weighted\n            # Note: We check against the mapped names now (e.g. \"f1\", \"precision\")\n            if is_multiclass and metric in [\"f1\", \"precision\", \"recall\", \"roc_auc\"]:\n                metric = f\"{metric}_weighted\"\n                # roc_auc needs special handling (ovr/ovo) usually, but weighted often works for simple cases\n                if (\n                    config.metric == \"roc_auc\"\n                ):  # Check original config metric name just in case\n                    metric = \"roc_auc_ovr_weighted\"\n\n        if config.strategy in [\"grid\", \"random\"]:\n            # Use custom loop to support progress and log callbacks\n            if log_callback:\n                log_callback(\n                    f\"Starting {config.strategy} search with custom loop for detailed logging...\"\n                )\n\n            # 1. Generate Candidates\n            param_space = self._clean_search_space(config.search_space)\n            candidates = []\n\n            if config.strategy == \"grid\":\n                candidates = list(ParameterGrid(param_space))\n            else:\n                # Random Search\n                candidates = list(\n                    ParameterSampler(\n                        param_space,\n                        n_iter=config.n_trials,\n                        random_state=config.random_state,\n                    )\n                )\n\n            total_candidates = len(candidates)\n            if log_callback:\n                log_callback(f\"Total candidates to evaluate: {total_candidates}\")\n\n            trials: List[Dict[str, Any]] = []\n            best_score = -float(\"inf\")\n            best_params = None\n\n            # 2. Iterate Candidates\n            for i, params in enumerate(candidates):\n                if log_callback:\n                    log_callback(\n                        f\"Evaluating Candidate {i + 1}/{total_candidates}: {params}\"\n                    )\n\n                # Use custom cross-validation loop to enable per-fold logging and progress tracking.\n                # We instantiate the model with the current candidate parameters and evaluate it\n                # using the configured CV strategy.\n\n                fold_scores = []\n\n                # Ensure numpy\n                X_arr = (\n                    X_for_search.to_numpy()\n                    if hasattr(X_for_search, \"to_numpy\")\n                    else X_for_search\n                )\n                y_arr = (\n                    y_for_search.to_numpy()\n                    if hasattr(y_for_search, \"to_numpy\")\n                    else y_for_search\n                )\n\n                for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_arr, y_arr)):\n                    # Split\n                    X_train_fold = (\n                        X_for_search.iloc[train_idx]\n                        if hasattr(X_for_search, \"iloc\")\n                        else X_for_search[train_idx]\n                    )\n                    y_train_fold = (\n                        y_for_search.iloc[train_idx]\n                        if hasattr(y_for_search, \"iloc\")\n                        else y_for_search[train_idx]\n                    )\n                    X_val_fold = (\n                        X_for_search.iloc[val_idx]\n                        if hasattr(X_for_search, \"iloc\")\n                        else X_for_search[val_idx]\n                    )\n                    y_val_fold = (\n                        y_for_search.iloc[val_idx]\n                        if hasattr(y_for_search, \"iloc\")\n                        else y_for_search[val_idx]\n                    )\n\n                    # Instantiate and Fit\n                    # Note: We must handle potential errors (e.g. incompatible params)\n                    try:\n                        model = self.model_calculator.model_class(\n                            **{**self.model_calculator.default_params, **params}\n                        )\n                        model.fit(X_train_fold, y_train_fold)\n\n                        # Score\n                        from sklearn.metrics import get_scorer\n\n                        scorer = get_scorer(metric)\n                        score = scorer(model, X_val_fold, y_val_fold)\n                        fold_scores.append(score)\n\n                        if log_callback:\n                            n_splits = cv.get_n_splits(X_arr, y_arr)\n                            log_callback(\n                                f\"  [Candidate {i + 1}] CV Fold {fold_idx + 1}/{n_splits} Score: {score:.4f}\"\n                            )\n                    except Exception as e:\n                        if log_callback:\n                            n_splits = cv.get_n_splits(X_arr, y_arr)\n                            log_callback(\n                                f\"  [Candidate {i + 1}] CV Fold {fold_idx + 1}/{n_splits} Failed: {str(e)}\"\n                            )\n                        fold_scores.append(-float(\"inf\"))\n\n                # Filter out failed folds for mean calculation if possible, or penalize\n                valid_scores = [s for s in fold_scores if s != -float(\"inf\")]\n                if valid_scores:\n                    mean_score = np.mean(valid_scores)\n                else:\n                    mean_score = -float(\"inf\")\n\n                if log_callback:\n                    log_callback(f\"Candidate {i + 1} Mean Score: {mean_score:.4f}\")\n\n                if progress_callback:\n                    progress_callback(i + 1, total_candidates, mean_score, params)\n\n                trials.append({\"params\": params, \"score\": mean_score})\n\n                if mean_score &gt; best_score:\n                    best_score = mean_score\n                    best_params = params\n\n            if log_callback:\n                log_callback(f\"Tuning Completed. Best Score: {best_score:.4f}\")\n                log_callback(f\"Best Params: {best_params}\")\n\n            return TuningResult(\n                best_params=best_params if best_params is not None else {},\n                best_score=best_score,\n                n_trials=total_candidates,\n                trials=trials,\n            )\n\n        elif config.strategy == \"halving_grid\":\n            searcher = HalvingGridSearchCV(\n                estimator=base_estimator,\n                param_grid=self._clean_search_space(config.search_space),\n                scoring=metric,\n                cv=cv,\n                n_jobs=-1,\n                random_state=config.random_state,\n                refit=False,\n                error_score=np.nan,\n            )\n        elif config.strategy == \"halving_random\":\n            searcher = HalvingRandomSearchCV(\n                estimator=base_estimator,\n                param_distributions=self._clean_search_space(config.search_space),\n                n_candidates=config.n_trials,  # Map n_trials to n_candidates\n                scoring=metric,\n                cv=cv,\n                n_jobs=-1,\n                random_state=config.random_state,\n                refit=False,\n                error_score=np.nan,\n            )\n        elif config.strategy == \"optuna\":\n            if not HAS_OPTUNA:\n                raise ImportError(\n                    \"Optuna is not installed. Please install 'optuna' and 'optuna-integration'.\"\n                )\n\n            # Convert search space to Optuna distributions\n            distributions = {}\n            for k, v in config.search_space.items():\n                if isinstance(v, list):\n                    distributions[k] = optuna.distributions.CategoricalDistribution(v)\n                else:\n                    distributions[k] = v\n\n            # Optuna callbacks\n            callbacks = []\n            if progress_callback:\n\n                def _optuna_callback(study, trial):\n                    # Optuna doesn't know total trials upfront easily if not set, but we have config.n_trials\n                    # trial.value is the score (or None if failed/pruned)\n                    score = trial.value if trial.value is not None else None\n\n                    if log_callback:\n                        log_callback(\n                            f\"Optuna Trial {trial.number + 1} finished. Mean CV Score: {score}\"\n                        )\n\n                    progress_callback(\n                        trial.number + 1, config.n_trials, score, trial.params\n                    )\n\n                callbacks.append(_optuna_callback)\n\n            searcher = OptunaSearchCV(\n                estimator=base_estimator,\n                param_distributions=distributions,\n                n_trials=config.n_trials,\n                timeout=config.timeout,\n                cv=cv,  # type: ignore\n                scoring=metric,\n                n_jobs=-1,\n                random_state=config.random_state,\n                refit=False,\n                verbose=0,\n                callbacks=callbacks,\n            )\n        else:\n            raise ValueError(f\"Unknown tuning strategy: {config.strategy}\")\n\n        # 4. Run Search\n        # Ensure numpy\n        X_arr = (\n            X_for_search.to_numpy()\n            if hasattr(X_for_search, \"to_numpy\")\n            else X_for_search\n        )\n        y_arr = (\n            y_for_search.to_numpy()\n            if hasattr(y_for_search, \"to_numpy\")\n            else y_for_search\n        )\n\n        try:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\",\n                    message=\"Failed to report cross validation scores for TerminatorCallback\",\n                )\n                searcher.fit(X_arr, y_arr)\n        except Exception as e:\n            logger.error(f\"Hyperparameter tuning failed: {str(e)}\")\n            error_msg = str(e)\n            if \"No trials are completed yet\" in error_msg:\n                raise ValueError(\n                    \"Hyperparameter tuning failed: No trials completed successfully. \"\n                    \"This usually means the model failed to train with the provided hyperparameter combinations. \"\n                    \"Please check your search space and data.\"\n                ) from e\n\n            if (\n                \"n_samples\" in error_msg\n                and \"resample\" in error_msg\n                and \"Got 0\" in error_msg\n            ):\n                raise ValueError(\n                    \"Hyperparameter tuning with Halving strategy failed because the dataset is too small \"\n                    \"for the configured halving parameters. Please try using 'Random Search' or 'Grid Search' instead, \"\n                    \"or increase your dataset size.\"\n                ) from e\n\n            raise e\n\n        # 5. Extract Results\n        if not hasattr(searcher, \"best_params_\"):\n            raise ValueError(\n                \"Hyperparameter tuning failed to find any valid combination of parameters. All trials likely failed.\"\n            )\n\n        best_params = searcher.best_params_\n        best_score = searcher.best_score_\n\n        # Collect trials\n        trials = []\n        # Special handling for Optuna\n        if config.strategy == \"optuna\" and hasattr(searcher, \"study_\"):\n            for trial in searcher.study_.trials:\n                # Only include completed trials\n                if trial.state.name == \"COMPLETE\":\n                    trials.append({\"params\": trial.params, \"score\": trial.value})\n        elif hasattr(searcher, \"cv_results_\"):\n            results = searcher.cv_results_\n            if \"params\" in results:\n                n_candidates = len(results[\"params\"])\n                for i in range(n_candidates):\n                    trials.append(\n                        {\n                            \"params\": results[\"params\"][i],\n                            \"score\": results[\"mean_test_score\"][i],\n                        }\n                    )\n\n        return TuningResult(\n            best_params=best_params,\n            best_score=best_score,\n            n_trials=len(trials),\n            trials=trials,\n        )\n</code></pre>"},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner.TunerCalculator.fit","title":"<code>fit(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>","text":"<p>Fits the tuner (runs tuning). Adapts the generic fit interface to the specific tune method.</p> Source code in <code>skyulf-core/skyulf/modeling/tuning/tuner.py</code> <pre><code>def fit(\n    self,\n    X: Union[pd.DataFrame, SkyulfDataFrame],\n    y: Union[pd.Series, Any],\n    config: Dict[str, Any],\n    progress_callback: Optional[\n        Callable[[int, int, Optional[float], Optional[Dict]], None]\n    ] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    validation_data: Optional[tuple[Union[pd.DataFrame, SkyulfDataFrame], Union[pd.Series, Any]]] = None,\n) -&gt; Any:\n    \"\"\"\n    Fits the tuner (runs tuning).\n    Adapts the generic fit interface to the specific tune method.\n    \"\"\"\n    # Convert config dict to TuningConfig\n    if isinstance(config, TuningConfig):\n        tuning_config = config\n    else:\n        # Extract valid keys for TuningConfig\n        valid_keys = TuningConfig.__annotations__.keys()\n        filtered_config = {k: v for k, v in config.items() if k in valid_keys}\n        tuning_config = TuningConfig(**filtered_config)  # type: ignore\n\n    # Convert data to Numpy for tuning\n    X_np, y_np = SklearnBridge.to_sklearn((X, y))\n\n    validation_data_np = None\n    if validation_data:\n        X_val, y_val = validation_data\n        X_val_np, y_val_np = SklearnBridge.to_sklearn((X_val, y_val))\n        validation_data_np = (X_val_np, y_val_np)\n\n    tuning_result = self.tune(\n        X_np,\n        y_np,\n        tuning_config,\n        progress_callback=progress_callback,\n        log_callback=log_callback,\n        validation_data=validation_data_np,\n    )\n\n    # Refit the best model on the full dataset\n    best_params = tuning_result.best_params\n    final_params = {**self.model_calculator.default_params, **best_params}\n\n    # Ensure random_state is passed if available in config and not in params\n    if \"random_state\" not in final_params and hasattr(\n        tuning_config, \"random_state\"\n    ):\n        final_params[\"random_state\"] = tuning_config.random_state\n\n    if log_callback:\n        log_callback(f\"Refitting best model with params: {final_params}\")\n\n    # Mypy doesn't know that model_calculator has model_class because it's typed as BaseModelCalculator\n    # We can cast it or ignore it.\n    model_cls = getattr(self.model_calculator, \"model_class\", None)\n    if not model_cls:\n        raise ValueError(\"Model calculator does not have a model_class attribute\")\n\n    model = model_cls(**final_params)\n    model.fit(X_np, y_np)\n\n    return (model, tuning_result)\n</code></pre>"},{"location":"reference/api/modeling/tuning_tuner.html#skyulf.modeling.tuning.tuner.TunerCalculator.tune","title":"<code>tune(X, y, config, progress_callback=None, log_callback=None, validation_data=None)</code>","text":"<p>Runs hyperparameter tuning.</p> Source code in <code>skyulf-core/skyulf/modeling/tuning/tuner.py</code> <pre><code>def tune(  # noqa: C901\n    self,\n    X: Any,\n    y: Any,\n    config: TuningConfig,\n    progress_callback: Optional[\n        Callable[[int, int, Optional[float], Optional[Dict]], None]\n    ] = None,\n    log_callback: Optional[Callable[[str], None]] = None,\n    validation_data: Optional[tuple[Any, Any]] = None,\n) -&gt; TuningResult:\n    \"\"\"\n    Runs hyperparameter tuning.\n    \"\"\"\n    # 1. Prepare Estimator\n    # We need a base estimator. Since our Calculator wraps the class,\n    # we need to instantiate the underlying sklearn model with default params.\n    # Assuming model_calculator is SklearnCalculator\n    if not hasattr(self.model_calculator, \"model_class\"):\n        raise ValueError(\"Tuner currently only supports SklearnCalculator\")\n\n    base_estimator = self.model_calculator.model_class(\n        **self.model_calculator.default_params\n    )\n\n    # 2. Prepare Splitter\n    # If validation data is provided, use PredefinedSplit to train on X and validate on validation_data\n    # Otherwise use CV\n\n    X_for_search = X\n    y_for_search = y\n\n    if validation_data is not None:\n        from sklearn.model_selection import PredefinedSplit\n\n        X_val, y_val = validation_data\n\n        # Concatenate Train and Val (Numpy arrays)\n        X_for_search = np.concatenate([X, X_val], axis=0)\n        y_for_search = np.concatenate([y, y_val], axis=0)\n\n        # Create test_fold array: -1 for train, 0 for val\n        # -1 means \"never in test set\" (so always in training set)\n        # 0 means \"in test set for fold 0\"\n        test_fold = np.concatenate([np.full(len(X), -1), np.full(len(X_val), 0)])\n\n        cv = PredefinedSplit(test_fold)\n    else:\n        if not config.cv_enabled:\n            # Single split validation (20% holdout)\n            cv = ShuffleSplit(\n                n_splits=1, test_size=0.2, random_state=config.random_state\n            )\n        elif config.cv_type == \"time_series_split\":\n            cv = TimeSeriesSplit(n_splits=config.cv_folds)\n        elif config.cv_type == \"shuffle_split\":\n            cv = ShuffleSplit(\n                n_splits=config.cv_folds,\n                test_size=0.2,\n                random_state=config.random_state,\n            )\n        elif (\n            config.cv_type == \"stratified_k_fold\"\n            and self.model_calculator.problem_type == \"classification\"\n        ):\n            cv = StratifiedKFold(\n                n_splits=config.cv_folds,\n                shuffle=True,\n                random_state=config.random_state,\n            )\n        else:\n            # Default to KFold (also fallback for stratified if regression)\n            cv = KFold(\n                n_splits=config.cv_folds,\n                shuffle=True,\n                random_state=config.random_state,\n            )\n\n    # 3. Select Search Strategy\n    searcher = None\n\n    # Handle multiclass metrics and map user-friendly names\n    metric = config.metric\n\n    # Map common user-friendly metrics to sklearn scoring strings\n    metric_map = {\n        \"mse\": \"neg_mean_squared_error\",\n        \"mae\": \"neg_mean_absolute_error\",\n        \"rmse\": \"neg_root_mean_squared_error\",\n        \"r2\": \"r2\",\n        \"accuracy\": \"accuracy\",\n        \"f1\": \"f1\",\n        \"precision\": \"precision\",\n        \"recall\": \"recall\",\n        \"roc_auc\": \"roc_auc\",\n    }\n\n    if metric in metric_map:\n        metric = metric_map[metric]\n\n    if self.model_calculator.problem_type == \"classification\":\n        # Check if target is multiclass\n        is_multiclass = False\n        if isinstance(y, pd.Series):\n            is_multiclass = y.nunique() &gt; 2\n        elif isinstance(y, np.ndarray):\n            is_multiclass = len(np.unique(y)) &gt; 2\n\n        # If multiclass and metric is binary-default, switch to weighted\n        # Note: We check against the mapped names now (e.g. \"f1\", \"precision\")\n        if is_multiclass and metric in [\"f1\", \"precision\", \"recall\", \"roc_auc\"]:\n            metric = f\"{metric}_weighted\"\n            # roc_auc needs special handling (ovr/ovo) usually, but weighted often works for simple cases\n            if (\n                config.metric == \"roc_auc\"\n            ):  # Check original config metric name just in case\n                metric = \"roc_auc_ovr_weighted\"\n\n    if config.strategy in [\"grid\", \"random\"]:\n        # Use custom loop to support progress and log callbacks\n        if log_callback:\n            log_callback(\n                f\"Starting {config.strategy} search with custom loop for detailed logging...\"\n            )\n\n        # 1. Generate Candidates\n        param_space = self._clean_search_space(config.search_space)\n        candidates = []\n\n        if config.strategy == \"grid\":\n            candidates = list(ParameterGrid(param_space))\n        else:\n            # Random Search\n            candidates = list(\n                ParameterSampler(\n                    param_space,\n                    n_iter=config.n_trials,\n                    random_state=config.random_state,\n                )\n            )\n\n        total_candidates = len(candidates)\n        if log_callback:\n            log_callback(f\"Total candidates to evaluate: {total_candidates}\")\n\n        trials: List[Dict[str, Any]] = []\n        best_score = -float(\"inf\")\n        best_params = None\n\n        # 2. Iterate Candidates\n        for i, params in enumerate(candidates):\n            if log_callback:\n                log_callback(\n                    f\"Evaluating Candidate {i + 1}/{total_candidates}: {params}\"\n                )\n\n            # Use custom cross-validation loop to enable per-fold logging and progress tracking.\n            # We instantiate the model with the current candidate parameters and evaluate it\n            # using the configured CV strategy.\n\n            fold_scores = []\n\n            # Ensure numpy\n            X_arr = (\n                X_for_search.to_numpy()\n                if hasattr(X_for_search, \"to_numpy\")\n                else X_for_search\n            )\n            y_arr = (\n                y_for_search.to_numpy()\n                if hasattr(y_for_search, \"to_numpy\")\n                else y_for_search\n            )\n\n            for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_arr, y_arr)):\n                # Split\n                X_train_fold = (\n                    X_for_search.iloc[train_idx]\n                    if hasattr(X_for_search, \"iloc\")\n                    else X_for_search[train_idx]\n                )\n                y_train_fold = (\n                    y_for_search.iloc[train_idx]\n                    if hasattr(y_for_search, \"iloc\")\n                    else y_for_search[train_idx]\n                )\n                X_val_fold = (\n                    X_for_search.iloc[val_idx]\n                    if hasattr(X_for_search, \"iloc\")\n                    else X_for_search[val_idx]\n                )\n                y_val_fold = (\n                    y_for_search.iloc[val_idx]\n                    if hasattr(y_for_search, \"iloc\")\n                    else y_for_search[val_idx]\n                )\n\n                # Instantiate and Fit\n                # Note: We must handle potential errors (e.g. incompatible params)\n                try:\n                    model = self.model_calculator.model_class(\n                        **{**self.model_calculator.default_params, **params}\n                    )\n                    model.fit(X_train_fold, y_train_fold)\n\n                    # Score\n                    from sklearn.metrics import get_scorer\n\n                    scorer = get_scorer(metric)\n                    score = scorer(model, X_val_fold, y_val_fold)\n                    fold_scores.append(score)\n\n                    if log_callback:\n                        n_splits = cv.get_n_splits(X_arr, y_arr)\n                        log_callback(\n                            f\"  [Candidate {i + 1}] CV Fold {fold_idx + 1}/{n_splits} Score: {score:.4f}\"\n                        )\n                except Exception as e:\n                    if log_callback:\n                        n_splits = cv.get_n_splits(X_arr, y_arr)\n                        log_callback(\n                            f\"  [Candidate {i + 1}] CV Fold {fold_idx + 1}/{n_splits} Failed: {str(e)}\"\n                        )\n                    fold_scores.append(-float(\"inf\"))\n\n            # Filter out failed folds for mean calculation if possible, or penalize\n            valid_scores = [s for s in fold_scores if s != -float(\"inf\")]\n            if valid_scores:\n                mean_score = np.mean(valid_scores)\n            else:\n                mean_score = -float(\"inf\")\n\n            if log_callback:\n                log_callback(f\"Candidate {i + 1} Mean Score: {mean_score:.4f}\")\n\n            if progress_callback:\n                progress_callback(i + 1, total_candidates, mean_score, params)\n\n            trials.append({\"params\": params, \"score\": mean_score})\n\n            if mean_score &gt; best_score:\n                best_score = mean_score\n                best_params = params\n\n        if log_callback:\n            log_callback(f\"Tuning Completed. Best Score: {best_score:.4f}\")\n            log_callback(f\"Best Params: {best_params}\")\n\n        return TuningResult(\n            best_params=best_params if best_params is not None else {},\n            best_score=best_score,\n            n_trials=total_candidates,\n            trials=trials,\n        )\n\n    elif config.strategy == \"halving_grid\":\n        searcher = HalvingGridSearchCV(\n            estimator=base_estimator,\n            param_grid=self._clean_search_space(config.search_space),\n            scoring=metric,\n            cv=cv,\n            n_jobs=-1,\n            random_state=config.random_state,\n            refit=False,\n            error_score=np.nan,\n        )\n    elif config.strategy == \"halving_random\":\n        searcher = HalvingRandomSearchCV(\n            estimator=base_estimator,\n            param_distributions=self._clean_search_space(config.search_space),\n            n_candidates=config.n_trials,  # Map n_trials to n_candidates\n            scoring=metric,\n            cv=cv,\n            n_jobs=-1,\n            random_state=config.random_state,\n            refit=False,\n            error_score=np.nan,\n        )\n    elif config.strategy == \"optuna\":\n        if not HAS_OPTUNA:\n            raise ImportError(\n                \"Optuna is not installed. Please install 'optuna' and 'optuna-integration'.\"\n            )\n\n        # Convert search space to Optuna distributions\n        distributions = {}\n        for k, v in config.search_space.items():\n            if isinstance(v, list):\n                distributions[k] = optuna.distributions.CategoricalDistribution(v)\n            else:\n                distributions[k] = v\n\n        # Optuna callbacks\n        callbacks = []\n        if progress_callback:\n\n            def _optuna_callback(study, trial):\n                # Optuna doesn't know total trials upfront easily if not set, but we have config.n_trials\n                # trial.value is the score (or None if failed/pruned)\n                score = trial.value if trial.value is not None else None\n\n                if log_callback:\n                    log_callback(\n                        f\"Optuna Trial {trial.number + 1} finished. Mean CV Score: {score}\"\n                    )\n\n                progress_callback(\n                    trial.number + 1, config.n_trials, score, trial.params\n                )\n\n            callbacks.append(_optuna_callback)\n\n        searcher = OptunaSearchCV(\n            estimator=base_estimator,\n            param_distributions=distributions,\n            n_trials=config.n_trials,\n            timeout=config.timeout,\n            cv=cv,  # type: ignore\n            scoring=metric,\n            n_jobs=-1,\n            random_state=config.random_state,\n            refit=False,\n            verbose=0,\n            callbacks=callbacks,\n        )\n    else:\n        raise ValueError(f\"Unknown tuning strategy: {config.strategy}\")\n\n    # 4. Run Search\n    # Ensure numpy\n    X_arr = (\n        X_for_search.to_numpy()\n        if hasattr(X_for_search, \"to_numpy\")\n        else X_for_search\n    )\n    y_arr = (\n        y_for_search.to_numpy()\n        if hasattr(y_for_search, \"to_numpy\")\n        else y_for_search\n    )\n\n    try:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Failed to report cross validation scores for TerminatorCallback\",\n            )\n            searcher.fit(X_arr, y_arr)\n    except Exception as e:\n        logger.error(f\"Hyperparameter tuning failed: {str(e)}\")\n        error_msg = str(e)\n        if \"No trials are completed yet\" in error_msg:\n            raise ValueError(\n                \"Hyperparameter tuning failed: No trials completed successfully. \"\n                \"This usually means the model failed to train with the provided hyperparameter combinations. \"\n                \"Please check your search space and data.\"\n            ) from e\n\n        if (\n            \"n_samples\" in error_msg\n            and \"resample\" in error_msg\n            and \"Got 0\" in error_msg\n        ):\n            raise ValueError(\n                \"Hyperparameter tuning with Halving strategy failed because the dataset is too small \"\n                \"for the configured halving parameters. Please try using 'Random Search' or 'Grid Search' instead, \"\n                \"or increase your dataset size.\"\n            ) from e\n\n        raise e\n\n    # 5. Extract Results\n    if not hasattr(searcher, \"best_params_\"):\n        raise ValueError(\n            \"Hyperparameter tuning failed to find any valid combination of parameters. All trials likely failed.\"\n        )\n\n    best_params = searcher.best_params_\n    best_score = searcher.best_score_\n\n    # Collect trials\n    trials = []\n    # Special handling for Optuna\n    if config.strategy == \"optuna\" and hasattr(searcher, \"study_\"):\n        for trial in searcher.study_.trials:\n            # Only include completed trials\n            if trial.state.name == \"COMPLETE\":\n                trials.append({\"params\": trial.params, \"score\": trial.value})\n    elif hasattr(searcher, \"cv_results_\"):\n        results = searcher.cv_results_\n        if \"params\" in results:\n            n_candidates = len(results[\"params\"])\n            for i in range(n_candidates):\n                trials.append(\n                    {\n                        \"params\": results[\"params\"][i],\n                        \"score\": results[\"mean_test_score\"][i],\n                    }\n                )\n\n    return TuningResult(\n        best_params=best_params,\n        best_score=best_score,\n        n_trials=len(trials),\n        trials=trials,\n    )\n</code></pre>"},{"location":"reference/api/preprocessing/index.html","title":"API: preprocessing","text":"<p>The preprocessing package contains Calculator/Applier nodes and the <code>FeatureEngineer</code> orchestrator.</p>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing","title":"<code>skyulf.preprocessing</code>","text":""},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.BaseApplier","title":"<code>BaseApplier</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>class BaseApplier(ABC):\n    @abstractmethod\n    def apply(\n        self, df: Union[pd.DataFrame, SkyulfDataFrame, tuple], params: Dict[str, Any]\n    ) -&gt; Union[pd.DataFrame, SkyulfDataFrame, tuple, SplitDataset]:\n        \"\"\"\n        Applies the transformation using fitted parameters.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.BaseApplier.apply","title":"<code>apply(df, params)</code>  <code>abstractmethod</code>","text":"<p>Applies the transformation using fitted parameters.</p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>@abstractmethod\ndef apply(\n    self, df: Union[pd.DataFrame, SkyulfDataFrame, tuple], params: Dict[str, Any]\n) -&gt; Union[pd.DataFrame, SkyulfDataFrame, tuple, SplitDataset]:\n    \"\"\"\n    Applies the transformation using fitted parameters.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.BaseCalculator","title":"<code>BaseCalculator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>class BaseCalculator(ABC):\n    @abstractmethod\n    def fit(\n        self, df: Union[pd.DataFrame, SkyulfDataFrame, tuple], config: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Calculates parameters from the training data.\n        Returns a dictionary of fitted parameters (serializable).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.BaseCalculator.fit","title":"<code>fit(df, config)</code>  <code>abstractmethod</code>","text":"<p>Calculates parameters from the training data. Returns a dictionary of fitted parameters (serializable).</p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>@abstractmethod\ndef fit(\n    self, df: Union[pd.DataFrame, SkyulfDataFrame, tuple], config: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Calculates parameters from the training data.\n    Returns a dictionary of fitted parameters (serializable).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.CustomBinningCalculator","title":"<code>CustomBinningCalculator</code>","text":"<p>               Bases: <code>BaseCalculator</code></p> <p>Calculator for CustomBinning node. Applies specific bin edges to selected columns.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>@NodeRegistry.register(\"CustomBinning\", CustomBinningApplier)\nclass CustomBinningCalculator(BaseCalculator):\n    \"\"\"\n    Calculator for CustomBinning node.\n    Applies specific bin edges to selected columns.\n    \"\"\"\n\n    def fit(\n        self,\n        df: SkyulfDataFrame,\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n\n        # Ensure X is pandas for fitting logic\n        engine = get_engine(X)\n        if engine.name == \"polars\":\n            X = X.to_pandas()\n\n        columns = resolve_columns(X, config, detect_numeric_columns)\n        bins = config.get(\"bins\")\n\n        bin_edges_map = {}\n        if bins:\n            sorted_bins = sorted(bins)\n            for col in columns:\n                if col in X.columns:\n                    bin_edges_map[col] = sorted_bins\n\n        return {\n            \"type\": \"general_binning\",  # Use GeneralBinningApplier\n            \"bin_edges\": bin_edges_map,\n            \"output_suffix\": config.get(\"output_suffix\", \"_binned\"),\n            \"drop_original\": config.get(\"drop_original\", False),\n            \"label_format\": config.get(\"label_format\", \"ordinal\"),\n            \"missing_strategy\": config.get(\"missing_strategy\", \"keep\"),\n            \"missing_label\": config.get(\"missing_label\", \"Missing\"),\n            \"include_lowest\": config.get(\"include_lowest\", True),\n            \"precision\": config.get(\"precision\", 3),\n        }\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.FeatureEngineer","title":"<code>FeatureEngineer</code>","text":"<p>Orchestrates a sequence of feature engineering steps.</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>class FeatureEngineer:\n    \"\"\"\n    Orchestrates a sequence of feature engineering steps.\n    \"\"\"\n\n    def __init__(self, steps_config: List[Dict[str, Any]]):\n        self.steps_config = steps_config\n        self.fitted_steps: List[Dict[str, Any]] = []\n\n    def transform(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply fitted transformations to new data.\n        \"\"\"\n        current_data = data\n\n        for step in self.fitted_steps:\n            name = step[\"name\"]\n            transformer_type = step[\"type\"]\n            applier = step[\"applier\"]\n            artifact = step[\"artifact\"]\n\n            # Skip splitters during inference/transform\n            if transformer_type in [\n                \"TrainTestSplitter\",\n                \"feature_target_split\",\n                \"Oversampling\",\n                \"Undersampling\",\n            ]:\n                continue\n\n            logger.debug(f\"Applying step: {name} ({transformer_type})\")\n            current_data = applier.apply(current_data, artifact)\n\n        return current_data\n\n    def fit_transform(self, data: Union[pd.DataFrame, Any], node_id_prefix=\"\") -&gt; Any:  # noqa: C901\n        \"\"\"\n        Runs the pipeline on data.\n        Returns: (transformed_data, metrics_dict)\n        \"\"\"\n        self.fitted_steps = []  # Reset fitted steps\n        current_data = data\n        metrics: Dict[str, Any] = {}\n\n        for i, step in enumerate(self.steps_config):\n            name = step[\"name\"]\n            transformer_type = step[\"transformer\"]\n            params = step.get(\"params\", {})\n\n            logger.info(f\"Running step {i}: {name} ({transformer_type})\")\n            logger.debug(\n                f\"FeatureEngineer running step {i}: {name} ({transformer_type})\"\n            )\n            logger.debug(f\"current_data type: {type(current_data)}\")\n\n            # Capture metrics before\n            rows_before, cols_before = get_data_stats(current_data)\n\n            # Keep reference for comparison (for Winsorize metrics)\n            data_before = current_data\n\n            calculator, applier = self._get_transformer_components(transformer_type)\n\n            # We need a unique ID for this step's artifacts\n            step_node_id = f\"{node_id_prefix}_{name}\"\n\n            transformer = StatefulTransformer(calculator, applier, step_node_id)\n\n            # Handle special transformers that change data structure\n            # Splitters return SplitDataset or (X, y) tuples instead of a simple DataFrame,\n            # so they bypass the standard StatefulTransformer wrapper.\n\n            # Initialize fitted_params\n            fitted_params = {}\n\n            if transformer_type == \"TrainTestSplitter\":\n                logger.debug(\"Handling TrainTestSplitter\")\n                # TrainTestSplitter changes DataFrame -&gt; SplitDataset.\n                # We bypass StatefulTransformer to allow this structural change.\n                # It can also handle (X, y) tuple if FeatureTargetSplit was done first.\n                if isinstance(current_data, (pd.DataFrame, tuple)):\n                    logger.debug(\"Executing TrainTestSplitter logic\")\n                    params = calculator.fit(current_data, params)\n                    current_data = applier.apply(current_data, params)\n                    # In SDK, params are returned but not auto-saved to artifact store here.\n                    # The Pipeline object will handle state persistence.\n                else:\n                    logger.debug(\n                        f\"Skipping TrainTestSplitter. current_data is {type(current_data)}\"\n                    )\n                    logger.warning(\n                        \"Attempting to split an already split dataset. Skipping TrainTestSplitter.\"\n                    )\n\n            elif transformer_type == \"feature_target_split\":\n                logger.debug(\"Handling feature_target_split\")\n                # FeatureTargetSplitter changes structure to (X, y) or Dict of (X, y).\n                # We bypass StatefulTransformer to allow this structural change.\n                params = calculator.fit(current_data, params)\n                current_data = applier.apply(current_data, params)\n\n            else:\n                logger.debug(\"Handling standard transformer via StatefulTransformer\")\n                current_data = transformer.fit_transform(current_data, params)\n                # In SDK, transformer.params holds the state.\n                fitted_params = transformer.params\n\n                self.fitted_steps.append(\n                    {\n                        \"name\": name,\n                        \"type\": transformer_type,\n                        \"applier\": applier,\n                        \"artifact\": fitted_params,\n                    }\n                )\n\n            logger.debug(f\"Step {i} complete. New data type: {type(current_data)}\")\n\n            # Retrieve fitted params to get metrics from the calculator\n            try:\n                if fitted_params:\n                    # Imputation Metrics\n                    if transformer_type in [\n                        \"SimpleImputer\",\n                        \"KNNImputer\",\n                        \"IterativeImputer\",\n                    ]:\n                        if \"missing_counts\" in fitted_params:\n                            metrics[\"missing_counts\"] = fitted_params[\"missing_counts\"]\n                        if \"total_missing\" in fitted_params:\n                            metrics[\"total_missing\"] = fitted_params[\"total_missing\"]\n                        if \"fill_values\" in fitted_params:\n                            metrics[\"fill_values\"] = fitted_params[\"fill_values\"]\n\n                    # Feature Selection Metrics\n                    if transformer_type in [\n                        \"feature_selection\",\n                        \"UnivariateSelection\",\n                        \"ModelBasedSelection\",\n                        \"VarianceThreshold\",\n                    ]:\n                        if \"feature_scores\" in fitted_params:\n                            metrics[\"feature_scores\"] = fitted_params[\"feature_scores\"]\n                        if \"p_values\" in fitted_params:\n                            metrics[\"p_values\"] = fitted_params[\"p_values\"]\n                        if \"feature_importances\" in fitted_params:\n                            metrics[\"feature_importances\"] = fitted_params[\n                                \"feature_importances\"\n                            ]\n                        if \"variances\" in fitted_params:\n                            metrics[\"variances\"] = fitted_params[\"variances\"]\n                        if \"ranking\" in fitted_params:\n                            metrics[\"ranking\"] = fitted_params[\"ranking\"]\n                        if \"selected_columns\" in fitted_params:\n                            metrics[\"selected_columns\"] = fitted_params[\n                                \"selected_columns\"\n                            ]\n\n                    # Scaling Metrics\n                    if transformer_type in [\n                        \"StandardScaler\",\n                        \"MinMaxScaler\",\n                        \"RobustScaler\",\n                        \"MaxAbsScaler\",\n                    ]:\n                        if \"mean\" in fitted_params:\n                            metrics[\"mean\"] = fitted_params[\"mean\"]\n                        if \"scale\" in fitted_params:\n                            metrics[\"scale\"] = fitted_params[\"scale\"]\n                        if \"var\" in fitted_params:\n                            metrics[\"var\"] = fitted_params[\"var\"]\n                        if \"min\" in fitted_params:\n                            metrics[\"min\"] = fitted_params[\"min\"]\n                        if \"data_min\" in fitted_params:\n                            metrics[\"data_min\"] = fitted_params[\"data_min\"]\n                        if \"data_max\" in fitted_params:\n                            metrics[\"data_max\"] = fitted_params[\"data_max\"]\n                        if \"center\" in fitted_params:\n                            metrics[\"center\"] = fitted_params[\"center\"]\n                        if \"max_abs\" in fitted_params:\n                            metrics[\"max_abs\"] = fitted_params[\"max_abs\"]\n                        if \"columns\" in fitted_params:\n                            metrics[\"columns\"] = fitted_params[\"columns\"]\n\n                    # Outlier Metrics\n                    if transformer_type in [\n                        \"IQR\",\n                        \"Winsorize\",\n                        \"ZScore\",\n                        \"EllipticEnvelope\",\n                    ]:\n                        if \"warnings\" in fitted_params:\n                            metrics[\"warnings\"] = fitted_params[\"warnings\"]\n\n                    if transformer_type in [\"IQR\", \"Winsorize\"]:\n                        if \"bounds\" in fitted_params:\n                            metrics[\"bounds\"] = fitted_params[\"bounds\"]\n\n                    if transformer_type == \"ZScore\":\n                        if \"stats\" in fitted_params:\n                            metrics[\"stats\"] = fitted_params[\"stats\"]\n\n                    if transformer_type == \"EllipticEnvelope\":\n                        if \"contamination\" in fitted_params:\n                            metrics[\"contamination\"] = fitted_params[\"contamination\"]\n\n                    # Bucketing Metrics\n                    if transformer_type in [\n                        \"GeneralBinning\",\n                        \"EqualWidthBinning\",\n                        \"EqualFrequencyBinning\",\n                        \"CustomBinning\",\n                        \"KBinsDiscretizer\",\n                    ]:\n                        if \"bin_edges\" in fitted_params:\n                            metrics[\"bin_edges\"] = fitted_params[\"bin_edges\"]\n                        if \"n_bins\" in fitted_params:\n                            metrics[\"n_bins\"] = fitted_params[\"n_bins\"]\n\n                    # Feature Generation Metrics\n                    if transformer_type in [\"FeatureMath\", \"FeatureGenerationNode\"]:\n                        if \"operations\" in fitted_params:\n                            metrics[\"operations_count\"] = len(\n                                fitted_params[\"operations\"]\n                            )\n                            metrics[\"operations\"] = fitted_params[\"operations\"]\n                        # Calculate generated features by comparing columns\n                        if isinstance(data_before, pd.DataFrame) and isinstance(\n                            current_data, pd.DataFrame\n                        ):\n                            new_cols = list(\n                                set(current_data.columns) - set(data_before.columns)\n                            )\n                            metrics[\"generated_features\"] = new_cols\n                        elif isinstance(data_before, SplitDataset) and isinstance(\n                            current_data, SplitDataset\n                        ):\n                            # Check train set\n                            if isinstance(\n                                data_before.train, pd.DataFrame\n                            ) and isinstance(current_data.train, pd.DataFrame):\n                                new_cols = list(\n                                    set(current_data.train.columns)\n                                    - set(data_before.train.columns)\n                                )\n                                metrics[\"generated_features\"] = new_cols\n                            elif isinstance(data_before.train, tuple) and isinstance(\n                                current_data.train, tuple\n                            ):\n                                # (X, y) tuple\n                                X_before, _ = data_before.train\n                                X_after, _ = current_data.train\n                                if isinstance(X_before, pd.DataFrame) and isinstance(\n                                    X_after, pd.DataFrame\n                                ):\n                                    new_cols = list(\n                                        set(X_after.columns) - set(X_before.columns)\n                                    )\n                                    metrics[\"generated_features\"] = new_cols\n\n            except Exception as e:\n                logger.warning(f\"Failed to retrieve metrics for step {name}: {e}\")\n\n            # Capture metrics after\n            rows_after, cols_after = get_data_stats(current_data)\n\n            # Resampling Metrics (Calculated from data)\n            if transformer_type in [\"Oversampling\", \"Undersampling\"]:\n                try:\n                    # Extract y to calculate class counts\n                    y_res = None\n                    if isinstance(current_data, SplitDataset):\n                        if isinstance(current_data.train, tuple):\n                            _, y_res = current_data.train\n                        elif isinstance(current_data.train, pd.DataFrame):\n                            # Try to find target column from params\n                            target_col = params.get(\"target_column\")\n                            if target_col and target_col in current_data.train.columns:\n                                y_res = current_data.train[target_col]\n                    elif isinstance(current_data, tuple):\n                        _, y_res = current_data\n                    elif isinstance(current_data, pd.DataFrame):\n                        target_col = params.get(\"target_column\")\n                        if target_col and target_col in current_data.columns:\n                            y_res = current_data[target_col]\n\n                    if y_res is not None:\n                        counts = y_res.value_counts().to_dict()\n                        # Convert keys to string to ensure JSON serializability\n                        metrics[\"class_counts\"] = {\n                            str(k): int(v) for k, v in counts.items()\n                        }\n                        metrics[\"total_samples\"] = int(len(y_res))\n                except Exception as e:\n                    logger.warning(f\"Failed to calculate resampling metrics: {e}\")\n\n            if rows_after &gt; 0 or cols_after:\n                if transformer_type in [\n                    \"DropMissingRows\",\n                    \"Deduplicate\",\n                    \"IQR\",\n                    \"ZScore\",\n                    \"EllipticEnvelope\",\n                    \"Winsorize\",\n                ]:\n                    dropped = rows_before - rows_after\n                    metrics[f\"{transformer_type}_rows_removed\"] = dropped\n                    metrics[f\"{transformer_type}_rows_remaining\"] = rows_after\n                    metrics[f\"{transformer_type}_rows_total\"] = rows_before\n                    metrics[\"rows_removed\"] = dropped\n                    metrics[\"rows_total\"] = rows_before\n\n                    # Special metric for Winsorize: Values Clipped\n                    if transformer_type == \"Winsorize\":\n                        try:\n                            clipped_count = 0\n\n                            # Helper to count diffs\n                            def count_diffs(df1, df2):\n                                if isinstance(df1, pd.DataFrame) and isinstance(\n                                    df2, pd.DataFrame\n                                ):\n                                    if df1.shape == df2.shape:\n                                        return int(df1.ne(df2).sum().sum())\n                                elif (\n                                    isinstance(df1, tuple)\n                                    and isinstance(df2, tuple)\n                                    and len(df1) == 2\n                                    and len(df2) == 2\n                                ):\n                                    # Handle (X, y) tuple\n                                    diffs = 0\n                                    # Compare X (index 0)\n                                    if isinstance(df1[0], pd.DataFrame) and isinstance(\n                                        df2[0], pd.DataFrame\n                                    ):\n                                        if df1[0].shape == df2[0].shape:\n                                            diffs += int(df1[0].ne(df2[0]).sum().sum())\n                                    # Compare y (index 1) - usually Series\n                                    if isinstance(\n                                        df1[1], (pd.DataFrame, pd.Series)\n                                    ) and isinstance(df2[1], (pd.DataFrame, pd.Series)):\n                                        if df1[1].shape == df2[1].shape:\n                                            diffs += int(df1[1].ne(df2[1]).sum().sum())  # type: ignore\n                                    return diffs\n                                return 0\n\n                            if isinstance(data_before, pd.DataFrame) and isinstance(\n                                current_data, pd.DataFrame\n                            ):\n                                clipped_count = count_diffs(data_before, current_data)\n                            elif isinstance(data_before, SplitDataset) and isinstance(\n                                current_data, SplitDataset\n                            ):\n                                clipped_count += count_diffs(\n                                    data_before.train, current_data.train\n                                )\n                                clipped_count += count_diffs(\n                                    data_before.test, current_data.test\n                                )\n                                clipped_count += count_diffs(\n                                    data_before.validation, current_data.validation\n                                )\n\n                            metrics[\"values_clipped\"] = clipped_count\n                        except Exception as e:\n                            logger.warning(\n                                f\"Failed to calculate values_clipped for Winsorize: {e}\"\n                            )\n                            pass\n\n                if transformer_type == \"MissingIndicator\":\n                    new_cols_set = cols_after - cols_before\n                    metrics[\"missing_indicators_created\"] = len(new_cols_set)\n                    cast(Dict[str, Any], metrics)[\"missing_indicators_columns\"] = list(\n                        new_cols_set\n                    )\n\n                if transformer_type == \"DropMissingColumns\":\n                    dropped_cols_set = cols_before - cols_after\n                    cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                        dropped_cols_set\n                    )\n                    metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n                if transformer_type == \"feature_selection\":\n                    dropped_cols_set = cols_before - cols_after\n                    cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                        dropped_cols_set\n                    )\n                    metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n                if transformer_type in [\n                    \"OneHotEncoder\",\n                    \"LabelEncoder\",\n                    \"OrdinalEncoder\",\n                    \"TargetEncoder\",\n                    \"HashEncoder\",\n                    \"DummyEncoder\",\n                ]:\n                    new_cols_set = cols_after - cols_before\n                    metrics[\"new_features_count\"] = len(new_cols_set)\n                    metrics[\"encoded_columns_count\"] = len(params.get(\"columns\", []))\n\n                    if \"categories_count\" in params:\n                        metrics[\"categories_count\"] = params[\"categories_count\"]\n                    if \"classes_count\" in params:\n                        metrics[\"classes_count\"] = params[\"classes_count\"]\n\n        return current_data, metrics\n\n    def _get_transformer_components(self, type_name: str):  # noqa: C901\n        # Try Registry first\n        try:\n            return (\n                NodeRegistry.get_calculator(type_name)(),\n                NodeRegistry.get_applier(type_name)(),\n            )\n        except ValueError:\n            raise ValueError(f\"Unknown transformer type: {type_name}\")\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.FeatureEngineer.fit_transform","title":"<code>fit_transform(data, node_id_prefix='')</code>","text":"<p>Runs the pipeline on data. Returns: (transformed_data, metrics_dict)</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>def fit_transform(self, data: Union[pd.DataFrame, Any], node_id_prefix=\"\") -&gt; Any:  # noqa: C901\n    \"\"\"\n    Runs the pipeline on data.\n    Returns: (transformed_data, metrics_dict)\n    \"\"\"\n    self.fitted_steps = []  # Reset fitted steps\n    current_data = data\n    metrics: Dict[str, Any] = {}\n\n    for i, step in enumerate(self.steps_config):\n        name = step[\"name\"]\n        transformer_type = step[\"transformer\"]\n        params = step.get(\"params\", {})\n\n        logger.info(f\"Running step {i}: {name} ({transformer_type})\")\n        logger.debug(\n            f\"FeatureEngineer running step {i}: {name} ({transformer_type})\"\n        )\n        logger.debug(f\"current_data type: {type(current_data)}\")\n\n        # Capture metrics before\n        rows_before, cols_before = get_data_stats(current_data)\n\n        # Keep reference for comparison (for Winsorize metrics)\n        data_before = current_data\n\n        calculator, applier = self._get_transformer_components(transformer_type)\n\n        # We need a unique ID for this step's artifacts\n        step_node_id = f\"{node_id_prefix}_{name}\"\n\n        transformer = StatefulTransformer(calculator, applier, step_node_id)\n\n        # Handle special transformers that change data structure\n        # Splitters return SplitDataset or (X, y) tuples instead of a simple DataFrame,\n        # so they bypass the standard StatefulTransformer wrapper.\n\n        # Initialize fitted_params\n        fitted_params = {}\n\n        if transformer_type == \"TrainTestSplitter\":\n            logger.debug(\"Handling TrainTestSplitter\")\n            # TrainTestSplitter changes DataFrame -&gt; SplitDataset.\n            # We bypass StatefulTransformer to allow this structural change.\n            # It can also handle (X, y) tuple if FeatureTargetSplit was done first.\n            if isinstance(current_data, (pd.DataFrame, tuple)):\n                logger.debug(\"Executing TrainTestSplitter logic\")\n                params = calculator.fit(current_data, params)\n                current_data = applier.apply(current_data, params)\n                # In SDK, params are returned but not auto-saved to artifact store here.\n                # The Pipeline object will handle state persistence.\n            else:\n                logger.debug(\n                    f\"Skipping TrainTestSplitter. current_data is {type(current_data)}\"\n                )\n                logger.warning(\n                    \"Attempting to split an already split dataset. Skipping TrainTestSplitter.\"\n                )\n\n        elif transformer_type == \"feature_target_split\":\n            logger.debug(\"Handling feature_target_split\")\n            # FeatureTargetSplitter changes structure to (X, y) or Dict of (X, y).\n            # We bypass StatefulTransformer to allow this structural change.\n            params = calculator.fit(current_data, params)\n            current_data = applier.apply(current_data, params)\n\n        else:\n            logger.debug(\"Handling standard transformer via StatefulTransformer\")\n            current_data = transformer.fit_transform(current_data, params)\n            # In SDK, transformer.params holds the state.\n            fitted_params = transformer.params\n\n            self.fitted_steps.append(\n                {\n                    \"name\": name,\n                    \"type\": transformer_type,\n                    \"applier\": applier,\n                    \"artifact\": fitted_params,\n                }\n            )\n\n        logger.debug(f\"Step {i} complete. New data type: {type(current_data)}\")\n\n        # Retrieve fitted params to get metrics from the calculator\n        try:\n            if fitted_params:\n                # Imputation Metrics\n                if transformer_type in [\n                    \"SimpleImputer\",\n                    \"KNNImputer\",\n                    \"IterativeImputer\",\n                ]:\n                    if \"missing_counts\" in fitted_params:\n                        metrics[\"missing_counts\"] = fitted_params[\"missing_counts\"]\n                    if \"total_missing\" in fitted_params:\n                        metrics[\"total_missing\"] = fitted_params[\"total_missing\"]\n                    if \"fill_values\" in fitted_params:\n                        metrics[\"fill_values\"] = fitted_params[\"fill_values\"]\n\n                # Feature Selection Metrics\n                if transformer_type in [\n                    \"feature_selection\",\n                    \"UnivariateSelection\",\n                    \"ModelBasedSelection\",\n                    \"VarianceThreshold\",\n                ]:\n                    if \"feature_scores\" in fitted_params:\n                        metrics[\"feature_scores\"] = fitted_params[\"feature_scores\"]\n                    if \"p_values\" in fitted_params:\n                        metrics[\"p_values\"] = fitted_params[\"p_values\"]\n                    if \"feature_importances\" in fitted_params:\n                        metrics[\"feature_importances\"] = fitted_params[\n                            \"feature_importances\"\n                        ]\n                    if \"variances\" in fitted_params:\n                        metrics[\"variances\"] = fitted_params[\"variances\"]\n                    if \"ranking\" in fitted_params:\n                        metrics[\"ranking\"] = fitted_params[\"ranking\"]\n                    if \"selected_columns\" in fitted_params:\n                        metrics[\"selected_columns\"] = fitted_params[\n                            \"selected_columns\"\n                        ]\n\n                # Scaling Metrics\n                if transformer_type in [\n                    \"StandardScaler\",\n                    \"MinMaxScaler\",\n                    \"RobustScaler\",\n                    \"MaxAbsScaler\",\n                ]:\n                    if \"mean\" in fitted_params:\n                        metrics[\"mean\"] = fitted_params[\"mean\"]\n                    if \"scale\" in fitted_params:\n                        metrics[\"scale\"] = fitted_params[\"scale\"]\n                    if \"var\" in fitted_params:\n                        metrics[\"var\"] = fitted_params[\"var\"]\n                    if \"min\" in fitted_params:\n                        metrics[\"min\"] = fitted_params[\"min\"]\n                    if \"data_min\" in fitted_params:\n                        metrics[\"data_min\"] = fitted_params[\"data_min\"]\n                    if \"data_max\" in fitted_params:\n                        metrics[\"data_max\"] = fitted_params[\"data_max\"]\n                    if \"center\" in fitted_params:\n                        metrics[\"center\"] = fitted_params[\"center\"]\n                    if \"max_abs\" in fitted_params:\n                        metrics[\"max_abs\"] = fitted_params[\"max_abs\"]\n                    if \"columns\" in fitted_params:\n                        metrics[\"columns\"] = fitted_params[\"columns\"]\n\n                # Outlier Metrics\n                if transformer_type in [\n                    \"IQR\",\n                    \"Winsorize\",\n                    \"ZScore\",\n                    \"EllipticEnvelope\",\n                ]:\n                    if \"warnings\" in fitted_params:\n                        metrics[\"warnings\"] = fitted_params[\"warnings\"]\n\n                if transformer_type in [\"IQR\", \"Winsorize\"]:\n                    if \"bounds\" in fitted_params:\n                        metrics[\"bounds\"] = fitted_params[\"bounds\"]\n\n                if transformer_type == \"ZScore\":\n                    if \"stats\" in fitted_params:\n                        metrics[\"stats\"] = fitted_params[\"stats\"]\n\n                if transformer_type == \"EllipticEnvelope\":\n                    if \"contamination\" in fitted_params:\n                        metrics[\"contamination\"] = fitted_params[\"contamination\"]\n\n                # Bucketing Metrics\n                if transformer_type in [\n                    \"GeneralBinning\",\n                    \"EqualWidthBinning\",\n                    \"EqualFrequencyBinning\",\n                    \"CustomBinning\",\n                    \"KBinsDiscretizer\",\n                ]:\n                    if \"bin_edges\" in fitted_params:\n                        metrics[\"bin_edges\"] = fitted_params[\"bin_edges\"]\n                    if \"n_bins\" in fitted_params:\n                        metrics[\"n_bins\"] = fitted_params[\"n_bins\"]\n\n                # Feature Generation Metrics\n                if transformer_type in [\"FeatureMath\", \"FeatureGenerationNode\"]:\n                    if \"operations\" in fitted_params:\n                        metrics[\"operations_count\"] = len(\n                            fitted_params[\"operations\"]\n                        )\n                        metrics[\"operations\"] = fitted_params[\"operations\"]\n                    # Calculate generated features by comparing columns\n                    if isinstance(data_before, pd.DataFrame) and isinstance(\n                        current_data, pd.DataFrame\n                    ):\n                        new_cols = list(\n                            set(current_data.columns) - set(data_before.columns)\n                        )\n                        metrics[\"generated_features\"] = new_cols\n                    elif isinstance(data_before, SplitDataset) and isinstance(\n                        current_data, SplitDataset\n                    ):\n                        # Check train set\n                        if isinstance(\n                            data_before.train, pd.DataFrame\n                        ) and isinstance(current_data.train, pd.DataFrame):\n                            new_cols = list(\n                                set(current_data.train.columns)\n                                - set(data_before.train.columns)\n                            )\n                            metrics[\"generated_features\"] = new_cols\n                        elif isinstance(data_before.train, tuple) and isinstance(\n                            current_data.train, tuple\n                        ):\n                            # (X, y) tuple\n                            X_before, _ = data_before.train\n                            X_after, _ = current_data.train\n                            if isinstance(X_before, pd.DataFrame) and isinstance(\n                                X_after, pd.DataFrame\n                            ):\n                                new_cols = list(\n                                    set(X_after.columns) - set(X_before.columns)\n                                )\n                                metrics[\"generated_features\"] = new_cols\n\n        except Exception as e:\n            logger.warning(f\"Failed to retrieve metrics for step {name}: {e}\")\n\n        # Capture metrics after\n        rows_after, cols_after = get_data_stats(current_data)\n\n        # Resampling Metrics (Calculated from data)\n        if transformer_type in [\"Oversampling\", \"Undersampling\"]:\n            try:\n                # Extract y to calculate class counts\n                y_res = None\n                if isinstance(current_data, SplitDataset):\n                    if isinstance(current_data.train, tuple):\n                        _, y_res = current_data.train\n                    elif isinstance(current_data.train, pd.DataFrame):\n                        # Try to find target column from params\n                        target_col = params.get(\"target_column\")\n                        if target_col and target_col in current_data.train.columns:\n                            y_res = current_data.train[target_col]\n                elif isinstance(current_data, tuple):\n                    _, y_res = current_data\n                elif isinstance(current_data, pd.DataFrame):\n                    target_col = params.get(\"target_column\")\n                    if target_col and target_col in current_data.columns:\n                        y_res = current_data[target_col]\n\n                if y_res is not None:\n                    counts = y_res.value_counts().to_dict()\n                    # Convert keys to string to ensure JSON serializability\n                    metrics[\"class_counts\"] = {\n                        str(k): int(v) for k, v in counts.items()\n                    }\n                    metrics[\"total_samples\"] = int(len(y_res))\n            except Exception as e:\n                logger.warning(f\"Failed to calculate resampling metrics: {e}\")\n\n        if rows_after &gt; 0 or cols_after:\n            if transformer_type in [\n                \"DropMissingRows\",\n                \"Deduplicate\",\n                \"IQR\",\n                \"ZScore\",\n                \"EllipticEnvelope\",\n                \"Winsorize\",\n            ]:\n                dropped = rows_before - rows_after\n                metrics[f\"{transformer_type}_rows_removed\"] = dropped\n                metrics[f\"{transformer_type}_rows_remaining\"] = rows_after\n                metrics[f\"{transformer_type}_rows_total\"] = rows_before\n                metrics[\"rows_removed\"] = dropped\n                metrics[\"rows_total\"] = rows_before\n\n                # Special metric for Winsorize: Values Clipped\n                if transformer_type == \"Winsorize\":\n                    try:\n                        clipped_count = 0\n\n                        # Helper to count diffs\n                        def count_diffs(df1, df2):\n                            if isinstance(df1, pd.DataFrame) and isinstance(\n                                df2, pd.DataFrame\n                            ):\n                                if df1.shape == df2.shape:\n                                    return int(df1.ne(df2).sum().sum())\n                            elif (\n                                isinstance(df1, tuple)\n                                and isinstance(df2, tuple)\n                                and len(df1) == 2\n                                and len(df2) == 2\n                            ):\n                                # Handle (X, y) tuple\n                                diffs = 0\n                                # Compare X (index 0)\n                                if isinstance(df1[0], pd.DataFrame) and isinstance(\n                                    df2[0], pd.DataFrame\n                                ):\n                                    if df1[0].shape == df2[0].shape:\n                                        diffs += int(df1[0].ne(df2[0]).sum().sum())\n                                # Compare y (index 1) - usually Series\n                                if isinstance(\n                                    df1[1], (pd.DataFrame, pd.Series)\n                                ) and isinstance(df2[1], (pd.DataFrame, pd.Series)):\n                                    if df1[1].shape == df2[1].shape:\n                                        diffs += int(df1[1].ne(df2[1]).sum().sum())  # type: ignore\n                                return diffs\n                            return 0\n\n                        if isinstance(data_before, pd.DataFrame) and isinstance(\n                            current_data, pd.DataFrame\n                        ):\n                            clipped_count = count_diffs(data_before, current_data)\n                        elif isinstance(data_before, SplitDataset) and isinstance(\n                            current_data, SplitDataset\n                        ):\n                            clipped_count += count_diffs(\n                                data_before.train, current_data.train\n                            )\n                            clipped_count += count_diffs(\n                                data_before.test, current_data.test\n                            )\n                            clipped_count += count_diffs(\n                                data_before.validation, current_data.validation\n                            )\n\n                        metrics[\"values_clipped\"] = clipped_count\n                    except Exception as e:\n                        logger.warning(\n                            f\"Failed to calculate values_clipped for Winsorize: {e}\"\n                        )\n                        pass\n\n            if transformer_type == \"MissingIndicator\":\n                new_cols_set = cols_after - cols_before\n                metrics[\"missing_indicators_created\"] = len(new_cols_set)\n                cast(Dict[str, Any], metrics)[\"missing_indicators_columns\"] = list(\n                    new_cols_set\n                )\n\n            if transformer_type == \"DropMissingColumns\":\n                dropped_cols_set = cols_before - cols_after\n                cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                    dropped_cols_set\n                )\n                metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n            if transformer_type == \"feature_selection\":\n                dropped_cols_set = cols_before - cols_after\n                cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                    dropped_cols_set\n                )\n                metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n            if transformer_type in [\n                \"OneHotEncoder\",\n                \"LabelEncoder\",\n                \"OrdinalEncoder\",\n                \"TargetEncoder\",\n                \"HashEncoder\",\n                \"DummyEncoder\",\n            ]:\n                new_cols_set = cols_after - cols_before\n                metrics[\"new_features_count\"] = len(new_cols_set)\n                metrics[\"encoded_columns_count\"] = len(params.get(\"columns\", []))\n\n                if \"categories_count\" in params:\n                    metrics[\"categories_count\"] = params[\"categories_count\"]\n                if \"classes_count\" in params:\n                    metrics[\"classes_count\"] = params[\"classes_count\"]\n\n    return current_data, metrics\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.FeatureEngineer.transform","title":"<code>transform(data)</code>","text":"<p>Apply fitted transformations to new data.</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>def transform(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply fitted transformations to new data.\n    \"\"\"\n    current_data = data\n\n    for step in self.fitted_steps:\n        name = step[\"name\"]\n        transformer_type = step[\"type\"]\n        applier = step[\"applier\"]\n        artifact = step[\"artifact\"]\n\n        # Skip splitters during inference/transform\n        if transformer_type in [\n            \"TrainTestSplitter\",\n            \"feature_target_split\",\n            \"Oversampling\",\n            \"Undersampling\",\n        ]:\n            continue\n\n        logger.debug(f\"Applying step: {name} ({transformer_type})\")\n        current_data = applier.apply(current_data, artifact)\n\n    return current_data\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.GeneralBinningCalculator","title":"<code>GeneralBinningCalculator</code>","text":"<p>               Bases: <code>BaseCalculator</code></p> <p>Master calculator that handles mixed strategies and overrides.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>@NodeRegistry.register(\"GeneralBinning\", GeneralBinningApplier)\nclass GeneralBinningCalculator(BaseCalculator):\n    \"\"\"\n    Master calculator that handles mixed strategies and overrides.\n    \"\"\"\n\n    def fit(  # noqa: C901\n        self, df: SkyulfDataFrame, config: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n\n        # Ensure X is pandas for fitting logic\n        engine = get_engine(X)\n        if engine.name == \"polars\":\n            X = X.to_pandas()\n\n        columns = resolve_columns(X, config, detect_numeric_columns)\n\n        global_strategy = config.get(\"strategy\", \"equal_width\")\n        column_strategies = config.get(\"column_strategies\", {})\n\n        # Global settings\n        default_n_bins = config.get(\"n_bins\", 5)\n        n_bins_global = config.get(\"equal_width_bins\", default_n_bins)\n        q_bins_global = config.get(\"equal_frequency_bins\", default_n_bins)\n        duplicates_global = config.get(\"duplicates\", \"drop\")\n\n        valid_cols = [c for c in columns if c in X.columns]\n        bin_edges_map = {}\n        custom_labels_map = {}\n\n        for col in valid_cols:\n            # Determine strategy and params for this column\n            override = column_strategies.get(col, {})\n            strategy = override.get(\"strategy\", global_strategy)\n\n            try:\n                series = X[col].dropna()\n                if series.empty:\n                    continue\n\n                edges = None\n\n                if strategy == \"equal_width\":\n                    n_bins = override.get(\"equal_width_bins\", n_bins_global)\n                    _, edges = pd.cut(series, bins=n_bins, retbins=True)\n                    # Clamp first edge to min if it was extended\n                    if len(edges) &gt; 0 and edges[0] &lt; series.min():\n                        edges[0] = series.min()\n\n                elif strategy == \"equal_frequency\":\n                    n_bins = override.get(\"equal_frequency_bins\", q_bins_global)\n                    duplicates = override.get(\"duplicates\", duplicates_global)\n                    _, edges = pd.qcut(\n                        series, q=n_bins, retbins=True, duplicates=duplicates\n                    )\n                    # Clamp first edge to min if it was extended\n                    if len(edges) &gt; 0 and edges[0] &lt; series.min():\n                        edges[0] = series.min()\n\n                elif strategy == \"kmeans\":\n                    n_bins = override.get(\"n_bins\", default_n_bins)\n                    est = KBinsDiscretizer(\n                        n_bins=n_bins, strategy=\"kmeans\", encode=\"ordinal\", quantile_method=\"averaged_inverted_cdf\"\n                    )\n                    est.fit(series.values.reshape(-1, 1))  # type: ignore\n                    edges = est.bin_edges_[0]\n\n                elif strategy == \"custom\":\n                    # Check override first, then global custom_bins\n                    custom_bins = override.get(\"custom_bins\")\n                    if not custom_bins:\n                        custom_bins = config.get(\"custom_bins\", {}).get(col)\n\n                    if custom_bins:\n                        edges = np.array(sorted(custom_bins))\n\n                    # Handle custom labels\n                    labels = override.get(\"custom_labels\")\n                    if not labels:\n                        labels = config.get(\"custom_labels\", {}).get(col)\n                    if labels:\n                        custom_labels_map[col] = labels\n\n                elif strategy == \"kbins\":\n                    n_bins = override.get(\n                        \"kbins_n_bins\", config.get(\"kbins_n_bins\", default_n_bins)\n                    )\n                    k_strategy = override.get(\n                        \"kbins_strategy\", config.get(\"kbins_strategy\", \"quantile\")\n                    )\n\n                    # Map strategy names\n                    sklearn_strategy = k_strategy\n                    if k_strategy == \"equal_width\":\n                        sklearn_strategy = \"uniform\"\n                    elif k_strategy == \"equal_frequency\":\n                        sklearn_strategy = \"quantile\"\n\n                    est = KBinsDiscretizer(\n                        n_bins=n_bins, strategy=sklearn_strategy, encode=\"ordinal\", quantile_method=\"averaged_inverted_cdf\"\n                    )\n                    est.fit(series.values.reshape(-1, 1))  # type: ignore\n                    edges = est.bin_edges_[0]\n\n                if edges is not None:\n                    bin_edges_map[col] = edges.tolist()\n\n            except Exception:\n                continue\n\n        return {\n            \"type\": \"general_binning\",\n            \"bin_edges\": bin_edges_map,\n            \"custom_labels\": custom_labels_map,\n            \"output_suffix\": config.get(\"output_suffix\", \"_binned\"),\n            \"drop_original\": config.get(\"drop_original\", False),\n            \"label_format\": config.get(\"label_format\", \"ordinal\"),\n            \"missing_strategy\": config.get(\"missing_strategy\", \"keep\"),\n            \"missing_label\": config.get(\"missing_label\", \"Missing\"),\n            \"include_lowest\": config.get(\"include_lowest\", True),\n            \"precision\": config.get(\"precision\", 3),\n        }\n</code></pre>"},{"location":"reference/api/preprocessing/index.html#skyulf.preprocessing.KBinsDiscretizerCalculator","title":"<code>KBinsDiscretizerCalculator</code>","text":"<p>               Bases: <code>GeneralBinningCalculator</code></p> <p>Calculator for KBinsDiscretizer node. Wraps GeneralBinningCalculator with kbins strategy.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>@NodeRegistry.register(\"KBinsDiscretizer\", KBinsDiscretizerApplier)\nclass KBinsDiscretizerCalculator(GeneralBinningCalculator):\n    \"\"\"\n    Calculator for KBinsDiscretizer node.\n    Wraps GeneralBinningCalculator with kbins strategy.\n    \"\"\"\n\n    def fit(\n        self,\n        df: SkyulfDataFrame,\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        new_config = config.copy()\n        new_config[\"strategy\"] = \"kbins\"\n\n        # Map sklearn params to GeneralBinning params\n        if \"n_bins\" in config:\n            new_config[\"kbins_n_bins\"] = config[\"n_bins\"]\n\n        # sklearn strategy: uniform, quantile, kmeans\n        # GeneralBinning kbins_strategy: same\n        if \"strategy\" in config and config[\"strategy\"] != \"kbins\":\n            new_config[\"kbins_strategy\"] = config[\"strategy\"]\n\n        return super().fit(df, new_config)\n</code></pre>"},{"location":"reference/api/preprocessing/base.html","title":"API: preprocessing.base","text":""},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base","title":"<code>skyulf.preprocessing.base</code>","text":""},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base.BaseApplier","title":"<code>BaseApplier</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>class BaseApplier(ABC):\n    @abstractmethod\n    def apply(\n        self, df: Union[pd.DataFrame, SkyulfDataFrame, tuple], params: Dict[str, Any]\n    ) -&gt; Union[pd.DataFrame, SkyulfDataFrame, tuple, SplitDataset]:\n        \"\"\"\n        Applies the transformation using fitted parameters.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base.BaseApplier.apply","title":"<code>apply(df, params)</code>  <code>abstractmethod</code>","text":"<p>Applies the transformation using fitted parameters.</p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>@abstractmethod\ndef apply(\n    self, df: Union[pd.DataFrame, SkyulfDataFrame, tuple], params: Dict[str, Any]\n) -&gt; Union[pd.DataFrame, SkyulfDataFrame, tuple, SplitDataset]:\n    \"\"\"\n    Applies the transformation using fitted parameters.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base.BaseCalculator","title":"<code>BaseCalculator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>class BaseCalculator(ABC):\n    @abstractmethod\n    def fit(\n        self, df: Union[pd.DataFrame, SkyulfDataFrame, tuple], config: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Calculates parameters from the training data.\n        Returns a dictionary of fitted parameters (serializable).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/preprocessing/base.html#skyulf.preprocessing.base.BaseCalculator.fit","title":"<code>fit(df, config)</code>  <code>abstractmethod</code>","text":"<p>Calculates parameters from the training data. Returns a dictionary of fitted parameters (serializable).</p> Source code in <code>skyulf-core/skyulf/preprocessing/base.py</code> <pre><code>@abstractmethod\ndef fit(\n    self, df: Union[pd.DataFrame, SkyulfDataFrame, tuple], config: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Calculates parameters from the training data.\n    Returns a dictionary of fitted parameters (serializable).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/preprocessing/bucketing.html","title":"API: preprocessing.bucketing","text":""},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing","title":"<code>skyulf.preprocessing.bucketing</code>","text":""},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing.BaseBinningApplier","title":"<code>BaseBinningApplier</code>","text":"<p>               Bases: <code>BaseApplier</code></p> <p>Base class for applying binning transformations. Expects 'bin_edges' in params: Dict[str, List[float]] mapping column names to bin edges.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>class BaseBinningApplier(BaseApplier):\n    \"\"\"\n    Base class for applying binning transformations.\n    Expects 'bin_edges' in params: Dict[str, List[float]] mapping column names to bin edges.\n    \"\"\"\n\n    def apply(  # noqa: C901\n        self,\n        df: SkyulfDataFrame,\n        params: Dict[str, Any],\n    ) -&gt; Union[SkyulfDataFrame, Tuple[SkyulfDataFrame, Any]]:\n        X, y, is_tuple = unpack_pipeline_input(df)\n        engine = get_engine(X)\n\n        bin_edges_map = params.get(\"bin_edges\", {})\n        if not bin_edges_map:\n            return pack_pipeline_output(X, y, is_tuple)\n\n        output_suffix = params.get(\"output_suffix\", \"_binned\")\n        drop_original = params.get(\"drop_original\", False)\n        label_format = params.get(\n            \"label_format\", \"ordinal\"\n        )  # ordinal, range, bin_index\n        missing_strategy = params.get(\"missing_strategy\", \"keep\")  # keep, label\n        missing_label = params.get(\"missing_label\", \"Missing\")\n        include_lowest = params.get(\"include_lowest\", True)\n        precision = params.get(\"precision\", 3)\n        custom_labels_map = params.get(\"custom_labels\", {})\n\n        # Polars Path\n        if engine.name == \"polars\":\n            import polars as pl\n\n            exprs = []\n            cols_to_drop = []\n\n            for col, edges in bin_edges_map.items():\n                if col not in X.columns:\n                    continue\n\n                if drop_original:\n                    cols_to_drop.append(col)\n\n                sorted_edges = sorted(list(set(edges)))\n                if len(sorted_edges) &lt; 2:\n                    continue\n\n                # Determine labels\n                labels = None\n                col_custom_labels = custom_labels_map.get(col)\n                if col_custom_labels and len(col_custom_labels) == len(sorted_edges) - 1:\n                    labels = col_custom_labels\n\n                # Polars cut\n                # breaks are the internal cut points\n                breaks = sorted_edges[1:-1]\n\n                # Polars cut\n                cut_expr = pl.col(col).cut(\n                    breaks=breaks,\n                    labels=labels,\n                    left_closed=False, # (a, b]\n                    include_breaks=False\n                )\n\n                target_col_name = f\"{col}{output_suffix}\"\n\n                if label_format in [\"ordinal\", \"bin_index\"] and not labels:\n                    # We want integer indices.\n                    # Polars cut returns Categorical. Cast to UInt32 gives the physical index.\n                    exprs.append(cut_expr.cast(pl.UInt32).alias(target_col_name))\n                else:\n                    # Range or Custom Labels\n                    exprs.append(cut_expr.alias(target_col_name))\n\n            X_out = X.with_columns(exprs)\n            if drop_original:\n                X_out = X_out.drop(cols_to_drop)\n\n            return pack_pipeline_output(X_out, y, is_tuple)\n\n        # Pandas Path\n        df_out = X.copy()\n        processed_cols = []\n\n        for col, edges in bin_edges_map.items():\n            if col not in df_out.columns:\n                continue\n\n            processed_cols.append(col)\n\n            # Determine labels for pd.cut\n            labels: Union[Literal[False], List[Any], None] = (\n                False  # Default for ordinal (returns integers)\n            )\n\n            # Check for custom labels first\n            col_custom_labels = custom_labels_map.get(col)\n            if col_custom_labels and len(col_custom_labels) == len(edges) - 1:\n                labels = col_custom_labels\n            elif label_format == \"range\":\n                labels = None  # Returns intervals\n            elif label_format == \"bin_index\":\n                labels = False  # Returns integers 0..n-1\n\n            # Apply cut\n            try:\n                # Ensure edges are unique and sorted\n                sorted_edges = sorted(list(set(edges)))\n                if len(sorted_edges) &lt; 2:\n                    continue\n\n                binned_series = pd.cut(\n                    df_out[col],\n                    bins=sorted_edges,\n                    labels=labels,\n                    include_lowest=include_lowest,\n                )\n\n                # Handle missing values\n                if missing_strategy == \"label\":\n                    # If categorical (range or custom labels), add category\n                    if isinstance(binned_series.dtype, pd.CategoricalDtype):\n                        if missing_label not in binned_series.cat.categories:\n                            binned_series = binned_series.cat.add_categories(\n                                [missing_label]\n                            )\n                        binned_series = binned_series.fillna(missing_label)\n                    else:\n                        # If numeric (ordinal/bin_index), we convert to object/str to support \"Missing\" label\n                        binned_series = binned_series.astype(object).fillna(\n                            missing_label\n                        )\n\n                # Format ranges if needed\n                if label_format == \"range\" and labels is None:\n                    # Convert intervals to string with precision\n                    if isinstance(binned_series.dtype, pd.CategoricalDtype):\n                        # It's a categorical of intervals\n                        def format_interval(iv):\n                            if pd.isna(iv) or isinstance(iv, str):\n                                return iv\n\n                            # Use the logical left edge if it's the first bin and include_lowest is True\n                            left_val = iv.left\n                            if (\n                                include_lowest\n                                and len(sorted_edges) &gt; 0\n                                and left_val &lt; sorted_edges[0]\n                            ):\n                                left_val = sorted_edges[0]\n\n                            l_val = round(left_val, precision)\n                            r_val = round(iv.right, precision)\n                            if include_lowest:\n                                return f\"[{l_val}, {r_val}]\"\n                            else:\n                                return f\"({l_val}, {r_val}]\"\n\n                        # We need to map the categories themselves\n                        new_categories = [\n                            format_interval(c) for c in binned_series.cat.categories\n                        ]\n                        binned_series = binned_series.cat.rename_categories(\n                            new_categories\n                        )\n                        binned_series = binned_series.astype(str)\n                    else:\n                        binned_series = binned_series.astype(str)\n\n                    if missing_strategy == \"keep\":\n                        # Restore NaNs if they were converted to 'nan' string\n                        binned_series = binned_series.replace(\"nan\", np.nan)\n\n                out_col = f\"{col}{output_suffix}\"\n                df_out[out_col] = binned_series\n\n            except Exception:\n                # Log error or skip\n                pass\n\n        if drop_original:\n            df_out = df_out.drop(columns=processed_cols)\n\n        return pack_pipeline_output(df_out, y, is_tuple)\n</code></pre>"},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing.CustomBinningCalculator","title":"<code>CustomBinningCalculator</code>","text":"<p>               Bases: <code>BaseCalculator</code></p> <p>Calculator for CustomBinning node. Applies specific bin edges to selected columns.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>@NodeRegistry.register(\"CustomBinning\", CustomBinningApplier)\nclass CustomBinningCalculator(BaseCalculator):\n    \"\"\"\n    Calculator for CustomBinning node.\n    Applies specific bin edges to selected columns.\n    \"\"\"\n\n    def fit(\n        self,\n        df: SkyulfDataFrame,\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n\n        # Ensure X is pandas for fitting logic\n        engine = get_engine(X)\n        if engine.name == \"polars\":\n            X = X.to_pandas()\n\n        columns = resolve_columns(X, config, detect_numeric_columns)\n        bins = config.get(\"bins\")\n\n        bin_edges_map = {}\n        if bins:\n            sorted_bins = sorted(bins)\n            for col in columns:\n                if col in X.columns:\n                    bin_edges_map[col] = sorted_bins\n\n        return {\n            \"type\": \"general_binning\",  # Use GeneralBinningApplier\n            \"bin_edges\": bin_edges_map,\n            \"output_suffix\": config.get(\"output_suffix\", \"_binned\"),\n            \"drop_original\": config.get(\"drop_original\", False),\n            \"label_format\": config.get(\"label_format\", \"ordinal\"),\n            \"missing_strategy\": config.get(\"missing_strategy\", \"keep\"),\n            \"missing_label\": config.get(\"missing_label\", \"Missing\"),\n            \"include_lowest\": config.get(\"include_lowest\", True),\n            \"precision\": config.get(\"precision\", 3),\n        }\n</code></pre>"},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing.GeneralBinningCalculator","title":"<code>GeneralBinningCalculator</code>","text":"<p>               Bases: <code>BaseCalculator</code></p> <p>Master calculator that handles mixed strategies and overrides.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>@NodeRegistry.register(\"GeneralBinning\", GeneralBinningApplier)\nclass GeneralBinningCalculator(BaseCalculator):\n    \"\"\"\n    Master calculator that handles mixed strategies and overrides.\n    \"\"\"\n\n    def fit(  # noqa: C901\n        self, df: SkyulfDataFrame, config: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n\n        # Ensure X is pandas for fitting logic\n        engine = get_engine(X)\n        if engine.name == \"polars\":\n            X = X.to_pandas()\n\n        columns = resolve_columns(X, config, detect_numeric_columns)\n\n        global_strategy = config.get(\"strategy\", \"equal_width\")\n        column_strategies = config.get(\"column_strategies\", {})\n\n        # Global settings\n        default_n_bins = config.get(\"n_bins\", 5)\n        n_bins_global = config.get(\"equal_width_bins\", default_n_bins)\n        q_bins_global = config.get(\"equal_frequency_bins\", default_n_bins)\n        duplicates_global = config.get(\"duplicates\", \"drop\")\n\n        valid_cols = [c for c in columns if c in X.columns]\n        bin_edges_map = {}\n        custom_labels_map = {}\n\n        for col in valid_cols:\n            # Determine strategy and params for this column\n            override = column_strategies.get(col, {})\n            strategy = override.get(\"strategy\", global_strategy)\n\n            try:\n                series = X[col].dropna()\n                if series.empty:\n                    continue\n\n                edges = None\n\n                if strategy == \"equal_width\":\n                    n_bins = override.get(\"equal_width_bins\", n_bins_global)\n                    _, edges = pd.cut(series, bins=n_bins, retbins=True)\n                    # Clamp first edge to min if it was extended\n                    if len(edges) &gt; 0 and edges[0] &lt; series.min():\n                        edges[0] = series.min()\n\n                elif strategy == \"equal_frequency\":\n                    n_bins = override.get(\"equal_frequency_bins\", q_bins_global)\n                    duplicates = override.get(\"duplicates\", duplicates_global)\n                    _, edges = pd.qcut(\n                        series, q=n_bins, retbins=True, duplicates=duplicates\n                    )\n                    # Clamp first edge to min if it was extended\n                    if len(edges) &gt; 0 and edges[0] &lt; series.min():\n                        edges[0] = series.min()\n\n                elif strategy == \"kmeans\":\n                    n_bins = override.get(\"n_bins\", default_n_bins)\n                    est = KBinsDiscretizer(\n                        n_bins=n_bins, strategy=\"kmeans\", encode=\"ordinal\", quantile_method=\"averaged_inverted_cdf\"\n                    )\n                    est.fit(series.values.reshape(-1, 1))  # type: ignore\n                    edges = est.bin_edges_[0]\n\n                elif strategy == \"custom\":\n                    # Check override first, then global custom_bins\n                    custom_bins = override.get(\"custom_bins\")\n                    if not custom_bins:\n                        custom_bins = config.get(\"custom_bins\", {}).get(col)\n\n                    if custom_bins:\n                        edges = np.array(sorted(custom_bins))\n\n                    # Handle custom labels\n                    labels = override.get(\"custom_labels\")\n                    if not labels:\n                        labels = config.get(\"custom_labels\", {}).get(col)\n                    if labels:\n                        custom_labels_map[col] = labels\n\n                elif strategy == \"kbins\":\n                    n_bins = override.get(\n                        \"kbins_n_bins\", config.get(\"kbins_n_bins\", default_n_bins)\n                    )\n                    k_strategy = override.get(\n                        \"kbins_strategy\", config.get(\"kbins_strategy\", \"quantile\")\n                    )\n\n                    # Map strategy names\n                    sklearn_strategy = k_strategy\n                    if k_strategy == \"equal_width\":\n                        sklearn_strategy = \"uniform\"\n                    elif k_strategy == \"equal_frequency\":\n                        sklearn_strategy = \"quantile\"\n\n                    est = KBinsDiscretizer(\n                        n_bins=n_bins, strategy=sklearn_strategy, encode=\"ordinal\", quantile_method=\"averaged_inverted_cdf\"\n                    )\n                    est.fit(series.values.reshape(-1, 1))  # type: ignore\n                    edges = est.bin_edges_[0]\n\n                if edges is not None:\n                    bin_edges_map[col] = edges.tolist()\n\n            except Exception:\n                continue\n\n        return {\n            \"type\": \"general_binning\",\n            \"bin_edges\": bin_edges_map,\n            \"custom_labels\": custom_labels_map,\n            \"output_suffix\": config.get(\"output_suffix\", \"_binned\"),\n            \"drop_original\": config.get(\"drop_original\", False),\n            \"label_format\": config.get(\"label_format\", \"ordinal\"),\n            \"missing_strategy\": config.get(\"missing_strategy\", \"keep\"),\n            \"missing_label\": config.get(\"missing_label\", \"Missing\"),\n            \"include_lowest\": config.get(\"include_lowest\", True),\n            \"precision\": config.get(\"precision\", 3),\n        }\n</code></pre>"},{"location":"reference/api/preprocessing/bucketing.html#skyulf.preprocessing.bucketing.KBinsDiscretizerCalculator","title":"<code>KBinsDiscretizerCalculator</code>","text":"<p>               Bases: <code>GeneralBinningCalculator</code></p> <p>Calculator for KBinsDiscretizer node. Wraps GeneralBinningCalculator with kbins strategy.</p> Source code in <code>skyulf-core/skyulf/preprocessing/bucketing.py</code> <pre><code>@NodeRegistry.register(\"KBinsDiscretizer\", KBinsDiscretizerApplier)\nclass KBinsDiscretizerCalculator(GeneralBinningCalculator):\n    \"\"\"\n    Calculator for KBinsDiscretizer node.\n    Wraps GeneralBinningCalculator with kbins strategy.\n    \"\"\"\n\n    def fit(\n        self,\n        df: SkyulfDataFrame,\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        new_config = config.copy()\n        new_config[\"strategy\"] = \"kbins\"\n\n        # Map sklearn params to GeneralBinning params\n        if \"n_bins\" in config:\n            new_config[\"kbins_n_bins\"] = config[\"n_bins\"]\n\n        # sklearn strategy: uniform, quantile, kmeans\n        # GeneralBinning kbins_strategy: same\n        if \"strategy\" in config and config[\"strategy\"] != \"kbins\":\n            new_config[\"kbins_strategy\"] = config[\"strategy\"]\n\n        return super().fit(df, new_config)\n</code></pre>"},{"location":"reference/api/preprocessing/casting.html","title":"API: preprocessing.casting","text":""},{"location":"reference/api/preprocessing/casting.html#skyulf.preprocessing.casting","title":"<code>skyulf.preprocessing.casting</code>","text":""},{"location":"reference/api/preprocessing/cleaning.html","title":"API: preprocessing.cleaning","text":""},{"location":"reference/api/preprocessing/cleaning.html#skyulf.preprocessing.cleaning","title":"<code>skyulf.preprocessing.cleaning</code>","text":""},{"location":"reference/api/preprocessing/drop_and_missing.html","title":"API: preprocessing.drop_and_missing","text":""},{"location":"reference/api/preprocessing/drop_and_missing.html#skyulf.preprocessing.drop_and_missing","title":"<code>skyulf.preprocessing.drop_and_missing</code>","text":""},{"location":"reference/api/preprocessing/encoding.html","title":"API: preprocessing.encoding","text":""},{"location":"reference/api/preprocessing/encoding.html#skyulf.preprocessing.encoding","title":"<code>skyulf.preprocessing.encoding</code>","text":""},{"location":"reference/api/preprocessing/feature_generation.html","title":"API: preprocessing.feature_generation","text":""},{"location":"reference/api/preprocessing/feature_generation.html#skyulf.preprocessing.feature_generation","title":"<code>skyulf.preprocessing.feature_generation</code>","text":""},{"location":"reference/api/preprocessing/feature_selection.html","title":"API: preprocessing.feature_selection","text":""},{"location":"reference/api/preprocessing/feature_selection.html#skyulf.preprocessing.feature_selection","title":"<code>skyulf.preprocessing.feature_selection</code>","text":""},{"location":"reference/api/preprocessing/imputation.html","title":"API: preprocessing.imputation","text":""},{"location":"reference/api/preprocessing/imputation.html#skyulf.preprocessing.imputation","title":"<code>skyulf.preprocessing.imputation</code>","text":""},{"location":"reference/api/preprocessing/inspection.html","title":"API: preprocessing.inspection","text":""},{"location":"reference/api/preprocessing/inspection.html#skyulf.preprocessing.inspection","title":"<code>skyulf.preprocessing.inspection</code>","text":""},{"location":"reference/api/preprocessing/outliers.html","title":"API: preprocessing.outliers","text":""},{"location":"reference/api/preprocessing/outliers.html#skyulf.preprocessing.outliers","title":"<code>skyulf.preprocessing.outliers</code>","text":""},{"location":"reference/api/preprocessing/pipeline.html","title":"API: preprocessing.pipeline","text":""},{"location":"reference/api/preprocessing/pipeline.html#skyulf.preprocessing.pipeline","title":"<code>skyulf.preprocessing.pipeline</code>","text":"<p>Feature Engineering Pipeline Orchestrator.</p>"},{"location":"reference/api/preprocessing/pipeline.html#skyulf.preprocessing.pipeline.FeatureEngineer","title":"<code>FeatureEngineer</code>","text":"<p>Orchestrates a sequence of feature engineering steps.</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>class FeatureEngineer:\n    \"\"\"\n    Orchestrates a sequence of feature engineering steps.\n    \"\"\"\n\n    def __init__(self, steps_config: List[Dict[str, Any]]):\n        self.steps_config = steps_config\n        self.fitted_steps: List[Dict[str, Any]] = []\n\n    def transform(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply fitted transformations to new data.\n        \"\"\"\n        current_data = data\n\n        for step in self.fitted_steps:\n            name = step[\"name\"]\n            transformer_type = step[\"type\"]\n            applier = step[\"applier\"]\n            artifact = step[\"artifact\"]\n\n            # Skip splitters during inference/transform\n            if transformer_type in [\n                \"TrainTestSplitter\",\n                \"feature_target_split\",\n                \"Oversampling\",\n                \"Undersampling\",\n            ]:\n                continue\n\n            logger.debug(f\"Applying step: {name} ({transformer_type})\")\n            current_data = applier.apply(current_data, artifact)\n\n        return current_data\n\n    def fit_transform(self, data: Union[pd.DataFrame, Any], node_id_prefix=\"\") -&gt; Any:  # noqa: C901\n        \"\"\"\n        Runs the pipeline on data.\n        Returns: (transformed_data, metrics_dict)\n        \"\"\"\n        self.fitted_steps = []  # Reset fitted steps\n        current_data = data\n        metrics: Dict[str, Any] = {}\n\n        for i, step in enumerate(self.steps_config):\n            name = step[\"name\"]\n            transformer_type = step[\"transformer\"]\n            params = step.get(\"params\", {})\n\n            logger.info(f\"Running step {i}: {name} ({transformer_type})\")\n            logger.debug(\n                f\"FeatureEngineer running step {i}: {name} ({transformer_type})\"\n            )\n            logger.debug(f\"current_data type: {type(current_data)}\")\n\n            # Capture metrics before\n            rows_before, cols_before = get_data_stats(current_data)\n\n            # Keep reference for comparison (for Winsorize metrics)\n            data_before = current_data\n\n            calculator, applier = self._get_transformer_components(transformer_type)\n\n            # We need a unique ID for this step's artifacts\n            step_node_id = f\"{node_id_prefix}_{name}\"\n\n            transformer = StatefulTransformer(calculator, applier, step_node_id)\n\n            # Handle special transformers that change data structure\n            # Splitters return SplitDataset or (X, y) tuples instead of a simple DataFrame,\n            # so they bypass the standard StatefulTransformer wrapper.\n\n            # Initialize fitted_params\n            fitted_params = {}\n\n            if transformer_type == \"TrainTestSplitter\":\n                logger.debug(\"Handling TrainTestSplitter\")\n                # TrainTestSplitter changes DataFrame -&gt; SplitDataset.\n                # We bypass StatefulTransformer to allow this structural change.\n                # It can also handle (X, y) tuple if FeatureTargetSplit was done first.\n                if isinstance(current_data, (pd.DataFrame, tuple)):\n                    logger.debug(\"Executing TrainTestSplitter logic\")\n                    params = calculator.fit(current_data, params)\n                    current_data = applier.apply(current_data, params)\n                    # In SDK, params are returned but not auto-saved to artifact store here.\n                    # The Pipeline object will handle state persistence.\n                else:\n                    logger.debug(\n                        f\"Skipping TrainTestSplitter. current_data is {type(current_data)}\"\n                    )\n                    logger.warning(\n                        \"Attempting to split an already split dataset. Skipping TrainTestSplitter.\"\n                    )\n\n            elif transformer_type == \"feature_target_split\":\n                logger.debug(\"Handling feature_target_split\")\n                # FeatureTargetSplitter changes structure to (X, y) or Dict of (X, y).\n                # We bypass StatefulTransformer to allow this structural change.\n                params = calculator.fit(current_data, params)\n                current_data = applier.apply(current_data, params)\n\n            else:\n                logger.debug(\"Handling standard transformer via StatefulTransformer\")\n                current_data = transformer.fit_transform(current_data, params)\n                # In SDK, transformer.params holds the state.\n                fitted_params = transformer.params\n\n                self.fitted_steps.append(\n                    {\n                        \"name\": name,\n                        \"type\": transformer_type,\n                        \"applier\": applier,\n                        \"artifact\": fitted_params,\n                    }\n                )\n\n            logger.debug(f\"Step {i} complete. New data type: {type(current_data)}\")\n\n            # Retrieve fitted params to get metrics from the calculator\n            try:\n                if fitted_params:\n                    # Imputation Metrics\n                    if transformer_type in [\n                        \"SimpleImputer\",\n                        \"KNNImputer\",\n                        \"IterativeImputer\",\n                    ]:\n                        if \"missing_counts\" in fitted_params:\n                            metrics[\"missing_counts\"] = fitted_params[\"missing_counts\"]\n                        if \"total_missing\" in fitted_params:\n                            metrics[\"total_missing\"] = fitted_params[\"total_missing\"]\n                        if \"fill_values\" in fitted_params:\n                            metrics[\"fill_values\"] = fitted_params[\"fill_values\"]\n\n                    # Feature Selection Metrics\n                    if transformer_type in [\n                        \"feature_selection\",\n                        \"UnivariateSelection\",\n                        \"ModelBasedSelection\",\n                        \"VarianceThreshold\",\n                    ]:\n                        if \"feature_scores\" in fitted_params:\n                            metrics[\"feature_scores\"] = fitted_params[\"feature_scores\"]\n                        if \"p_values\" in fitted_params:\n                            metrics[\"p_values\"] = fitted_params[\"p_values\"]\n                        if \"feature_importances\" in fitted_params:\n                            metrics[\"feature_importances\"] = fitted_params[\n                                \"feature_importances\"\n                            ]\n                        if \"variances\" in fitted_params:\n                            metrics[\"variances\"] = fitted_params[\"variances\"]\n                        if \"ranking\" in fitted_params:\n                            metrics[\"ranking\"] = fitted_params[\"ranking\"]\n                        if \"selected_columns\" in fitted_params:\n                            metrics[\"selected_columns\"] = fitted_params[\n                                \"selected_columns\"\n                            ]\n\n                    # Scaling Metrics\n                    if transformer_type in [\n                        \"StandardScaler\",\n                        \"MinMaxScaler\",\n                        \"RobustScaler\",\n                        \"MaxAbsScaler\",\n                    ]:\n                        if \"mean\" in fitted_params:\n                            metrics[\"mean\"] = fitted_params[\"mean\"]\n                        if \"scale\" in fitted_params:\n                            metrics[\"scale\"] = fitted_params[\"scale\"]\n                        if \"var\" in fitted_params:\n                            metrics[\"var\"] = fitted_params[\"var\"]\n                        if \"min\" in fitted_params:\n                            metrics[\"min\"] = fitted_params[\"min\"]\n                        if \"data_min\" in fitted_params:\n                            metrics[\"data_min\"] = fitted_params[\"data_min\"]\n                        if \"data_max\" in fitted_params:\n                            metrics[\"data_max\"] = fitted_params[\"data_max\"]\n                        if \"center\" in fitted_params:\n                            metrics[\"center\"] = fitted_params[\"center\"]\n                        if \"max_abs\" in fitted_params:\n                            metrics[\"max_abs\"] = fitted_params[\"max_abs\"]\n                        if \"columns\" in fitted_params:\n                            metrics[\"columns\"] = fitted_params[\"columns\"]\n\n                    # Outlier Metrics\n                    if transformer_type in [\n                        \"IQR\",\n                        \"Winsorize\",\n                        \"ZScore\",\n                        \"EllipticEnvelope\",\n                    ]:\n                        if \"warnings\" in fitted_params:\n                            metrics[\"warnings\"] = fitted_params[\"warnings\"]\n\n                    if transformer_type in [\"IQR\", \"Winsorize\"]:\n                        if \"bounds\" in fitted_params:\n                            metrics[\"bounds\"] = fitted_params[\"bounds\"]\n\n                    if transformer_type == \"ZScore\":\n                        if \"stats\" in fitted_params:\n                            metrics[\"stats\"] = fitted_params[\"stats\"]\n\n                    if transformer_type == \"EllipticEnvelope\":\n                        if \"contamination\" in fitted_params:\n                            metrics[\"contamination\"] = fitted_params[\"contamination\"]\n\n                    # Bucketing Metrics\n                    if transformer_type in [\n                        \"GeneralBinning\",\n                        \"EqualWidthBinning\",\n                        \"EqualFrequencyBinning\",\n                        \"CustomBinning\",\n                        \"KBinsDiscretizer\",\n                    ]:\n                        if \"bin_edges\" in fitted_params:\n                            metrics[\"bin_edges\"] = fitted_params[\"bin_edges\"]\n                        if \"n_bins\" in fitted_params:\n                            metrics[\"n_bins\"] = fitted_params[\"n_bins\"]\n\n                    # Feature Generation Metrics\n                    if transformer_type in [\"FeatureMath\", \"FeatureGenerationNode\"]:\n                        if \"operations\" in fitted_params:\n                            metrics[\"operations_count\"] = len(\n                                fitted_params[\"operations\"]\n                            )\n                            metrics[\"operations\"] = fitted_params[\"operations\"]\n                        # Calculate generated features by comparing columns\n                        if isinstance(data_before, pd.DataFrame) and isinstance(\n                            current_data, pd.DataFrame\n                        ):\n                            new_cols = list(\n                                set(current_data.columns) - set(data_before.columns)\n                            )\n                            metrics[\"generated_features\"] = new_cols\n                        elif isinstance(data_before, SplitDataset) and isinstance(\n                            current_data, SplitDataset\n                        ):\n                            # Check train set\n                            if isinstance(\n                                data_before.train, pd.DataFrame\n                            ) and isinstance(current_data.train, pd.DataFrame):\n                                new_cols = list(\n                                    set(current_data.train.columns)\n                                    - set(data_before.train.columns)\n                                )\n                                metrics[\"generated_features\"] = new_cols\n                            elif isinstance(data_before.train, tuple) and isinstance(\n                                current_data.train, tuple\n                            ):\n                                # (X, y) tuple\n                                X_before, _ = data_before.train\n                                X_after, _ = current_data.train\n                                if isinstance(X_before, pd.DataFrame) and isinstance(\n                                    X_after, pd.DataFrame\n                                ):\n                                    new_cols = list(\n                                        set(X_after.columns) - set(X_before.columns)\n                                    )\n                                    metrics[\"generated_features\"] = new_cols\n\n            except Exception as e:\n                logger.warning(f\"Failed to retrieve metrics for step {name}: {e}\")\n\n            # Capture metrics after\n            rows_after, cols_after = get_data_stats(current_data)\n\n            # Resampling Metrics (Calculated from data)\n            if transformer_type in [\"Oversampling\", \"Undersampling\"]:\n                try:\n                    # Extract y to calculate class counts\n                    y_res = None\n                    if isinstance(current_data, SplitDataset):\n                        if isinstance(current_data.train, tuple):\n                            _, y_res = current_data.train\n                        elif isinstance(current_data.train, pd.DataFrame):\n                            # Try to find target column from params\n                            target_col = params.get(\"target_column\")\n                            if target_col and target_col in current_data.train.columns:\n                                y_res = current_data.train[target_col]\n                    elif isinstance(current_data, tuple):\n                        _, y_res = current_data\n                    elif isinstance(current_data, pd.DataFrame):\n                        target_col = params.get(\"target_column\")\n                        if target_col and target_col in current_data.columns:\n                            y_res = current_data[target_col]\n\n                    if y_res is not None:\n                        counts = y_res.value_counts().to_dict()\n                        # Convert keys to string to ensure JSON serializability\n                        metrics[\"class_counts\"] = {\n                            str(k): int(v) for k, v in counts.items()\n                        }\n                        metrics[\"total_samples\"] = int(len(y_res))\n                except Exception as e:\n                    logger.warning(f\"Failed to calculate resampling metrics: {e}\")\n\n            if rows_after &gt; 0 or cols_after:\n                if transformer_type in [\n                    \"DropMissingRows\",\n                    \"Deduplicate\",\n                    \"IQR\",\n                    \"ZScore\",\n                    \"EllipticEnvelope\",\n                    \"Winsorize\",\n                ]:\n                    dropped = rows_before - rows_after\n                    metrics[f\"{transformer_type}_rows_removed\"] = dropped\n                    metrics[f\"{transformer_type}_rows_remaining\"] = rows_after\n                    metrics[f\"{transformer_type}_rows_total\"] = rows_before\n                    metrics[\"rows_removed\"] = dropped\n                    metrics[\"rows_total\"] = rows_before\n\n                    # Special metric for Winsorize: Values Clipped\n                    if transformer_type == \"Winsorize\":\n                        try:\n                            clipped_count = 0\n\n                            # Helper to count diffs\n                            def count_diffs(df1, df2):\n                                if isinstance(df1, pd.DataFrame) and isinstance(\n                                    df2, pd.DataFrame\n                                ):\n                                    if df1.shape == df2.shape:\n                                        return int(df1.ne(df2).sum().sum())\n                                elif (\n                                    isinstance(df1, tuple)\n                                    and isinstance(df2, tuple)\n                                    and len(df1) == 2\n                                    and len(df2) == 2\n                                ):\n                                    # Handle (X, y) tuple\n                                    diffs = 0\n                                    # Compare X (index 0)\n                                    if isinstance(df1[0], pd.DataFrame) and isinstance(\n                                        df2[0], pd.DataFrame\n                                    ):\n                                        if df1[0].shape == df2[0].shape:\n                                            diffs += int(df1[0].ne(df2[0]).sum().sum())\n                                    # Compare y (index 1) - usually Series\n                                    if isinstance(\n                                        df1[1], (pd.DataFrame, pd.Series)\n                                    ) and isinstance(df2[1], (pd.DataFrame, pd.Series)):\n                                        if df1[1].shape == df2[1].shape:\n                                            diffs += int(df1[1].ne(df2[1]).sum().sum())  # type: ignore\n                                    return diffs\n                                return 0\n\n                            if isinstance(data_before, pd.DataFrame) and isinstance(\n                                current_data, pd.DataFrame\n                            ):\n                                clipped_count = count_diffs(data_before, current_data)\n                            elif isinstance(data_before, SplitDataset) and isinstance(\n                                current_data, SplitDataset\n                            ):\n                                clipped_count += count_diffs(\n                                    data_before.train, current_data.train\n                                )\n                                clipped_count += count_diffs(\n                                    data_before.test, current_data.test\n                                )\n                                clipped_count += count_diffs(\n                                    data_before.validation, current_data.validation\n                                )\n\n                            metrics[\"values_clipped\"] = clipped_count\n                        except Exception as e:\n                            logger.warning(\n                                f\"Failed to calculate values_clipped for Winsorize: {e}\"\n                            )\n                            pass\n\n                if transformer_type == \"MissingIndicator\":\n                    new_cols_set = cols_after - cols_before\n                    metrics[\"missing_indicators_created\"] = len(new_cols_set)\n                    cast(Dict[str, Any], metrics)[\"missing_indicators_columns\"] = list(\n                        new_cols_set\n                    )\n\n                if transformer_type == \"DropMissingColumns\":\n                    dropped_cols_set = cols_before - cols_after\n                    cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                        dropped_cols_set\n                    )\n                    metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n                if transformer_type == \"feature_selection\":\n                    dropped_cols_set = cols_before - cols_after\n                    cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                        dropped_cols_set\n                    )\n                    metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n                if transformer_type in [\n                    \"OneHotEncoder\",\n                    \"LabelEncoder\",\n                    \"OrdinalEncoder\",\n                    \"TargetEncoder\",\n                    \"HashEncoder\",\n                    \"DummyEncoder\",\n                ]:\n                    new_cols_set = cols_after - cols_before\n                    metrics[\"new_features_count\"] = len(new_cols_set)\n                    metrics[\"encoded_columns_count\"] = len(params.get(\"columns\", []))\n\n                    if \"categories_count\" in params:\n                        metrics[\"categories_count\"] = params[\"categories_count\"]\n                    if \"classes_count\" in params:\n                        metrics[\"classes_count\"] = params[\"classes_count\"]\n\n        return current_data, metrics\n\n    def _get_transformer_components(self, type_name: str):  # noqa: C901\n        # Try Registry first\n        try:\n            return (\n                NodeRegistry.get_calculator(type_name)(),\n                NodeRegistry.get_applier(type_name)(),\n            )\n        except ValueError:\n            raise ValueError(f\"Unknown transformer type: {type_name}\")\n</code></pre>"},{"location":"reference/api/preprocessing/pipeline.html#skyulf.preprocessing.pipeline.FeatureEngineer.fit_transform","title":"<code>fit_transform(data, node_id_prefix='')</code>","text":"<p>Runs the pipeline on data. Returns: (transformed_data, metrics_dict)</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>def fit_transform(self, data: Union[pd.DataFrame, Any], node_id_prefix=\"\") -&gt; Any:  # noqa: C901\n    \"\"\"\n    Runs the pipeline on data.\n    Returns: (transformed_data, metrics_dict)\n    \"\"\"\n    self.fitted_steps = []  # Reset fitted steps\n    current_data = data\n    metrics: Dict[str, Any] = {}\n\n    for i, step in enumerate(self.steps_config):\n        name = step[\"name\"]\n        transformer_type = step[\"transformer\"]\n        params = step.get(\"params\", {})\n\n        logger.info(f\"Running step {i}: {name} ({transformer_type})\")\n        logger.debug(\n            f\"FeatureEngineer running step {i}: {name} ({transformer_type})\"\n        )\n        logger.debug(f\"current_data type: {type(current_data)}\")\n\n        # Capture metrics before\n        rows_before, cols_before = get_data_stats(current_data)\n\n        # Keep reference for comparison (for Winsorize metrics)\n        data_before = current_data\n\n        calculator, applier = self._get_transformer_components(transformer_type)\n\n        # We need a unique ID for this step's artifacts\n        step_node_id = f\"{node_id_prefix}_{name}\"\n\n        transformer = StatefulTransformer(calculator, applier, step_node_id)\n\n        # Handle special transformers that change data structure\n        # Splitters return SplitDataset or (X, y) tuples instead of a simple DataFrame,\n        # so they bypass the standard StatefulTransformer wrapper.\n\n        # Initialize fitted_params\n        fitted_params = {}\n\n        if transformer_type == \"TrainTestSplitter\":\n            logger.debug(\"Handling TrainTestSplitter\")\n            # TrainTestSplitter changes DataFrame -&gt; SplitDataset.\n            # We bypass StatefulTransformer to allow this structural change.\n            # It can also handle (X, y) tuple if FeatureTargetSplit was done first.\n            if isinstance(current_data, (pd.DataFrame, tuple)):\n                logger.debug(\"Executing TrainTestSplitter logic\")\n                params = calculator.fit(current_data, params)\n                current_data = applier.apply(current_data, params)\n                # In SDK, params are returned but not auto-saved to artifact store here.\n                # The Pipeline object will handle state persistence.\n            else:\n                logger.debug(\n                    f\"Skipping TrainTestSplitter. current_data is {type(current_data)}\"\n                )\n                logger.warning(\n                    \"Attempting to split an already split dataset. Skipping TrainTestSplitter.\"\n                )\n\n        elif transformer_type == \"feature_target_split\":\n            logger.debug(\"Handling feature_target_split\")\n            # FeatureTargetSplitter changes structure to (X, y) or Dict of (X, y).\n            # We bypass StatefulTransformer to allow this structural change.\n            params = calculator.fit(current_data, params)\n            current_data = applier.apply(current_data, params)\n\n        else:\n            logger.debug(\"Handling standard transformer via StatefulTransformer\")\n            current_data = transformer.fit_transform(current_data, params)\n            # In SDK, transformer.params holds the state.\n            fitted_params = transformer.params\n\n            self.fitted_steps.append(\n                {\n                    \"name\": name,\n                    \"type\": transformer_type,\n                    \"applier\": applier,\n                    \"artifact\": fitted_params,\n                }\n            )\n\n        logger.debug(f\"Step {i} complete. New data type: {type(current_data)}\")\n\n        # Retrieve fitted params to get metrics from the calculator\n        try:\n            if fitted_params:\n                # Imputation Metrics\n                if transformer_type in [\n                    \"SimpleImputer\",\n                    \"KNNImputer\",\n                    \"IterativeImputer\",\n                ]:\n                    if \"missing_counts\" in fitted_params:\n                        metrics[\"missing_counts\"] = fitted_params[\"missing_counts\"]\n                    if \"total_missing\" in fitted_params:\n                        metrics[\"total_missing\"] = fitted_params[\"total_missing\"]\n                    if \"fill_values\" in fitted_params:\n                        metrics[\"fill_values\"] = fitted_params[\"fill_values\"]\n\n                # Feature Selection Metrics\n                if transformer_type in [\n                    \"feature_selection\",\n                    \"UnivariateSelection\",\n                    \"ModelBasedSelection\",\n                    \"VarianceThreshold\",\n                ]:\n                    if \"feature_scores\" in fitted_params:\n                        metrics[\"feature_scores\"] = fitted_params[\"feature_scores\"]\n                    if \"p_values\" in fitted_params:\n                        metrics[\"p_values\"] = fitted_params[\"p_values\"]\n                    if \"feature_importances\" in fitted_params:\n                        metrics[\"feature_importances\"] = fitted_params[\n                            \"feature_importances\"\n                        ]\n                    if \"variances\" in fitted_params:\n                        metrics[\"variances\"] = fitted_params[\"variances\"]\n                    if \"ranking\" in fitted_params:\n                        metrics[\"ranking\"] = fitted_params[\"ranking\"]\n                    if \"selected_columns\" in fitted_params:\n                        metrics[\"selected_columns\"] = fitted_params[\n                            \"selected_columns\"\n                        ]\n\n                # Scaling Metrics\n                if transformer_type in [\n                    \"StandardScaler\",\n                    \"MinMaxScaler\",\n                    \"RobustScaler\",\n                    \"MaxAbsScaler\",\n                ]:\n                    if \"mean\" in fitted_params:\n                        metrics[\"mean\"] = fitted_params[\"mean\"]\n                    if \"scale\" in fitted_params:\n                        metrics[\"scale\"] = fitted_params[\"scale\"]\n                    if \"var\" in fitted_params:\n                        metrics[\"var\"] = fitted_params[\"var\"]\n                    if \"min\" in fitted_params:\n                        metrics[\"min\"] = fitted_params[\"min\"]\n                    if \"data_min\" in fitted_params:\n                        metrics[\"data_min\"] = fitted_params[\"data_min\"]\n                    if \"data_max\" in fitted_params:\n                        metrics[\"data_max\"] = fitted_params[\"data_max\"]\n                    if \"center\" in fitted_params:\n                        metrics[\"center\"] = fitted_params[\"center\"]\n                    if \"max_abs\" in fitted_params:\n                        metrics[\"max_abs\"] = fitted_params[\"max_abs\"]\n                    if \"columns\" in fitted_params:\n                        metrics[\"columns\"] = fitted_params[\"columns\"]\n\n                # Outlier Metrics\n                if transformer_type in [\n                    \"IQR\",\n                    \"Winsorize\",\n                    \"ZScore\",\n                    \"EllipticEnvelope\",\n                ]:\n                    if \"warnings\" in fitted_params:\n                        metrics[\"warnings\"] = fitted_params[\"warnings\"]\n\n                if transformer_type in [\"IQR\", \"Winsorize\"]:\n                    if \"bounds\" in fitted_params:\n                        metrics[\"bounds\"] = fitted_params[\"bounds\"]\n\n                if transformer_type == \"ZScore\":\n                    if \"stats\" in fitted_params:\n                        metrics[\"stats\"] = fitted_params[\"stats\"]\n\n                if transformer_type == \"EllipticEnvelope\":\n                    if \"contamination\" in fitted_params:\n                        metrics[\"contamination\"] = fitted_params[\"contamination\"]\n\n                # Bucketing Metrics\n                if transformer_type in [\n                    \"GeneralBinning\",\n                    \"EqualWidthBinning\",\n                    \"EqualFrequencyBinning\",\n                    \"CustomBinning\",\n                    \"KBinsDiscretizer\",\n                ]:\n                    if \"bin_edges\" in fitted_params:\n                        metrics[\"bin_edges\"] = fitted_params[\"bin_edges\"]\n                    if \"n_bins\" in fitted_params:\n                        metrics[\"n_bins\"] = fitted_params[\"n_bins\"]\n\n                # Feature Generation Metrics\n                if transformer_type in [\"FeatureMath\", \"FeatureGenerationNode\"]:\n                    if \"operations\" in fitted_params:\n                        metrics[\"operations_count\"] = len(\n                            fitted_params[\"operations\"]\n                        )\n                        metrics[\"operations\"] = fitted_params[\"operations\"]\n                    # Calculate generated features by comparing columns\n                    if isinstance(data_before, pd.DataFrame) and isinstance(\n                        current_data, pd.DataFrame\n                    ):\n                        new_cols = list(\n                            set(current_data.columns) - set(data_before.columns)\n                        )\n                        metrics[\"generated_features\"] = new_cols\n                    elif isinstance(data_before, SplitDataset) and isinstance(\n                        current_data, SplitDataset\n                    ):\n                        # Check train set\n                        if isinstance(\n                            data_before.train, pd.DataFrame\n                        ) and isinstance(current_data.train, pd.DataFrame):\n                            new_cols = list(\n                                set(current_data.train.columns)\n                                - set(data_before.train.columns)\n                            )\n                            metrics[\"generated_features\"] = new_cols\n                        elif isinstance(data_before.train, tuple) and isinstance(\n                            current_data.train, tuple\n                        ):\n                            # (X, y) tuple\n                            X_before, _ = data_before.train\n                            X_after, _ = current_data.train\n                            if isinstance(X_before, pd.DataFrame) and isinstance(\n                                X_after, pd.DataFrame\n                            ):\n                                new_cols = list(\n                                    set(X_after.columns) - set(X_before.columns)\n                                )\n                                metrics[\"generated_features\"] = new_cols\n\n        except Exception as e:\n            logger.warning(f\"Failed to retrieve metrics for step {name}: {e}\")\n\n        # Capture metrics after\n        rows_after, cols_after = get_data_stats(current_data)\n\n        # Resampling Metrics (Calculated from data)\n        if transformer_type in [\"Oversampling\", \"Undersampling\"]:\n            try:\n                # Extract y to calculate class counts\n                y_res = None\n                if isinstance(current_data, SplitDataset):\n                    if isinstance(current_data.train, tuple):\n                        _, y_res = current_data.train\n                    elif isinstance(current_data.train, pd.DataFrame):\n                        # Try to find target column from params\n                        target_col = params.get(\"target_column\")\n                        if target_col and target_col in current_data.train.columns:\n                            y_res = current_data.train[target_col]\n                elif isinstance(current_data, tuple):\n                    _, y_res = current_data\n                elif isinstance(current_data, pd.DataFrame):\n                    target_col = params.get(\"target_column\")\n                    if target_col and target_col in current_data.columns:\n                        y_res = current_data[target_col]\n\n                if y_res is not None:\n                    counts = y_res.value_counts().to_dict()\n                    # Convert keys to string to ensure JSON serializability\n                    metrics[\"class_counts\"] = {\n                        str(k): int(v) for k, v in counts.items()\n                    }\n                    metrics[\"total_samples\"] = int(len(y_res))\n            except Exception as e:\n                logger.warning(f\"Failed to calculate resampling metrics: {e}\")\n\n        if rows_after &gt; 0 or cols_after:\n            if transformer_type in [\n                \"DropMissingRows\",\n                \"Deduplicate\",\n                \"IQR\",\n                \"ZScore\",\n                \"EllipticEnvelope\",\n                \"Winsorize\",\n            ]:\n                dropped = rows_before - rows_after\n                metrics[f\"{transformer_type}_rows_removed\"] = dropped\n                metrics[f\"{transformer_type}_rows_remaining\"] = rows_after\n                metrics[f\"{transformer_type}_rows_total\"] = rows_before\n                metrics[\"rows_removed\"] = dropped\n                metrics[\"rows_total\"] = rows_before\n\n                # Special metric for Winsorize: Values Clipped\n                if transformer_type == \"Winsorize\":\n                    try:\n                        clipped_count = 0\n\n                        # Helper to count diffs\n                        def count_diffs(df1, df2):\n                            if isinstance(df1, pd.DataFrame) and isinstance(\n                                df2, pd.DataFrame\n                            ):\n                                if df1.shape == df2.shape:\n                                    return int(df1.ne(df2).sum().sum())\n                            elif (\n                                isinstance(df1, tuple)\n                                and isinstance(df2, tuple)\n                                and len(df1) == 2\n                                and len(df2) == 2\n                            ):\n                                # Handle (X, y) tuple\n                                diffs = 0\n                                # Compare X (index 0)\n                                if isinstance(df1[0], pd.DataFrame) and isinstance(\n                                    df2[0], pd.DataFrame\n                                ):\n                                    if df1[0].shape == df2[0].shape:\n                                        diffs += int(df1[0].ne(df2[0]).sum().sum())\n                                # Compare y (index 1) - usually Series\n                                if isinstance(\n                                    df1[1], (pd.DataFrame, pd.Series)\n                                ) and isinstance(df2[1], (pd.DataFrame, pd.Series)):\n                                    if df1[1].shape == df2[1].shape:\n                                        diffs += int(df1[1].ne(df2[1]).sum().sum())  # type: ignore\n                                return diffs\n                            return 0\n\n                        if isinstance(data_before, pd.DataFrame) and isinstance(\n                            current_data, pd.DataFrame\n                        ):\n                            clipped_count = count_diffs(data_before, current_data)\n                        elif isinstance(data_before, SplitDataset) and isinstance(\n                            current_data, SplitDataset\n                        ):\n                            clipped_count += count_diffs(\n                                data_before.train, current_data.train\n                            )\n                            clipped_count += count_diffs(\n                                data_before.test, current_data.test\n                            )\n                            clipped_count += count_diffs(\n                                data_before.validation, current_data.validation\n                            )\n\n                        metrics[\"values_clipped\"] = clipped_count\n                    except Exception as e:\n                        logger.warning(\n                            f\"Failed to calculate values_clipped for Winsorize: {e}\"\n                        )\n                        pass\n\n            if transformer_type == \"MissingIndicator\":\n                new_cols_set = cols_after - cols_before\n                metrics[\"missing_indicators_created\"] = len(new_cols_set)\n                cast(Dict[str, Any], metrics)[\"missing_indicators_columns\"] = list(\n                    new_cols_set\n                )\n\n            if transformer_type == \"DropMissingColumns\":\n                dropped_cols_set = cols_before - cols_after\n                cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                    dropped_cols_set\n                )\n                metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n            if transformer_type == \"feature_selection\":\n                dropped_cols_set = cols_before - cols_after\n                cast(Dict[str, Any], metrics)[\"dropped_columns\"] = list(\n                    dropped_cols_set\n                )\n                metrics[\"dropped_columns_count\"] = len(dropped_cols_set)\n\n            if transformer_type in [\n                \"OneHotEncoder\",\n                \"LabelEncoder\",\n                \"OrdinalEncoder\",\n                \"TargetEncoder\",\n                \"HashEncoder\",\n                \"DummyEncoder\",\n            ]:\n                new_cols_set = cols_after - cols_before\n                metrics[\"new_features_count\"] = len(new_cols_set)\n                metrics[\"encoded_columns_count\"] = len(params.get(\"columns\", []))\n\n                if \"categories_count\" in params:\n                    metrics[\"categories_count\"] = params[\"categories_count\"]\n                if \"classes_count\" in params:\n                    metrics[\"classes_count\"] = params[\"classes_count\"]\n\n    return current_data, metrics\n</code></pre>"},{"location":"reference/api/preprocessing/pipeline.html#skyulf.preprocessing.pipeline.FeatureEngineer.transform","title":"<code>transform(data)</code>","text":"<p>Apply fitted transformations to new data.</p> Source code in <code>skyulf-core/skyulf/preprocessing/pipeline.py</code> <pre><code>def transform(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply fitted transformations to new data.\n    \"\"\"\n    current_data = data\n\n    for step in self.fitted_steps:\n        name = step[\"name\"]\n        transformer_type = step[\"type\"]\n        applier = step[\"applier\"]\n        artifact = step[\"artifact\"]\n\n        # Skip splitters during inference/transform\n        if transformer_type in [\n            \"TrainTestSplitter\",\n            \"feature_target_split\",\n            \"Oversampling\",\n            \"Undersampling\",\n        ]:\n            continue\n\n        logger.debug(f\"Applying step: {name} ({transformer_type})\")\n        current_data = applier.apply(current_data, artifact)\n\n    return current_data\n</code></pre>"},{"location":"reference/api/preprocessing/resampling.html","title":"API: preprocessing.resampling","text":""},{"location":"reference/api/preprocessing/resampling.html#skyulf.preprocessing.resampling","title":"<code>skyulf.preprocessing.resampling</code>","text":""},{"location":"reference/api/preprocessing/scaling.html","title":"API: preprocessing.scaling","text":""},{"location":"reference/api/preprocessing/scaling.html#skyulf.preprocessing.scaling","title":"<code>skyulf.preprocessing.scaling</code>","text":""},{"location":"reference/api/preprocessing/split.html","title":"API: preprocessing.split","text":""},{"location":"reference/api/preprocessing/split.html#skyulf.preprocessing.split","title":"<code>skyulf.preprocessing.split</code>","text":""},{"location":"reference/api/preprocessing/split.html#skyulf.preprocessing.split.DataSplitter","title":"<code>DataSplitter</code>","text":"<p>Splits a DataFrame into Train, Test, and optionally Validation sets.</p> Source code in <code>skyulf-core/skyulf/preprocessing/split.py</code> <pre><code>class DataSplitter:\n    \"\"\"\n    Splits a DataFrame into Train, Test, and optionally Validation sets.\n    \"\"\"\n\n    def __init__(\n        self,\n        test_size: float = 0.2,\n        validation_size: float = 0.0,\n        random_state: int = 42,\n        shuffle: bool = True,\n        stratify_col: Optional[str] = None,\n    ):\n        self.test_size = test_size\n        self.validation_size = validation_size\n        self.random_state = random_state\n        self.shuffle = shuffle\n        self.stratify_col = stratify_col\n\n    def split_xy(self, X: Union[pd.DataFrame, SkyulfDataFrame], y: Union[pd.Series, Any]) -&gt; SplitDataset:\n        \"\"\"\n        Splits X and y arrays.\n        \"\"\"\n        engine = get_engine(X)\n        is_polars = engine.name == \"polars\"\n\n        if is_polars:\n            # Convert to Pandas to preserve schema/metadata during split\n            X_pd = X.to_pandas()\n            y_pd = y.to_pandas() if y is not None else None\n        else:\n            X_pd = X\n            y_pd = y\n\n        stratify = y_pd if self.stratify_col else None  # If stratify is requested, use y\n\n        if stratify is not None:\n            # Check value counts\n            class_counts = y_pd.value_counts()\n            min_count = class_counts.min()\n\n            if min_count &lt; 2:\n                logger.warning(\n                    f\"Stratified split requested but the least populated class has only {min_count} \"\n                    \"member(s). Stratification will be disabled.\"\n                )\n                stratify = None\n\n        # First split: Train+Val vs Test\n        X_train_val, X_test, y_train_val, y_test = train_test_split(\n            X_pd,\n            y_pd,\n            test_size=self.test_size,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n            stratify=stratify,\n        )\n\n        validation = None\n        if self.validation_size &gt; 0:\n            relative_val_size = self.validation_size / (1 - self.test_size)\n            stratify_val = y_train_val if self.stratify_col else None\n\n            if stratify_val is not None:\n                class_counts_val = y_train_val.value_counts()\n                min_count_val = class_counts_val.min()\n\n                if min_count_val &lt; 2:\n                    logger.warning(\n                        \"Stratified validation split requested but the least populated class has only \"\n                        f\"{min_count_val} member(s). Stratification will be disabled for validation split.\"\n                    )\n                    stratify_val = None\n\n            X_train, X_val, y_train, y_val = train_test_split(\n                X_train_val,\n                y_train_val,\n                test_size=relative_val_size,\n                random_state=self.random_state,\n                shuffle=self.shuffle,\n                stratify=stratify_val,\n            )\n            validation = (X_val, y_val)\n        else:\n            X_train, y_train = X_train_val, y_train_val\n\n        # Convert back to Polars if needed\n        if is_polars:\n            import polars as pl\n\n            def to_pl(df_or_series):\n                if df_or_series is None: return None\n                if isinstance(df_or_series, pd.DataFrame): return pl.from_pandas(df_or_series)\n                if isinstance(df_or_series, pd.Series): return pl.from_pandas(df_or_series)\n                return df_or_series\n\n            X_train = to_pl(X_train)\n            y_train = to_pl(y_train)\n            X_test = to_pl(X_test)\n            y_test = to_pl(y_test)\n\n            if validation:\n                validation = (to_pl(validation[0]), to_pl(validation[1]))\n\n        return SplitDataset(\n            train=(X_train, y_train), test=(X_test, y_test), validation=validation\n        )\n\n    def split(self, df: Union[pd.DataFrame, SkyulfDataFrame]) -&gt; SplitDataset:\n        \"\"\"\n        Splits a DataFrame.\n        \"\"\"\n        engine = get_engine(df)\n        is_polars = engine.name == \"polars\"\n\n        if is_polars:\n            # Convert to Pandas to preserve schema/metadata during split\n            df_pd = df.to_pandas()\n        else:\n            df_pd = df\n\n        stratify = None\n        if self.stratify_col and self.stratify_col in df_pd.columns:\n            stratify = df_pd[self.stratify_col]\n            class_counts = stratify.value_counts()\n            if class_counts.min() &lt; 2:\n                logger.warning(\n                    f\"Stratified split requested but the least populated class has only {class_counts.min()} \"\n                    \"member(s). Stratification will be disabled.\"\n                )\n                stratify = None\n\n        train_val, test = train_test_split(\n            df_pd,\n            test_size=self.test_size,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n            stratify=stratify,\n        )\n\n        validation = None\n        if self.validation_size &gt; 0:\n            relative_val_size = self.validation_size / (1 - self.test_size)\n\n            stratify_val = None\n            if self.stratify_col and self.stratify_col in train_val.columns:\n                stratify_val = train_val[self.stratify_col]\n                class_counts_val = stratify_val.value_counts()\n                if class_counts_val.min() &lt; 2:\n                    logger.warning(\n                        \"Stratified validation split requested but the least populated class has only \"\n                        f\"{class_counts_val.min()} member(s). Stratification will be disabled for validation split.\"\n                    )\n                    stratify_val = None\n\n            train, val = train_test_split(\n                train_val,\n                test_size=relative_val_size,\n                random_state=self.random_state,\n                shuffle=self.shuffle,\n                stratify=stratify_val,\n            )\n            validation = val\n        else:\n            train = train_val\n\n        # Convert back to Polars if needed\n        if is_polars:\n            import polars as pl\n            train = pl.from_pandas(train)\n            test = pl.from_pandas(test)\n            if validation is not None:\n                validation = pl.from_pandas(validation)\n\n        return SplitDataset(train=train, test=test, validation=validation)\n</code></pre>"},{"location":"reference/api/preprocessing/split.html#skyulf.preprocessing.split.DataSplitter.split","title":"<code>split(df)</code>","text":"<p>Splits a DataFrame.</p> Source code in <code>skyulf-core/skyulf/preprocessing/split.py</code> <pre><code>def split(self, df: Union[pd.DataFrame, SkyulfDataFrame]) -&gt; SplitDataset:\n    \"\"\"\n    Splits a DataFrame.\n    \"\"\"\n    engine = get_engine(df)\n    is_polars = engine.name == \"polars\"\n\n    if is_polars:\n        # Convert to Pandas to preserve schema/metadata during split\n        df_pd = df.to_pandas()\n    else:\n        df_pd = df\n\n    stratify = None\n    if self.stratify_col and self.stratify_col in df_pd.columns:\n        stratify = df_pd[self.stratify_col]\n        class_counts = stratify.value_counts()\n        if class_counts.min() &lt; 2:\n            logger.warning(\n                f\"Stratified split requested but the least populated class has only {class_counts.min()} \"\n                \"member(s). Stratification will be disabled.\"\n            )\n            stratify = None\n\n    train_val, test = train_test_split(\n        df_pd,\n        test_size=self.test_size,\n        random_state=self.random_state,\n        shuffle=self.shuffle,\n        stratify=stratify,\n    )\n\n    validation = None\n    if self.validation_size &gt; 0:\n        relative_val_size = self.validation_size / (1 - self.test_size)\n\n        stratify_val = None\n        if self.stratify_col and self.stratify_col in train_val.columns:\n            stratify_val = train_val[self.stratify_col]\n            class_counts_val = stratify_val.value_counts()\n            if class_counts_val.min() &lt; 2:\n                logger.warning(\n                    \"Stratified validation split requested but the least populated class has only \"\n                    f\"{class_counts_val.min()} member(s). Stratification will be disabled for validation split.\"\n                )\n                stratify_val = None\n\n        train, val = train_test_split(\n            train_val,\n            test_size=relative_val_size,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n            stratify=stratify_val,\n        )\n        validation = val\n    else:\n        train = train_val\n\n    # Convert back to Polars if needed\n    if is_polars:\n        import polars as pl\n        train = pl.from_pandas(train)\n        test = pl.from_pandas(test)\n        if validation is not None:\n            validation = pl.from_pandas(validation)\n\n    return SplitDataset(train=train, test=test, validation=validation)\n</code></pre>"},{"location":"reference/api/preprocessing/split.html#skyulf.preprocessing.split.DataSplitter.split_xy","title":"<code>split_xy(X, y)</code>","text":"<p>Splits X and y arrays.</p> Source code in <code>skyulf-core/skyulf/preprocessing/split.py</code> <pre><code>def split_xy(self, X: Union[pd.DataFrame, SkyulfDataFrame], y: Union[pd.Series, Any]) -&gt; SplitDataset:\n    \"\"\"\n    Splits X and y arrays.\n    \"\"\"\n    engine = get_engine(X)\n    is_polars = engine.name == \"polars\"\n\n    if is_polars:\n        # Convert to Pandas to preserve schema/metadata during split\n        X_pd = X.to_pandas()\n        y_pd = y.to_pandas() if y is not None else None\n    else:\n        X_pd = X\n        y_pd = y\n\n    stratify = y_pd if self.stratify_col else None  # If stratify is requested, use y\n\n    if stratify is not None:\n        # Check value counts\n        class_counts = y_pd.value_counts()\n        min_count = class_counts.min()\n\n        if min_count &lt; 2:\n            logger.warning(\n                f\"Stratified split requested but the least populated class has only {min_count} \"\n                \"member(s). Stratification will be disabled.\"\n            )\n            stratify = None\n\n    # First split: Train+Val vs Test\n    X_train_val, X_test, y_train_val, y_test = train_test_split(\n        X_pd,\n        y_pd,\n        test_size=self.test_size,\n        random_state=self.random_state,\n        shuffle=self.shuffle,\n        stratify=stratify,\n    )\n\n    validation = None\n    if self.validation_size &gt; 0:\n        relative_val_size = self.validation_size / (1 - self.test_size)\n        stratify_val = y_train_val if self.stratify_col else None\n\n        if stratify_val is not None:\n            class_counts_val = y_train_val.value_counts()\n            min_count_val = class_counts_val.min()\n\n            if min_count_val &lt; 2:\n                logger.warning(\n                    \"Stratified validation split requested but the least populated class has only \"\n                    f\"{min_count_val} member(s). Stratification will be disabled for validation split.\"\n                )\n                stratify_val = None\n\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_train_val,\n            y_train_val,\n            test_size=relative_val_size,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n            stratify=stratify_val,\n        )\n        validation = (X_val, y_val)\n    else:\n        X_train, y_train = X_train_val, y_train_val\n\n    # Convert back to Polars if needed\n    if is_polars:\n        import polars as pl\n\n        def to_pl(df_or_series):\n            if df_or_series is None: return None\n            if isinstance(df_or_series, pd.DataFrame): return pl.from_pandas(df_or_series)\n            if isinstance(df_or_series, pd.Series): return pl.from_pandas(df_or_series)\n            return df_or_series\n\n        X_train = to_pl(X_train)\n        y_train = to_pl(y_train)\n        X_test = to_pl(X_test)\n        y_test = to_pl(y_test)\n\n        if validation:\n            validation = (to_pl(validation[0]), to_pl(validation[1]))\n\n    return SplitDataset(\n        train=(X_train, y_train), test=(X_test, y_test), validation=validation\n    )\n</code></pre>"},{"location":"reference/api/preprocessing/transformations.html","title":"API: preprocessing.transformations","text":""},{"location":"reference/api/preprocessing/transformations.html#skyulf.preprocessing.transformations","title":"<code>skyulf.preprocessing.transformations</code>","text":""},{"location":"user_guide/configuration.html","title":"Configuration","text":"<p>This page documents the configuration schema consumed by <code>SkyulfPipeline</code> and <code>FeatureEngineer</code>.</p>"},{"location":"user_guide/configuration.html#pipeline-config","title":"Pipeline config","text":"<p><code>SkyulfPipeline</code> expects:</p> <pre><code>{\n  \"preprocessing\": [ ... ],\n  \"modeling\": { ... }\n}\n</code></pre>"},{"location":"user_guide/configuration.html#preprocessing-config","title":"Preprocessing config","text":"<p>The preprocessing list is executed in order.</p> <p>Each step is:</p> <pre><code>{\n  \"name\": \"step_name\",\n  \"transformer\": \"TransformerType\",\n  \"params\": { ... }\n}\n</code></pre> <p><code>TransformerType</code> is a string key that <code>FeatureEngineer</code> dispatches to a Calculator/Applier pair. For the full list and per-node parameters, see:</p> <ul> <li>Reference \u2192 Preprocessing Nodes</li> <li>Reference \u2192 API \u2192 Preprocessing \u2192 pipeline</li> </ul>"},{"location":"user_guide/configuration.html#minimal-examples","title":"Minimal examples","text":"<pre><code># Split to avoid leakage\n{\"name\": \"split\", \"transformer\": \"TrainTestSplitter\", \"params\": {\"test_size\": 0.2, \"random_state\": 42, \"target_column\": \"target\"}}\n</code></pre> <pre><code># Impute missing numeric values\n{\"name\": \"impute\", \"transformer\": \"SimpleImputer\", \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]}}\n</code></pre> <pre><code># Encode categoricals\n{\"name\": \"encode\", \"transformer\": \"OneHotEncoder\", \"params\": {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"}}\n</code></pre> <pre><code># Scale numeric columns\n{\"name\": \"scale\", \"transformer\": \"StandardScaler\", \"params\": {\"auto_detect\": True}}\n</code></pre>"},{"location":"user_guide/configuration.html#modeling-config","title":"Modeling config","text":"<p><code>SkyulfPipeline</code> currently supports these model types:</p> <ul> <li><code>logistic_regression</code></li> <li><code>random_forest_classifier</code></li> <li><code>ridge_regression</code></li> <li><code>random_forest_regressor</code></li> <li><code>hyperparameter_tuner</code></li> </ul> <p>Example:</p> <pre><code>{\n  \"type\": \"random_forest_classifier\",\n  \"node_id\": \"model_node\",\n  \"params\": {\n    \"n_estimators\": 200,\n    \"max_depth\": 10\n  }\n}\n</code></pre> <p>Tuner example:</p> <pre><code>{\n  \"type\": \"hyperparameter_tuner\",\n  \"base_model\": {\"type\": \"logistic_regression\"},\n  \"strategy\": \"random\",\n  \"search_space\": {\"C\": [0.1, 1.0, 10.0]},\n  \"n_trials\": 25,\n  \"metric\": \"accuracy\"\n}\n</code></pre> <p>See \u201cModeling Nodes\u201d in Reference for details.</p>"},{"location":"user_guide/extending_custom_nodes.html","title":"Extending Skyulf-Core","text":"<p>Skyulf-core is intentionally simple: calculators learn parameters, appliers apply them.</p>"},{"location":"user_guide/extending_custom_nodes.html#add-a-new-preprocessing-node","title":"Add a new preprocessing node","text":"<ol> <li>Create a new module in <code>skyulf.preprocessing</code>.</li> <li>Implement a <code>Calculator</code> and an <code>Applier</code>.</li> <li>Register the node type string in the <code>FeatureEngineer</code> dispatcher.</li> </ol>"},{"location":"user_guide/extending_custom_nodes.html#example-skeleton","title":"Example skeleton","text":"<pre><code>from typing import Any, Dict, Tuple, Union\n\nimport pandas as pd\n\nfrom skyulf.preprocessing.base import BaseApplier, BaseCalculator\nfrom skyulf.utils import pack_pipeline_output, unpack_pipeline_input\n\n\nclass MyNodeCalculator(BaseCalculator):\n    def fit(\n        self,\n        df: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        config: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        X, _, _ = unpack_pipeline_input(df)\n        # Learn something from X...\n        return {\"type\": \"my_node\", \"columns\": config.get(\"columns\", [])}\n\n\nclass MyNodeApplier(BaseApplier):\n    def apply(\n        self,\n        df: Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]],\n        params: Dict[str, Any],\n    ) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]]:\n        X, y, is_tuple = unpack_pipeline_input(df)\n        # Apply transformation...\n        return pack_pipeline_output(X, y, is_tuple)\n</code></pre>"},{"location":"user_guide/extending_custom_nodes.html#add-a-new-modeling-estimator","title":"Add a new modeling estimator","text":"<ol> <li>Implement a new <code>BaseModelCalculator</code> and <code>BaseModelApplier</code> (or subclass <code>SklearnCalculator/SklearnApplier</code>).</li> <li>Add a mapping entry in <code>SkyulfPipeline._init_model_estimator()</code>.</li> </ol>"},{"location":"user_guide/extending_custom_nodes.html#testing-guidance","title":"Testing guidance","text":"<p>Prefer integration tests that run:</p> <p><code>Calculator.fit</code> \u2192 <code>Applier.apply</code> on a small synthetic dataframe.</p>"},{"location":"user_guide/installation.html","title":"Installation","text":""},{"location":"user_guide/installation.html#editable-install-repo-checkout","title":"Editable install (repo checkout)","text":"<p>From the repository root:</p> <pre><code>pip install -e ./skyulf-core\n</code></pre>"},{"location":"user_guide/installation.html#runtime-dependencies","title":"Runtime dependencies","text":"<p><code>skyulf-core</code> primarily relies on:</p> <ul> <li>Pandas</li> <li>NumPy</li> <li>Scikit-Learn</li> </ul> <p>Some preprocessing nodes use optional dependencies (e.g., <code>rapidfuzz</code> for string similarity in feature generation).</p>"},{"location":"user_guide/installation.html#import-check","title":"Import check","text":"<pre><code>from skyulf.pipeline import SkyulfPipeline\nfrom skyulf.data.dataset import SplitDataset\n</code></pre>"},{"location":"user_guide/overview.html","title":"Overview","text":"<p><code>skyulf-core</code> is a standalone ML pipeline library designed for reproducible feature engineering and modeling.</p>"},{"location":"user_guide/overview.html#key-idea-explicit-learned-state","title":"Key idea: explicit learned state","text":"<p>Skyulf-core uses a strict Calculator \u2192 Applier pattern:</p> <ul> <li>A Calculator learns from data and returns a <code>params</code> dictionary.</li> <li>An Applier takes <code>params</code> and transforms data.</li> </ul> <p>This differs from scikit-learn\u2019s default pattern:</p> <ul> <li>In scikit-learn, <code>fit()</code> mutates the estimator/transformer object (e.g. <code>self.mean_</code>, <code>self.categories_</code>), and <code>transform()</code> uses those hidden internal attributes.</li> <li>In Skyulf-core, <code>fit()</code> returns the learned state explicitly as a plain <code>params</code> dictionary (ideally JSON-serializable; sometimes pickled for complex sklearn objects), and <code>apply()</code> uses only that dictionary.</li> </ul> <p>Practically, this makes learned state easier to inspect, persist, and apply consistently across train/test/inference.</p>"},{"location":"user_guide/overview.html#two-ways-to-use-the-library","title":"Two ways to use the library","text":""},{"location":"user_guide/overview.html#1-pipeline-way-recommended","title":"1) Pipeline way (recommended)","text":"<p>Use <code>SkyulfPipeline</code> to run preprocessing + modeling end-to-end.</p>"},{"location":"user_guide/overview.html#2-component-way-low-level","title":"2) Component way (low-level)","text":"<p>Call calculators/appliers directly for debugging, testing, or custom scripts.</p> <p>This is also where you\u2019ll see <code>StatefulEstimator</code> used: it\u2019s a small convenience wrapper that keeps a fitted model artifact in memory and can run <code>fit_predict()</code> on a <code>SplitDataset</code>. <code>SkyulfPipeline</code> uses the same underlying idea internally.</p>"},{"location":"user_guide/overview.html#how-fit-transform-works-in-skyulf","title":"How <code>fit</code> / <code>transform</code> works in Skyulf","text":"<p>At a high level:</p> <ul> <li> <p><code>SkyulfPipeline.fit(data, target_column=...)</code></p> <ul> <li>Runs preprocessing in order.</li> <li>For each preprocessing step: Calculator learns params (typically from train only), then Applier applies those params to train/test/validation.</li> <li>Trains the model and reports metrics.</li> </ul> </li> <li> <p><code>SkyulfPipeline.predict(df)</code></p> <ul> <li>Applies the already-learned preprocessing params (no re-fitting).</li> <li>Skips steps that only make sense during training (e.g., splitters / resampling).</li> <li>Runs the trained model to produce predictions.</li> </ul> </li> </ul> <p>If you want reproducible \u201cproof-style\u201d checks (sklearn-style <code>X/y</code> split + leakage demonstration), see:</p> <ul> <li>Validation vs scikit-learn</li> </ul>"},{"location":"user_guide/overview.html#why-splitting-matters-leakage","title":"Why splitting matters (leakage)","text":"<p>Many preprocessing nodes learn from data (means, categories, bin edges, vocabularies\u2026). If you learn those from the full dataset and then evaluate on a test set, you leak information.</p> <p>The safe pattern is:</p> <ul> <li>Split first (or provide a <code>SplitDataset</code>).</li> <li>Fit preprocessing on <code>train</code> only.</li> <li>Reuse the learned <code>params</code> to transform <code>test</code> / new inference data.</li> </ul>"},{"location":"user_guide/overview.html#where-things-live","title":"Where things live","text":"<ul> <li><code>skyulf.preprocessing</code>: feature engineering nodes (imputation, encoding, scaling, \u2026)</li> <li><code>skyulf.modeling</code>: estimators (classification/regression + tuning)</li> <li><code>skyulf.data</code>: <code>SplitDataset</code> for safe train/test/validation flows</li> </ul>"},{"location":"user_guide/pipeline_quickstart.html","title":"Pipeline Quickstart","text":"<p>This guide shows a production-style workflow: split \u2192 fit \u2192 evaluate \u2192 predict \u2192 save/load.</p>"},{"location":"user_guide/pipeline_quickstart.html#what-data-shape-should-i-pass","title":"What data shape should I pass?","text":"<p><code>SkyulfPipeline.fit(...)</code> supports two common inputs:</p> <ol> <li> <p>A single <code>pd.DataFrame</code> that includes the target column.</p> <ul> <li>You pass <code>target_column=\"...\"</code> and Skyulf splits features/target internally.</li> <li>This is what the quickstart uses, because it\u2019s the simplest onboarding path.</li> </ul> </li> <li> <p>A <code>SplitDataset</code> (recommended when you already have train/test splits).</p> <ul> <li>Each split can be either a DataFrame (with the target column) or a tuple <code>(X, y)</code>.</li> </ul> </li> </ol> <p>You do not need to manually build <code>X</code> and <code>y</code> for the pipeline unless you want to.</p> <p>If you prefer a scikit-learn-style workflow (<code>X</code>/<code>y</code> + <code>train_test_split</code>), see:</p> <ul> <li>Validation vs scikit-learn (Proof)</li> </ul>"},{"location":"user_guide/pipeline_quickstart.html#complete-runnable-example","title":"Complete runnable example","text":"<p>This single snippet is intentionally end-to-end (no duplicated setup across steps).</p> <pre><code>from __future__ import annotations\n\nimport tempfile\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\n# 1) Define a pipeline config\n# Each preprocessing step is:\n#   {\"name\": \"...\", \"transformer\": \"TransformerType\", \"params\": {...}}\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"split\",\n            \"transformer\": \"TrainTestSplitter\",\n            \"params\": {\n                \"test_size\": 0.25,\n                \"validation_size\": 0.0,\n                \"random_state\": 42,\n                \"shuffle\": True,\n                \"stratify\": True,\n                \"target_column\": \"target\",\n            },\n        },\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"},\n        },\n    ],\n    \"modeling\": {\n        \"type\": \"random_forest_classifier\",\n        \"params\": {\"n_estimators\": 50, \"random_state\": 42},\n    },\n}\n\n# 2) Training data (includes the target column)\ndf = pd.DataFrame(\n    {\n        \"age\": [10, 20, None, 40, 50, 60, None, 80],\n        \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n        \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n    }\n)\n\n# 3) Fit (learn params on train split, apply to test split)\npipeline = SkyulfPipeline(config)\nmetrics = pipeline.fit(df, target_column=\"target\")\nprint(\"Metrics keys:\", list(metrics.keys()))\n\n# 4) Predict (feature-only dataframe)\nincoming = pd.DataFrame({\"age\": [25, None], \"city\": [\"A\", \"C\"]})\npreds = pipeline.predict(incoming)\nprint(\"Preds:\")\nprint(preds)\n\n# 5) Save / load\nwith tempfile.TemporaryDirectory() as tmp:\n    model_path = Path(tmp) / \"model.pkl\"\n    pipeline.save(model_path)\n    loaded = SkyulfPipeline.load(model_path)\n    preds2 = loaded.predict(incoming)\n\nprint(\"Preds after reload:\")\nprint(preds2)\n</code></pre>"},{"location":"user_guide/serialization.html","title":"Serialization","text":""},{"location":"user_guide/serialization.html#what-is-persisted","title":"What is persisted","text":"<p><code>SkyulfPipeline.save()</code> uses Python <code>pickle</code> to serialize the entire pipeline object.</p> <p>That includes:</p> <ul> <li>preprocessing fitted artifacts (per-step <code>params</code>)</li> <li>the trained model (sklearn estimator object)</li> </ul>"},{"location":"user_guide/serialization.html#practical-guidance","title":"Practical guidance","text":"<ul> <li>Prefer saving in environments where the same library versions are available.</li> <li>Some preprocessing nodes store sklearn objects inside <code>params</code> (e.g., KNN/Iterative imputers, OneHotEncoder).   Those are not JSON-serializable and require pickling.</li> </ul>"},{"location":"user_guide/serialization.html#load-and-use","title":"Load and use","text":"<pre><code>from __future__ import annotations\n\nimport tempfile\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom skyulf.pipeline import SkyulfPipeline\n\ndf = pd.DataFrame(\n  {\n    \"age\": [10, 20, None, 40, 50, 60, None, 80],\n    \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n    \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n  }\n)\n\nconfig = {\n  \"preprocessing\": [\n    {\n      \"name\": \"split\",\n      \"transformer\": \"TrainTestSplitter\",\n      \"params\": {\n        \"test_size\": 0.2,\n        \"validation_size\": 0.0,\n        \"random_state\": 42,\n        \"shuffle\": True,\n        \"stratify\": True,\n        \"target_column\": \"target\",\n      },\n    },\n    {\n      \"name\": \"impute\",\n      \"transformer\": \"SimpleImputer\",\n      \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n    },\n    {\n      \"name\": \"encode\",\n      \"transformer\": \"OneHotEncoder\",\n      \"params\": {\"columns\": [\"city\"], \"drop_original\": True},\n    },\n  ],\n  \"modeling\": {\n    \"type\": \"random_forest_classifier\",\n    \"params\": {\"n_estimators\": 50, \"random_state\": 42},\n  },\n}\n\nwith tempfile.TemporaryDirectory() as tmp:\n  model_path = Path(tmp) / \"model.pkl\"\n\n  pipeline = SkyulfPipeline(config)\n  _ = pipeline.fit(df, target_column=\"target\")\n  pipeline.save(model_path)\n\n  loaded = SkyulfPipeline.load(model_path)\n  new_df = pd.DataFrame({\"age\": [25, None], \"city\": [\"A\", \"C\"]})\n  preds = loaded.predict(new_df)\n\nprint(preds)\n</code></pre>"},{"location":"user_guide/splitdataset_and_leakage.html","title":"SplitDataset &amp; Leakage","text":""},{"location":"user_guide/splitdataset_and_leakage.html#why-splitdataset-exists","title":"Why <code>SplitDataset</code> exists","text":"<p>Many preprocessing nodes learn statistics from data (means, categories, bin edges, \u2026). If those statistics are computed on the full dataset and then evaluated on test data, you leak information.</p> <p><code>SplitDataset</code> is a container for:</p> <ul> <li><code>train</code></li> <li><code>test</code></li> <li>optional <code>validation</code></li> </ul> <p>Each split can be either:</p> <ul> <li>a <code>pd.DataFrame</code> (with the target column inside), or</li> <li>a tuple <code>(X: pd.DataFrame, y: pd.Series)</code>.</li> </ul>"},{"location":"user_guide/splitdataset_and_leakage.html#recommended-patterns","title":"Recommended patterns","text":""},{"location":"user_guide/splitdataset_and_leakage.html#pattern-a-split-in-preprocessing","title":"Pattern A: split in preprocessing","text":"<p>Use the <code>TrainTestSplitter</code> transformer early.</p> <pre><code>{\n  \"name\": \"split\",\n  \"transformer\": \"TrainTestSplitter\",\n  \"params\": {\"test_size\": 0.2, \"random_state\": 42, \"target_column\": \"target\"}\n}\n</code></pre>"},{"location":"user_guide/splitdataset_and_leakage.html#pattern-b-create-splitdataset-yourself","title":"Pattern B: create <code>SplitDataset</code> yourself","text":"<pre><code>from __future__ import annotations\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom skyulf.data.dataset import SplitDataset\nfrom skyulf.pipeline import SkyulfPipeline\n\ndf = pd.DataFrame(\n  {\n    \"age\": [10, 20, None, 40, 50, 60, None, 80],\n    \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n    \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n  }\n)\n\nconfig = {\n  \"preprocessing\": [\n    {\n      \"name\": \"impute\",\n      \"transformer\": \"SimpleImputer\",\n      \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n    },\n    {\n      \"name\": \"encode\",\n      \"transformer\": \"OneHotEncoder\",\n      \"params\": {\"columns\": [\"city\"], \"drop_original\": True},\n    },\n  ],\n  \"modeling\": {\n    \"type\": \"random_forest_classifier\",\n    \"params\": {\"n_estimators\": 50, \"random_state\": 42},\n  },\n}\ntrain, test = train_test_split(df, test_size=0.2, random_state=42)\n\ndataset = SplitDataset(train=train, test=test, validation=None)\n\npipeline = SkyulfPipeline(config)\nmetrics = pipeline.fit(dataset, target_column=\"target\")\n\nprint(metrics)\n</code></pre>"},{"location":"user_guide/splitdataset_and_leakage.html#notes-on-inference","title":"Notes on inference","text":"<p>At inference time, <code>FeatureEngineer.transform()</code> skips splitters and resampling steps. That ensures your <code>predict()</code> path remains deterministic.</p>"},{"location":"user_guide/step_by_step_no_config.html","title":"Step-by-Step (No Config)","text":"<p>This tutorial runs an end-to-end workflow without building a pipeline <code>config</code> dictionary. Instead, it uses Skyulf\u2019s low-level building blocks directly:</p> <ul> <li>preprocessing Calculators and Appliers</li> <li><code>SplitDataset</code> to avoid data leakage</li> <li>modeling via <code>StatefulEstimator</code></li> </ul> <p>Use this approach when you want maximum transparency, debugging control, or custom orchestration.</p>"},{"location":"user_guide/step_by_step_no_config.html#what-you-will-build","title":"What you will build","text":"<ol> <li>Split a DataFrame into train/test using <code>TrainTestSplitter</code></li> <li>Apply a few preprocessing steps (imputation + encoding)</li> <li>Train a model and generate predictions</li> </ol>"},{"location":"user_guide/step_by_step_no_config.html#runnable-example","title":"Runnable example","text":"<p>This single snippet is fully self-contained and can be pasted into a Python session as-is.</p> <pre><code>from __future__ import annotations\n\nimport pandas as pd\n\nfrom skyulf.data.dataset import SplitDataset\nfrom skyulf.modeling.base import StatefulEstimator\nfrom skyulf.modeling.classification import (\n    RandomForestClassifierApplier,\n    RandomForestClassifierCalculator,\n)\nfrom skyulf.preprocessing.encoding import OneHotEncoderApplier, OneHotEncoderCalculator\nfrom skyulf.preprocessing.imputation import SimpleImputerApplier, SimpleImputerCalculator\nfrom skyulf.preprocessing.split import (\n    FeatureTargetSplitApplier,\n    FeatureTargetSplitCalculator,\n    SplitApplier,\n    SplitCalculator,\n)\n\n# 0) Setup data\ndf = pd.DataFrame(\n    {\n        \"age\": [10, 20, None, 40, 50, None, 70, 80],\n        \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n        \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n    }\n)\n\n# 1) Split the dataset (no leakage)\nsplit_params = {\n    \"test_size\": 0.25,\n    \"validation_size\": 0.0,\n    \"random_state\": 42,\n    \"shuffle\": True,\n    \"stratify\": True,\n    \"target_column\": \"target\",\n}\n\ndataset = SplitApplier().apply(df, SplitCalculator().fit(df, split_params))\n\n# 2) Convert DataFrames to (X, y) tuples\nfts_params = {\"target_column\": \"target\"}\ndataset_xy = FeatureTargetSplitApplier().apply(\n    dataset, FeatureTargetSplitCalculator().fit(dataset, fts_params)\n)\n\nX_train, y_train = dataset_xy.train\nX_test, y_test = dataset_xy.test\n\n# 3) Fit preprocessing on train, apply to test\nimp_cfg = {\"columns\": [\"age\"], \"strategy\": \"mean\"}\nimp_params = SimpleImputerCalculator().fit((X_train, y_train), imp_cfg)\nX_train_imp, y_train = SimpleImputerApplier().apply((X_train, y_train), imp_params)\nX_test_imp, y_test = SimpleImputerApplier().apply((X_test, y_test), imp_params)\n\nohe_cfg = {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"}\nohe_params = OneHotEncoderCalculator().fit((X_train_imp, y_train), ohe_cfg)\nX_train_fe, y_train = OneHotEncoderApplier().apply((X_train_imp, y_train), ohe_params)\nX_test_fe, y_test = OneHotEncoderApplier().apply((X_test_imp, y_test), ohe_params)\n\n# 4) Train a model and predict\nestimator = StatefulEstimator(\n    calculator=RandomForestClassifierCalculator(),\n    applier=RandomForestClassifierApplier(),\n    node_id=\"rf_model\",\n)\n\ndataset_for_model = SplitDataset(\n    train=(X_train_fe, y_train),\n    test=(X_test_fe, y_test),\n    validation=None,\n)\n\nmodel_cfg = {\"params\": {\"n_estimators\": 50, \"random_state\": 42}}\npreds = estimator.fit_predict(dataset=dataset_for_model, target_column=\"target\", config=model_cfg)\n\nprint(\"Train preds:\")\nprint(preds[\"train\"].head())\nprint(\"Test preds:\")\nprint(preds.get(\"test\", pd.Series(dtype=float)).head())\n</code></pre>"},{"location":"user_guide/step_by_step_no_config.html#5-summary-what-to-remember","title":"5) Summary: what to remember","text":"<ul> <li>Split first (or provide a <code>SplitDataset</code>) to prevent leakage.</li> <li>Fit preprocessing on train, reuse the learned <code>params</code> for test/inference.</li> <li>Modeling can be driven directly through <code>StatefulEstimator</code> when you don\u2019t want a pipeline config.</li> </ul>"},{"location":"user_guide/troubleshooting.html","title":"Troubleshooting","text":""},{"location":"user_guide/troubleshooting.html#unknown-transformer-type","title":"\u201cUnknown transformer type\u201d","text":"<p>This means your step\u2019s <code>transformer</code> string does not match the <code>FeatureEngineer</code> dispatcher. Check spelling and casing.</p>"},{"location":"user_guide/troubleshooting.html#resampling-errors-about-non-numeric-columns","title":"Resampling errors about non-numeric columns","text":"<p>Oversampling/undersampling require numeric features. Apply an encoder first (e.g., OneHotEncoder / OrdinalEncoder) before resampling.</p>"},{"location":"user_guide/troubleshooting.html#pickle-loading-errors","title":"Pickle loading errors","text":"<p>If <code>SkyulfPipeline.load()</code> fails, ensure you are using compatible versions of:</p> <ul> <li>Python</li> <li>scikit-learn</li> <li>pandas</li> </ul> <p>If you need portability across environments, consider constraining versions.</p>"},{"location":"user_guide/validation_vs_sklearn.html","title":"Validation vs scikit-learn (Proof)","text":"<p>This page gives reproducible, runnable checks that:</p> <ul> <li>Skyulf supports the familiar scikit-learn workflow (build <code>X</code>/<code>y</code>, run <code>train_test_split</code>, fit on train, transform/predict on test).</li> <li>Skyulf avoids common forms of data leakage by learning preprocessing parameters from train only.</li> </ul> <p>Goal: show verifiable behavior, not claim bit-for-bit identical models.</p>"},{"location":"user_guide/validation_vs_sklearn.html#1-scikit-learn-style-workflow-xy-train_test_split","title":"1) scikit-learn-style workflow (X/y + train_test_split)","text":"<p>This mirrors the sklearn pattern:</p> <ul> <li>sklearn: <code>fit(X_train, y_train)</code> then <code>predict(X_test)</code></li> <li>Skyulf: pass <code>SplitDataset(train=(X_train, y_train), test=(X_test, y_test))</code></li> </ul> <pre><code>from __future__ import annotations\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom skyulf.data.dataset import SplitDataset\nfrom skyulf.pipeline import SkyulfPipeline\n\n# Synthetic classification data\nraw = pd.DataFrame(\n    {\n        \"age\": [10, 20, None, 40, 50, 60, None, 80],\n        \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\"],\n        \"target\": [0, 1, 0, 1, 1, 0, 1, 0],\n    }\n)\n\nX = raw.drop(columns=[\"target\"])\ny = raw[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\ndataset = SplitDataset(train=(X_train, y_train), test=(X_test, y_test), validation=None)\n\n# IMPORTANT: because we already split, we do not add TrainTestSplitter here.\nconfig = {\n    \"preprocessing\": [\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\"]},\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": [\"city\"], \"drop_original\": True, \"handle_unknown\": \"ignore\"},\n        },\n    ],\n    \"modeling\": {\"type\": \"random_forest_classifier\", \"params\": {\"n_estimators\": 50, \"random_state\": 42}},\n}\n\npipeline = SkyulfPipeline(config)\nmetrics = pipeline.fit(dataset, target_column=\"target\")  # target_column ignored for (X, y) tuples\n\npreds = pipeline.predict(X_test)\n\n# Proof-like checks\nassert len(preds) == len(X_test)\nassert preds.index.equals(X_test.index)\nprint(\"OK: Skyulf fit/predict with sklearn-style train/test split\")\nprint(\"Metrics keys:\", list(metrics.keys()))\n</code></pre>"},{"location":"user_guide/validation_vs_sklearn.html#1b-side-by-side-run-sklearn-pipeline-vs-skyulfpipeline","title":"1b) Side-by-side run: sklearn Pipeline vs SkyulfPipeline","text":"<p>This comparison proves both stacks can run the same shape of workflow on the same split. We do not assert equality of predictions (different defaults / encodings can legitimately differ).</p> <p>For a stronger numeric sanity check, we also compute and print test accuracy for both pipelines and the absolute difference.</p> <pre><code>from __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.impute import SimpleImputer as SkSimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline as SkPipeline\nfrom sklearn.preprocessing import OneHotEncoder as SkOneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom skyulf import SkyulfPipeline\nfrom skyulf.data.dataset import SplitDataset\n\nnp.random.seed(42)\n\n# Real dataset + extra categorical feature + missingness\nraw = load_breast_cancer(as_frame=True)\ndf = raw.frame.copy().rename(columns={\"target\": \"label\"})\n\ndf[\"radius_band\"] = pd.cut(\n    df[\"mean radius\"],\n    bins=[0, 12, 15, 100],\n    labels=[\"small\", \"medium\", \"large\"],\n    include_lowest=True,\n)\n\nmissing_idx = np.random.choice(df.index, size=25, replace=False)\ndf.loc[missing_idx, \"mean texture\"] = np.nan\n\ntarget_col = \"label\"\ncat_cols = [\"radius_band\"]\nnum_cols = [c for c in df.columns if c not in [target_col, *cat_cols]]\n\nX = df[num_cols + cat_cols]\ny = df[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# --- scikit-learn pipeline ---\nnumeric_features = num_cols\ncategorical_features = cat_cols\n\nnumeric_pipe = SkPipeline(\n    steps=[\n        (\"imputer\", SkSimpleImputer(strategy=\"mean\")),\n        (\"scaler\", StandardScaler()),\n    ]\n)\n\ncategorical_pipe = SkPipeline(\n    steps=[\n        (\"imputer\", SkSimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", SkOneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n    ]\n)\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipe, numeric_features),\n        (\"cat\", categorical_pipe, categorical_features),\n    ],\n    remainder=\"drop\",\n)\n\nsk_model = LogisticRegression(max_iter=1000, random_state=42)\nsk = SkPipeline(steps=[(\"preprocess\", preprocess), (\"model\", sk_model)])\nsk.fit(X_train, y_train)\nsk_preds = pd.Series(sk.predict(X_test), index=X_test.index)\nsk_acc = accuracy_score(y_test, sk_preds)\n\n# --- Skyulf pipeline ---\ntrain_df = X_train.copy()\ntrain_df[target_col] = y_train\ntest_df = X_test.copy()\ntest_df[target_col] = y_test\n\ndataset = SplitDataset(train=train_df, test=test_df, validation=None)\nskyulf_config = {\n    \"preprocessing\": [\n        {\n            \"name\": \"impute\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"mean\", \"columns\": num_cols},\n        },\n        {\n            \"name\": \"impute_cat\",\n            \"transformer\": \"SimpleImputer\",\n            \"params\": {\"strategy\": \"most_frequent\", \"columns\": cat_cols},\n        },\n        {\n            \"name\": \"encode\",\n            \"transformer\": \"OneHotEncoder\",\n            \"params\": {\"columns\": cat_cols, \"drop_original\": True, \"handle_unknown\": \"ignore\"},\n        },\n        {\n            \"name\": \"scale\",\n            \"transformer\": \"StandardScaler\",\n            \"params\": {\"columns\": num_cols},\n        },\n    ],\n    \"modeling\": {\"type\": \"logistic_regression\", \"params\": {\"max_iter\": 1000, \"random_state\": 42}},\n}\n\nsky = SkyulfPipeline(skyulf_config)\n_ = sky.fit(dataset, target_column=target_col)\nsky_preds = sky.predict(X_test)\nsky_acc = accuracy_score(y_test, sky_preds)\ndelta = abs(sk_acc - sky_acc)\n\nassert sk_preds.index.equals(X_test.index)\nassert sky_preds.index.equals(X_test.index)\n\n# Proof-like checks\nassert len(sk_preds) == len(X_test)\nassert sk_preds.index.equals(X_test.index)\nassert len(sky_preds) == len(X_test)\nassert sky_preds.index.equals(X_test.index)\n\nprint(\"OK: sklearn Pipeline and SkyulfPipeline both run\")\nprint(f\"sklearn test accuracy: {sk_acc:.4f}\")\nprint(f\"skyulf  test accuracy: {sky_acc:.4f}\")\nprint(f\"delta accuracy: {delta:.4f}\")\n\n# --- Classification metrics (side-by-side) ---\nsk_report = classification_report(y_test, sk_preds, output_dict=True, zero_division=0)\nsky_report = classification_report(y_test, sky_preds, output_dict=True, zero_division=0)\n\nsk_df = pd.DataFrame(sk_report).T\nsky_df = pd.DataFrame(sky_report).T\n\n# Keep a consistent row order: class labels first, then summary rows (if present)\nlabel_rows = [str(v) for v in sorted(pd.unique(y_test))]\nsummary_rows = [r for r in [\"accuracy\", \"macro avg\", \"weighted avg\"] if r in sk_df.index]\nrow_order = [r for r in label_rows if r in sk_df.index] + summary_rows\n\nsk_df = sk_df.loc[row_order]\nsky_df = sky_df.loc[row_order]\n\nside_by_side = pd.concat(\n    {\n        \"sklearn\": sk_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]],\n        \"skyulf\": sky_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]],\n    },\n    axis=1,\n)\n\nprint(\"\\nClassification report (side-by-side):\")\nprint(side_by_side.to_string())\n\nlabels = sorted(pd.unique(y_test))\ncm_sk = confusion_matrix(y_test, sk_preds, labels=labels)\ncm_sky = confusion_matrix(y_test, sky_preds, labels=labels)\n\ncm_index = [f\"true_{l}\" for l in labels]\ncm_cols = [f\"pred_{l}\" for l in labels]\n\ncm_sk_df = pd.DataFrame(cm_sk, index=cm_index, columns=cm_cols)\ncm_sky_df = pd.DataFrame(cm_sky, index=cm_index, columns=cm_cols)\n\nprint(\"\\nConfusion matrix (sklearn):\")\nprint(cm_sk_df.to_string())\nprint(\"\\nConfusion matrix (skyulf):\")\nprint(cm_sky_df.to_string())\n\nprint(\"sklearn preds head:\")\nprint(sk_preds.head())\nprint(\"skyulf preds head:\")\nprint(sky_preds.head())\n\nassert (sk_pred.values == sky_pred.values).all()\nprint(\"Predictions match exactly.\")\n</code></pre>"},{"location":"user_guide/validation_vs_sklearn.html#example-output-from-the-notebook","title":"Example output (from the notebook)","text":"<p>This is the exact output from one notebook run (same dataset, same random seed/split):</p> <p>Classification report (side-by-side):</p> class/avg sklearn precision sklearn recall sklearn f1-score sklearn support skyulf precision skyulf recall skyulf f1-score skyulf support 0 0.962963 0.981132 0.971963 53.000000 0.962963 0.981132 0.971963 53.000000 1 0.988764 0.977778 0.983240 90.000000 0.988764 0.977778 0.983240 90.000000 accuracy 0.979021 0.979021 0.979021 0.979021 0.979021 0.979021 0.979021 0.979021 macro avg 0.975864 0.979455 0.977601 143.000000 0.975864 0.979455 0.977601 143.000000 weighted avg 0.979201 0.979021 0.979060 143.000000 0.979201 0.979021 0.979060 143.000000 <p>Confusion matrix (sklearn):</p> pred_0 pred_1 true_0 52 1 true_1 2 88 <p>Confusion matrix (skyulf):</p> pred_0 pred_1 true_0 52 1 true_1 2 88"},{"location":"user_guide/validation_vs_sklearn.html#2-proof-of-leakage-prevention-train-only-learned-params","title":"2) Proof of leakage prevention (train-only learned params)","text":"<p>A common leakage bug is fitting preprocessing on the full dataset.</p> <p>Here we construct a dataset where train and test have very different distributions. If an imputer learns the mean from the full dataset, it will be pulled toward the test distribution.</p> <p>Skyulf\u2019s pattern learns from train only (Calculator) and applies to test (Applier).</p> <pre><code>from __future__ import annotations\n\nimport pandas as pd\n\nfrom skyulf.preprocessing.imputation import SimpleImputerCalculator\n\n# Train has small ages; test has huge ages.\nX_train = pd.DataFrame({\"age\": [1.0, 2.0, None, 2.0]})\ny_train = pd.Series([0, 1, 0, 1])\n\nX_test = pd.DataFrame({\"age\": [1000.0, None, 1200.0]})\ny_test = pd.Series([0, 1, 1])\n\ncfg = {\"strategy\": \"mean\", \"columns\": [\"age\"]}\n\n# What train-only mean should be (ignoring NaNs)\nexpected_train_mean = float(pd.Series([1.0, 2.0, 2.0]).mean())\n\nparams = SimpleImputerCalculator().fit((X_train, y_train), cfg)\nlearned_mean = float(params[\"fill_values\"][\"age\"])\n\n# Proof: learned mean equals train mean (not influenced by test)\nassert abs(learned_mean - expected_train_mean) &lt; 1e-12\n\n# For comparison only: full-data mean would be very different\nfull_mean = float(pd.concat([X_train[\"age\"], X_test[\"age\"]]).mean())\nassert abs(full_mean - expected_train_mean) &gt; 1.0\n\nprint(\"OK: SimpleImputer learns from train only\")\nprint(\"train_mean:\", expected_train_mean)\nprint(\"full_mean:\", full_mean)\nprint(\"learned_mean:\", learned_mean)\n</code></pre>"},{"location":"user_guide/validation_vs_sklearn.html#3-what-this-proves-and-what-it-doesnt","title":"3) What this proves (and what it doesn\u2019t)","text":"<ul> <li>Proves the API supports sklearn-style <code>X/y</code> workflows and produces aligned predictions.</li> <li>Proves at least one common leakage-sensitive node (<code>SimpleImputer</code>) learns its statistics from the provided training data.</li> </ul> <p>It does not claim Skyulf will produce identical predictions to an arbitrary sklearn pipeline, because:</p> <ul> <li>different defaults/hyperparameters,</li> <li>different encoding conventions,</li> <li>and different ordering of operations</li> </ul> <p>can all change results while still being correct.</p>"}]}