{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Skyulf \ud83d\udc3a","text":"<p>Machine Learning Operations (MLOps) shouldn't be this hard.</p> <p>Skyulf is a self-hosted, privacy-first MLOps Hub. It is designed to be the \"glue\" that holds your data science workflow together\u2014without the glue code. Bring your data, clean it visually, engineer features with a node-based canvas, and train models, all in one place.</p> <p>Built with a modern stack: FastAPI (Backend), React (Frontend), Celery (Async Jobs), and Redis.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83c\udfa8 Visual Feature Canvas: A node-based editor to clean, transform, and engineer features without writing spaghetti code.</li> <li>\ud83d\ude80 Modern Backend: Built on FastAPI for high performance and easy API extension.</li> <li>\u26a1 Async by Default: Heavy training jobs run in the background via Celery &amp; Redis (or background threads)\u2014your UI never freezes.</li> <li>\ud83d\udcbe Flexible Data: Ingest CSV, Excel, JSON, Parquet, or SQL. Start with SQLite (zero-config) and scale to PostgreSQL.</li> <li>\ud83e\udde0 Model Training: Built-in support for Scikit-Learn models with hyperparameter search (Grid/Random/Halving) and optional Optuna integration.</li> <li>\ud83d\udce6 Model Registry &amp; Deployment: Version control your models, track metrics, and deploy them to a live inference API with a single click.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>Prerequisites: Python 3.10+</p> <pre><code>pip install -r requirements-fastapi.txt\n</code></pre>"},{"location":"#running-the-server","title":"Running the Server","text":"<pre><code>python run_skyulf.py\n</code></pre> <p>The server will start at <code>http://127.0.0.1:8000</code>.</p>"},{"location":"#running-without-celery-direct-mode","title":"Running Without Celery (Direct Mode)","text":"<p>By default, Skyulf runs without Celery for simplicity. Background tasks execute directly in the FastAPI process using background threads. This is perfect for development and small-scale deployments.</p> <p>To enable Celery for production workloads:</p> <ol> <li>Set the environment variable:    ```bash    # Windows    set USE_CELERY=true</li> </ol> <p># Linux/Mac    export USE_CELERY=true    ```</p> <ol> <li> <p>Start Redis (required for Celery):    <code>bash    docker run -d -p 6379:6379 redis:alpine</code></p> </li> <li> <p>Start the Celery worker:    <code>bash    celery -A celery_worker worker --loglevel=info</code></p> </li> <li> <p>Start the FastAPI server:    <code>bash    python run_skyulf.py</code></p> </li> </ol> <p>Tip: For local development, keep <code>USE_CELERY=false</code> (default) to avoid needing Redis.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>User Guide:</li> <li>Getting Started: Installation and basic usage.</li> <li>Common Recipes: How-to guides for common tasks.</li> <li>Python API Usage: Programmatic usage of backend modules.</li> <li>Architecture: System design and responsibilities.</li> <li>Experiments: Tracking and managing experiments.</li> <li>Model Registry: Managing model versions.</li> <li>Inference: Deploying models for inference.</li> <li>Modeling:</li> <li>Overview: Training and applying models.</li> <li>Classification: Classification algorithms.</li> <li>Regression: Regression algorithms.</li> <li>Training: Model training process.</li> <li>Tuning: Hyperparameter tuning.</li> <li>Preprocessing:</li> <li>Cleaning: Data cleaning operations.</li> <li>Feature Engineering: Creating new features.</li> <li>Transformations: Data transformations.</li> </ul>"},{"location":"#building-deploying-documentation","title":"Building &amp; Deploying Documentation","text":"<p>This documentation is built with MkDocs and the Material for MkDocs theme, with auto-generated API docs via mkdocstrings.</p>"},{"location":"#local-development","title":"Local Development","text":"<ol> <li> <p>Install dependencies: <code>bash    pip install mkdocs-material mkdocstrings[python]</code></p> </li> <li> <p>Serve locally with hot-reload: <code>bash    mkdocs serve</code>    Open <code>http://127.0.0.1:8000</code> in your browser.</p> </li> <li> <p>Build static files: <code>bash    mkdocs build</code>    This generates the <code>site/</code> directory with static HTML files.</p> </li> </ol>"},{"location":"#deploying-to-github-pages","title":"Deploying to GitHub Pages","text":"<p>The documentation is automatically deployed to GitHub Pages on every push to <code>main</code>, <code>master</code>, or <code>creatingnewversion</code> branches via GitHub Actions.</p> <p>Manual deployment:</p> <pre><code>mkdocs gh-deploy --force\n</code></pre> <p>This pushes the built docs to the <code>gh-pages</code> branch and makes them available at: <code>https://flyingriverhorse.github.io/Skyulf/site/</code></p>"},{"location":"#accessing-via-indexhtml","title":"Accessing via index.html","text":"<p>Once deployed, the site is accessible through: - Landing Page: <code>https://flyingriverhorse.github.io/Skyulf/</code> \u2192 Root <code>index.html</code> - Documentation: <code>https://flyingriverhorse.github.io/Skyulf/site/</code> \u2192 MkDocs site</p> <p>The <code>index.html</code> file is automatically generated by MkDocs and serves as the entry point. All navigation, search, and page routing work through this single-page application style setup.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Skyulf is built on a modular architecture designed for scalability and flexibility.</p>"},{"location":"architecture/#high-level-overview","title":"High-Level Overview","text":"<p>The system consists of three main components:</p> <ol> <li>Frontend (Feature Canvas): A React + Vite Single Page Application (SPA) that provides the visual interface for data cleaning and feature engineering.</li> <li>Backend (API): A FastAPI application that handles data ingestion, pipeline execution, and model management.</li> <li>Async Worker: A Celery worker (backed by Redis) that executes long-running tasks like model training and hyperparameter tuning.</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Frontend (React)                        \u2502\n\u2502                    Visual Feature Canvas UI                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     FastAPI Backend                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   Data      \u2502  \u2502  ML Pipeline \u2502  \u2502    Model Registry      \u2502  \u2502\n\u2502  \u2502  Ingestion  \u2502  \u2502   Engine     \u2502  \u2502    &amp; Deployment        \u2502  \u2502\n\u2502  \u2502  (Polars)   \u2502  \u2502  (Pandas)    \u2502  \u2502                        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u25bc                       \u25bc\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502   SQLite/    \u2502        \u2502    Redis     \u2502\n           \u2502  PostgreSQL  \u2502        \u2502   + Celery   \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#data-flow-polars-vs-pandas","title":"Data Flow: Polars vs Pandas","text":"<p>Skyulf uses two data libraries for optimal performance:</p> Stage Library Why Data Ingestion Polars Fast I/O, lazy evaluation, low memory footprint ML Pipeline Pandas Scikit-learn compatibility, rich ML ecosystem <p>Data automatically converts from Polars \u2192 Pandas when moving from ingestion to preprocessing.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-data-ingestion-coredata_ingestion","title":"1. Data Ingestion (<code>core.data_ingestion</code>)","text":"<p>Handles loading datasets from various sources using Polars: *   Connectors: <code>LocalFileConnector</code> (CSV, Excel, Parquet, JSON), <code>DatabaseConnector</code> (SQL), <code>ApiConnector</code> (REST) *   Profiler: <code>DataProfiler</code> computes statistics for schema discovery *   Service: <code>DataIngestionService</code> orchestrates uploads and background ingestion</p>"},{"location":"architecture/#2-ml-pipeline-data-skyulfdata","title":"2. ML Pipeline Data (<code>skyulf.data</code>)","text":"<p>Provided by the Skyulf Core library (<code>skyulf-core</code>, imported as <code>skyulf</code>): *   SplitDataset: Container for train/test/validation splits</p>"},{"location":"architecture/#3-feature-engineering-skyulfpreprocessing","title":"3. Feature Engineering (<code>skyulf.preprocessing</code>)","text":"<p>Implements the \"Calculator/Applier\" pattern: *   Calculator: Computes statistics (e.g., mean, std, vocabulary) from training data *   Applier: Applies computed statistics to new data (inference) in a stateless manner</p>"},{"location":"architecture/#4-execution-engine-coreml_pipelineexecution","title":"4. Execution Engine (<code>core.ml_pipeline.execution</code>)","text":"<p>Orchestrates pipeline execution: *   PipelineEngine: Runs the DAG of nodes (data_loader \u2192 feature_engineering \u2192 model_training) *   JobManager: Tracks job status, progress, and cancellation</p>"},{"location":"architecture/#5-model-registry-coreml_pipelinemodel_registry","title":"5. Model Registry (<code>core.ml_pipeline.model_registry</code>)","text":"<p>Manages model versions and lineage: *   Tracks which job produced which model *   Handles version incrementing per dataset/model type</p>"},{"location":"architecture/#6-artifact-store-coreml_pipelineartifacts","title":"6. Artifact Store (<code>core.ml_pipeline.artifacts</code>)","text":"<p>Abstracts storage of model binaries and fitted transformers: *   <code>LocalArtifactStore</code>: Filesystem storage (joblib serialization) *   Extensible to S3/Azure Blob for production deployments</p>"},{"location":"architecture/#7-deployment-coreml_pipelinedeployment","title":"7. Deployment (<code>core.ml_pipeline.deployment</code>)","text":"<p>Handles model serving: *   <code>DeploymentService</code>: Deploys models and handles predictions *   <code>APPLIER_MAP</code>: Registry of all transformer appliers for pipeline reconstruction</p>"},{"location":"architecture/#8-recommendations-coreml_pipelinerecommendations","title":"8. Recommendations (<code>core.ml_pipeline.recommendations</code>)","text":"<p>AI-powered preprocessing suggestions: *   <code>AdvisorEngine</code>: Analyzes data profiles and suggests transformations *   Plugins: CleaningAdvisor, ImputationAdvisor, ScalingAdvisor, etc.</p>"},{"location":"architecture/#database-schema","title":"Database Schema","text":"<p>Skyulf uses SQLAlchemy (async) with Alembic for migrations. Core entities:</p> Entity Description <code>DataSource</code> Uploaded/connected data with metadata and profiling results <code>TrainingJob</code> Training run with parameters, metrics, and artifact URI <code>HyperparameterTuningJob</code> Tuning job with search space and best results <code>Deployment</code> Active/historical model deployments <code>User</code> User accounts (optional authentication)"},{"location":"architecture/#file-structure","title":"File Structure","text":"<pre><code>core/\n\u251c\u2500\u2500 data_ingestion/          # Polars-based data loading\n\u2502   \u251c\u2500\u2500 connectors/          # File, SQL, API connectors\n\u2502   \u251c\u2500\u2500 engine/              # Profiler\n\u2502   \u2514\u2500\u2500 service.py           # Orchestration\n\u2502\n\u251c\u2500\u2500 ml_pipeline/             # Backend orchestration (uses Skyulf Core)\n\u2502   \u251c\u2500\u2500 execution/           # PipelineEngine, JobManager\n\u2502   \u251c\u2500\u2500 deployment/          # DeploymentService\n\u2502   \u251c\u2500\u2500 artifacts/           # ArtifactStore\n\u2502   \u251c\u2500\u2500 model_registry/      # Version management\n\u2502   \u2514\u2500\u2500 recommendations/     # AI suggestions\n\u2502\n\u2514\u2500\u2500 database/                # SQLAlchemy models\n\nskyulf-core/\n\u2514\u2500\u2500 skyulf/                  # Standalone library (preprocessing/modeling/pipeline)\n    \u251c\u2500\u2500 data/\n    \u251c\u2500\u2500 preprocessing/\n    \u251c\u2500\u2500 modeling/\n    \u2514\u2500\u2500 pipeline.py\n</code></pre>"},{"location":"python_api/","title":"Backend Python API Guide (<code>core.*</code>)","text":"<p>This guide documents Skyulf's backend Python modules (the <code>core.*</code> package) that power the FastAPI server and Celery worker.</p> <p>If you want the standalone Python library (imported as <code>skyulf</code>), see the Skyulf Core docs under the API Reference section.</p>"},{"location":"python_api/#1-environment-setup","title":"1. Environment Setup","text":"<p>Ensure you have the necessary dependencies installed and your environment configured.</p> <pre><code>pip install -r requirements-fastapi.txt\n</code></pre> <pre><code>import os\nimport sys\n\n# Ensure the repo root is on your PYTHONPATH so `import core` works.\nsys.path.append(os.getcwd())\n</code></pre>"},{"location":"python_api/#2-data-ingestion","title":"2. Data Ingestion","text":"<p>The first step is loading your data. Skyulf provides a <code>DataLoader</code> that handles various formats efficiently.</p> <pre><code>from core.ml_pipeline.execution.engine import DataLoader\nimport pandas as pd\n\n# Initialize Loader\nloader = DataLoader()\n\n# Load Data (Supports CSV and Parquet)\n# For this example, we assume data.csv exists\ndf = loader.load_full(\"data.csv\")\n\n# Inspect Data\nprint(f\"Loaded {len(df)} rows\")\nprint(df.head())\n</code></pre>"},{"location":"python_api/#3-defining-the-pipeline","title":"3. Defining the Pipeline","text":"<p>Pipelines are defined using a configuration object (<code>PipelineConfig</code>). This configuration describes the sequence of steps (nodes) to execute.</p> <pre><code>from core.ml_pipeline.execution.schemas import PipelineConfig, NodeConfig\n\npipeline_config = PipelineConfig(\n    pipeline_id=\"my_pipeline_01\",\n    nodes=[\n        # Node 1: Data Loader\n        NodeConfig(\n            node_id=\"data_loader\",\n            step_type=\"data_loader\",\n            params={\n                \"path\": \"data.csv\",\n                \"type\": \"csv\"\n            }\n        ),\n        # Node 2: Feature Engineering\n        NodeConfig(\n            node_id=\"feature_engineering\",\n            step_type=\"feature_engineering\",\n            inputs=[\"data_loader\"],\n            params={\n                \"steps\": [\n                    {\n                        \"name\": \"impute_age\",\n                        \"transformer\": \"SimpleImputer\",\n                        \"params\": {\n                            \"columns\": [\"age\"],\n                            \"strategy\": \"mean\"\n                        }\n                    },\n                    {\n                        \"name\": \"encode_city\",\n                        \"transformer\": \"OneHotEncoder\",\n                        \"params\": {\n                            \"columns\": [\"city\"]\n                        }\n                    }\n                ]\n            }\n        ),\n        # Node 3: Model Training\n        NodeConfig(\n            node_id=\"model_training\",\n            step_type=\"model_training\",\n            inputs=[\"feature_engineering\"],\n            params={\n                \"algorithm\": \"random_forest_classifier\",\n                \"target_column\": \"target\",\n                \"params\": {\n                    \"n_estimators\": 50\n                }\n            }\n        )\n    ]\n)\n</code></pre>"},{"location":"python_api/#4-executing-the-pipeline","title":"4. Executing the Pipeline","text":"<p>The <code>PipelineEngine</code> orchestrates the execution. It takes the config, runs each step, and stores the results.</p> <pre><code>from core.ml_pipeline.execution.engine import PipelineEngine\nfrom core.ml_pipeline.artifacts.local import LocalArtifactStore\n\n# 1. Setup Artifact Store\n# This is where trained models and preprocessors are saved\nartifact_store = LocalArtifactStore(\"./my_artifacts\")\n\n# 2. Initialize Engine\nengine = PipelineEngine(artifact_store=artifact_store)\n\n# 3. Run Pipeline\nresult = engine.run(pipeline_config, job_id=\"manual_run_001\")\n\nprint(f\"Status: {result.status}\")\n# Check node results\nfor node_id, res in result.node_results.items():\n    print(f\"Node {node_id}: {res.status}\")\n</code></pre>"},{"location":"python_api/#5-inference-making-predictions","title":"5. Inference (Making Predictions)","text":"<p>Once a pipeline is trained, you can use the saved artifacts to make predictions on new data. Note that Skyulf uses a Stateless Applier pattern. You load the parameters (artifacts) and pass them to an Applier instance.</p> <pre><code>from skyulf.preprocessing.imputation import SimpleImputerApplier\nfrom skyulf.preprocessing.encoding import OneHotEncoderApplier\nfrom skyulf.modeling.classification import RandomForestClassifierApplier\n\n# 1. Load Artifacts (Parameters)\n# The engine saves artifacts using the node ID.\n# For Feature Engineering, it saves steps as \"{node_id}_{step_name}\".\n\nimpute_params = artifact_store.load(\"feature_engineering_impute_age\")\nencode_params = artifact_store.load(\"feature_engineering_encode_city\")\nmodel_params = artifact_store.load(\"model_training\")\n\n# 2. Instantiate Appliers\nimputer = SimpleImputerApplier()\nencoder = OneHotEncoderApplier()\nmodel_applier = RandomForestClassifierApplier()\n\n# 3. New Data\nnew_data = pd.DataFrame({\n    \"age\": [22, None],\n    \"income\": [45000, 85000],\n    \"city\": [\"NY\", \"SF\"]\n})\n\n# 4. Apply Transformations\n# Appliers return (df, metadata) tuples; we keep the DataFrame.\nstep1_df, _ = imputer.apply(new_data, impute_params)\nstep2_df, _ = encoder.apply(step1_df, encode_params)\n\n# 5. Predict\npredictions = model_applier.predict(step2_df, model_params)\n\nprint(\"Predictions:\", predictions)\n</code></pre>"},{"location":"python_api/#6-advanced-hyperparameter-tuning","title":"6. Advanced: Hyperparameter Tuning","text":"<p>You can replace the <code>model_training</code> node with a <code>model_tuning</code> node to automatically find the best hyperparameters. The engine will tune the model and then retrain the final model with the best parameters found.</p> <pre><code>from core.ml_pipeline.execution.schemas import PipelineConfig, NodeConfig\n\ntuning_pipeline_config = PipelineConfig(\n    pipeline_id=\"tuning_pipeline_01\",\n    nodes=[\n        # ... Data Loader and Feature Engineering nodes (same as above) ...\n        NodeConfig(\n            node_id=\"data_loader\",\n            step_type=\"data_loader\",\n            params={\"path\": \"data.csv\", \"type\": \"csv\"}\n        ),\n        NodeConfig(\n            node_id=\"feature_engineering\",\n            step_type=\"feature_engineering\",\n            inputs=[\"data_loader\"],\n            params={\n                \"steps\": [\n                    {\"name\": \"impute\", \"transformer\": \"SimpleImputer\", \"params\": {\"strategy\": \"mean\"}}\n                ]\n            }\n        ),\n        # Tuning Node\n        NodeConfig(\n            node_id=\"model_tuning\",\n            step_type=\"model_tuning\",\n            inputs=[\"feature_engineering\"],\n            params={\n                \"algorithm\": \"random_forest_classifier\",\n                \"target_column\": \"target\",\n                \"tuning_config\": {\n                    \"strategy\": \"random\",  # grid, random, optuna\n                    \"metric\": \"accuracy\",\n                    \"n_trials\": 5,\n                    \"cv_folds\": 3,\n                    \"search_space\": {\n                        \"n_estimators\": [10, 50],\n                        \"max_depth\": [5, 10, None]\n                    }\n                }\n            }\n        )\n    ]\n)\n\n# Run Tuning\nresult = engine.run(tuning_pipeline_config, job_id=\"tuning_run_001\")\n\n# The result contains the best parameters and score\ntuning_metrics = result.node_results[\"model_tuning\"].metrics\nprint(\"Best Score:\", tuning_metrics[\"best_score\"])\nprint(\"Best Params:\", tuning_metrics[\"best_params\"])\n\n# The artifact at 'model_tuning' is the fully trained model with best params\nbest_model = artifact_store.load(\"model_tuning\")\n</code></pre>"},{"location":"contributing/writing_docs/","title":"Writing Documentation","text":"<p>We use MkDocs with the Material for MkDocs theme to build our documentation.</p>"},{"location":"contributing/writing_docs/#setup","title":"Setup","text":"<ol> <li> <p>Install Dependencies:     <code>bash     pip install mkdocs mkdocs-material mkdocstrings[python]</code></p> </li> <li> <p>Serve Locally:     <code>bash     mkdocs serve</code>     This will start a local server at <code>http://127.0.0.1:8000</code> that auto-reloads when you change files.</p> </li> </ol>"},{"location":"contributing/writing_docs/#directory-structure","title":"Directory Structure","text":"<ul> <li><code>docs/</code>: Contains all markdown files.</li> <li><code>mkdocs.yml</code>: The main configuration file.</li> </ul>"},{"location":"contributing/writing_docs/#adding-a-new-page","title":"Adding a New Page","text":"<ol> <li>Create a new <code>.md</code> file in <code>docs/</code>.</li> <li>Add the file to the <code>nav</code> section in <code>mkdocs.yml</code>.</li> </ol>"},{"location":"contributing/writing_docs/#writing-api-documentation","title":"Writing API Documentation","text":"<p>We use <code>mkdocstrings</code> to auto-generate API docs from Python docstrings.</p> <p>To document a module, class, or function, use the <code>:::</code> directive:</p> <pre><code>::: core.my_module.MyClass\n</code></pre>"},{"location":"contributing/writing_docs/#docstring-style","title":"Docstring Style","text":"<p>We follow the Google Style Guide for docstrings.</p> <pre><code>def my_function(arg1: int, arg2: str) -&gt; bool:\n    \"\"\"\n    Does something amazing.\n\n    Args:\n        arg1: The first argument.\n        arg2: The second argument.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"guides/experiments/","title":"Experiments","text":"<p>Experiments in Skyulf provide a unified view of all your training and tuning jobs. The Experiments page aggregates <code>TrainingJob</code> and <code>HyperparameterTuningJob</code> records from the database, allowing you to compare runs, view metrics, and track model versions.</p>"},{"location":"guides/experiments/#how-it-works","title":"How It Works","text":"<p>When you train a model (via the UI or API), Skyulf creates a job record in the database:</p> Job Type Database Table What It Produces Training <code>TrainingJob</code> Single model with fixed hyperparameters Tuning <code>HyperparameterTuningJob</code> Best model from hyperparameter search <p>Each job automatically tracks:</p> <ul> <li>Metrics: Accuracy, F1, RMSE, ROC-AUC, etc.</li> <li>Hyperparameters: The parameters used (or best params found for tuning).</li> <li>Artifacts: Trained model, confusion matrices, feature importance.</li> <li>Lineage: Dataset, pipeline, and node that produced the model.</li> <li>Version: Auto-incremented per <code>(dataset_id, model_type)</code> pair.</li> </ul>"},{"location":"guides/experiments/#listing-jobs-python-api","title":"Listing Jobs (Python API)","text":"<p>Use <code>JobManager.list_jobs()</code> to retrieve all training and tuning jobs:</p> <pre><code>from core.ml_pipeline.execution.jobs import JobManager\n\nasync def list_experiments(session):\n    jobs = await JobManager.list_jobs(session, limit=50)\n\n    for job in jobs:\n        print(f\"[{job.job_type.upper()}] {job.model_type} v{job.version}\")\n        print(f\"  Dataset: {job.dataset_name}\")\n        print(f\"  Status: {job.status.value}\")\n        print(f\"  Metrics: {job.metrics}\")\n        if job.job_type == \"tuning\":\n            print(f\"  Strategy: {job.search_strategy}\")\n        print()\n</code></pre>"},{"location":"guides/experiments/#getting-a-single-job","title":"Getting a Single Job","text":"<pre><code>async def get_job_details(session, job_id: str):\n    job = await JobManager.get_job(session, job_id)\n    if job:\n        print(f\"Job: {job.job_id}\")\n        print(f\"Model: {job.model_type}\")\n        print(f\"Hyperparameters: {job.hyperparameters}\")\n        print(f\"Metrics: {job.metrics}\")\n</code></pre>"},{"location":"guides/experiments/#filtering-by-job-type","title":"Filtering by Job Type","text":"<pre><code># Only training jobs\ntraining_jobs = await JobManager.list_jobs(session, job_type=\"training\")\n\n# Only tuning jobs\ntuning_jobs = await JobManager.list_jobs(session, job_type=\"tuning\")\n</code></pre>"},{"location":"guides/experiments/#job-fields-reference","title":"Job Fields Reference","text":"Field Description <code>job_id</code> Unique identifier (UUID) <code>pipeline_id</code> Pipeline that created this job <code>node_id</code> The specific node (model) in the pipeline <code>dataset_id</code> Source dataset identifier <code>dataset_name</code> Human-readable dataset name <code>job_type</code> <code>\"training\"</code> or <code>\"tuning\"</code> <code>status</code> <code>queued</code>, <code>running</code>, <code>completed</code>, <code>failed</code>, <code>cancelled</code> <code>model_type</code> Algorithm name (e.g., <code>RandomForest</code>, <code>LogisticRegression</code>) <code>hyperparameters</code> Parameters used (or best params for tuning) <code>metrics</code> Evaluation metrics (accuracy, f1, etc.) <code>version</code> Model version for this dataset/model_type <code>search_strategy</code> (Tuning only) <code>random</code>, <code>grid</code>, <code>bayesian</code>, <code>halving</code> <code>start_time</code> / <code>end_time</code> Job execution timestamps <code>error</code> Error message if job failed"},{"location":"guides/experiments/#rest-api-endpoints","title":"REST API Endpoints","text":"Endpoint Method Description <code>/api/v1/jobs</code> GET List all jobs with optional filters <code>/api/v1/jobs/{job_id}</code> GET Get details for a specific job <code>/api/v1/jobs/{job_id}/cancel</code> POST Cancel a running or queued job"},{"location":"guides/experiments/#comparison-in-the-ui","title":"Comparison in the UI","text":"<p>In the Experiments page, you can:</p> <ol> <li>Filter by job type, status, model type, or dataset.</li> <li>Sort by date, metrics, or version.</li> <li>Select multiple runs to compare metrics side-by-side.</li> <li>Deploy a completed job directly to the inference endpoint.</li> </ol>"},{"location":"guides/getting_started/","title":"Getting Started","text":"<p>This tutorial walks you through a complete ML workflow using Skyulf's Python API: loading data, preprocessing, training a model, and making predictions.</p>"},{"location":"guides/getting_started/#prerequisites","title":"Prerequisites","text":"<pre><code># Install dependencies\npip install -r requirements-fastapi.txt\n\n# Start the server (optional, for REST API)\npython run_skyulf.py\n</code></pre>"},{"location":"guides/getting_started/#tutorial-iris-classification","title":"Tutorial: Iris Classification","text":"<p>We'll build a classifier for the classic Iris dataset, covering the full pipeline.</p>"},{"location":"guides/getting_started/#step-1-load-and-prepare-data","title":"Step 1: Load and Prepare Data","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\n# Load Iris dataset\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['target'] = iris.target\n\n# Rename columns for simplicity\ndf.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'target']\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(df.head())\n</code></pre>"},{"location":"guides/getting_started/#step-2-create-traintest-split","title":"Step 2: Create Train/Test Split","text":"<pre><code>from skyulf.data.dataset import SplitDataset\nfrom sklearn.model_selection import train_test_split\n\n# Split 80/20\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n\n# Create SplitDataset container\ndataset = SplitDataset(\n    train=train_df.reset_index(drop=True),\n    test=test_df.reset_index(drop=True),\n    validation=None  # Optional\n)\n\nprint(f\"Train: {len(dataset.train)}, Test: {len(dataset.test)}\")\n</code></pre>"},{"location":"guides/getting_started/#step-3-define-preprocessing-steps","title":"Step 3: Define Preprocessing Steps","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\n\n# Define preprocessing steps\npreprocessing_steps = [\n    {\n        \"name\": \"scale_features\",\n        \"transformer\": \"StandardScaler\",\n        \"params\": {\n            \"columns\": [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n        }\n    }\n]\n\n# Initialize FeatureEngineer\nengineer = FeatureEngineer(preprocessing_steps)\n\n# Fit on training data\ntrain_processed, train_metrics = engineer.fit_transform(dataset.train.drop('target', axis=1))\n\n# Apply to test data (using fitted parameters)\ntest_processed = engineer.transform(dataset.test.drop('target', axis=1))\n\nprint(f\"Scaling metrics: {train_metrics}\")\nprint(f\"Processed train shape: {train_processed.shape}\")\n</code></pre>"},{"location":"guides/getting_started/#step-4-train-a-model","title":"Step 4: Train a Model","text":"<pre><code>from skyulf.modeling.classification import RandomForestClassifierCalculator, RandomForestClassifierApplier\nfrom skyulf.modeling.base import StatefulEstimator\n\n# Create estimator\nestimator = StatefulEstimator(\n    calculator=RandomForestClassifierCalculator(),\n    applier=RandomForestClassifierApplier(),\n    node_id=\"iris_classifier\"\n)\n\n# Prepare dataset with processed features\nprocessed_dataset = SplitDataset(\n    train=pd.concat([train_processed, dataset.train['target'].reset_index(drop=True)], axis=1),\n    test=pd.concat([test_processed, dataset.test['target'].reset_index(drop=True)], axis=1)\n)\n\n# Train and predict\npredictions = estimator.fit_predict(\n    dataset=processed_dataset,\n    target_column=\"target\",\n    config={\n        \"n_estimators\": 100,\n        \"max_depth\": 5,\n        \"random_state\": 42\n    }\n)\n\nprint(f\"Train predictions: {predictions['train'][:10]}\")\n</code></pre>"},{"location":"guides/getting_started/#step-5-evaluate-the-model","title":"Step 5: Evaluate the Model","text":"<pre><code>from sklearn.metrics import accuracy_score, classification_report\n\n# Get predictions\ntrain_preds = predictions['train']\ntest_preds = predictions['test']\n\n# Calculate metrics\ntrain_acc = accuracy_score(dataset.train['target'], train_preds)\ntest_acc = accuracy_score(dataset.test['target'], test_preds)\n\nprint(f\"Train Accuracy: {train_acc:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(\"\\nClassification Report (Test):\")\nprint(classification_report(dataset.test['target'], test_preds, target_names=iris.target_names))\n</code></pre>"},{"location":"guides/getting_started/#step-6-make-predictions-on-new-data","title":"Step 6: Make Predictions on New Data","text":"<pre><code># Load saved model\nmodel_artifact = artifact_store.load(\"iris_classifier\")\n\n# New sample data\nnew_samples = pd.DataFrame({\n    'sepal_length': [5.1, 6.3],\n    'sepal_width': [3.5, 2.5],\n    'petal_length': [1.4, 4.9],\n    'petal_width': [0.2, 1.5]\n})\n\n# Apply same preprocessing\nnew_processed = engineer.transform(new_samples)\n\n# Get applier and predict\napplier = RandomForestClassifierApplier()\nnew_predictions = applier.predict(new_processed, model_artifact)\n\nprint(f\"Predictions: {new_predictions}\")\nprint(f\"Species: {[iris.target_names[p] for p in new_predictions]}\")\n</code></pre>"},{"location":"guides/getting_started/#using-the-pipeline-engine","title":"Using the Pipeline Engine","text":"<p>For more complex workflows, use the <code>PipelineEngine</code> with a configuration-based approach:</p> <pre><code>from core.ml_pipeline.execution.schemas import PipelineConfig, NodeConfig\nfrom core.ml_pipeline.execution.engine import PipelineEngine\nfrom core.ml_pipeline.artifacts.local import LocalArtifactStore\n\n# Save dataset to file first\ndf.to_csv(\"iris_data.csv\", index=False)\n\n# Define pipeline config\nconfig = PipelineConfig(\n    pipeline_id=\"iris_pipeline\",\n    nodes=[\n        NodeConfig(\n            node_id=\"load_data\",\n            step_type=\"data_loader\",\n            params={\"path\": \"iris_data.csv\", \"type\": \"csv\"}\n        ),\n        NodeConfig(\n            node_id=\"preprocess\",\n            step_type=\"feature_engineering\",\n            inputs=[\"load_data\"],\n            params={\n                \"steps\": [\n                    {\n                        \"name\": \"scale\",\n                        \"transformer\": \"StandardScaler\",\n                        \"params\": {}\n                    }\n                ]\n            }\n        ),\n        NodeConfig(\n            node_id=\"train_model\",\n            step_type=\"model_training\",\n            inputs=[\"preprocess\"],\n            params={\n                \"algorithm\": \"random_forest_classifier\",\n                \"target_column\": \"target\",\n                \"params\": {\"n_estimators\": 100}\n            }\n        )\n    ]\n)\n\n# Run pipeline\nartifact_store = LocalArtifactStore(\"./pipeline_artifacts\")\nengine = PipelineEngine(artifact_store=artifact_store)\nresult = engine.run(config, job_id=\"iris_job_001\")\n\nprint(f\"Pipeline Status: {result.status}\")\nfor node_id, node_result in result.node_results.items():\n    print(f\"  {node_id}: {node_result.status} ({node_result.execution_time:.2f}s)\")\n</code></pre>"},{"location":"guides/getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Preprocessing Reference: Learn about all available transformers</li> <li>Hyperparameter Tuning: Optimize your model with Grid/Random/Optuna search</li> <li>Model Registry: Version and track your models</li> <li>Inference API: Deploy models to a REST endpoint</li> </ul>"},{"location":"guides/inference/","title":"Inference","text":"<p>Skyulf provides a real-time inference API for deployed models. The inference engine reconstructs the exact preprocessing pipeline used during training to ensure consistency.</p>"},{"location":"guides/inference/#how-it-works","title":"How It Works","text":"<p>When a model is deployed, the <code>DeploymentService</code>: 1.  Loads Artifacts: Retrieves the trained model and all fitted preprocessing transformers (e.g., scalers, encoders) from the <code>ArtifactStore</code>. 2.  Reconstructs Pipeline: Uses the <code>APPLIER_MAP</code> to instantiate the correct <code>Applier</code> classes for each preprocessing step. 3.  Executes: Incoming data flows through the reconstructed pipeline (transformations) and finally into the model for prediction.</p>"},{"location":"guides/inference/#deployment-service","title":"Deployment Service","text":"<p>The <code>DeploymentService</code> manages the lifecycle of deployments using async database sessions.</p> <pre><code>from sqlalchemy.ext.asyncio import AsyncSession\nfrom core.ml_pipeline.deployment.service import DeploymentService\n\nasync def deploy_and_predict(session: AsyncSession):\n    # 1. Deploy a trained model from a completed job\n    deployment = await DeploymentService.deploy_model(\n        session=session,\n        job_id=\"training_job_abc123\",\n        user_id=1  # Optional: track who deployed\n    )\n    print(f\"Deployed: {deployment.model_type} (ID: {deployment.id})\")\n\n    # 2. Get the currently active deployment\n    active = await DeploymentService.get_active_deployment(session)\n    print(f\"Active deployment: {active.id}\")\n\n    # 3. Make predictions (uses active deployment)\n    input_data = [\n        {\"age\": 25, \"income\": 50000, \"city\": \"New York\"},\n        {\"age\": 35, \"income\": 75000, \"city\": \"Boston\"}\n    ]\n    predictions = await DeploymentService.predict(session, data=input_data)\n    print(f\"Predictions: {predictions}\")\n\n    # 4. List deployment history\n    history = await DeploymentService.list_deployments(session, limit=10)\n    for dep in history:\n        print(f\"  {dep.created_at}: {dep.model_type} (active={dep.is_active})\")\n</code></pre>"},{"location":"guides/inference/#rest-api","title":"REST API","text":"<p>Deployments are exposed via the REST API.</p>"},{"location":"guides/inference/#deploy-a-model","title":"Deploy a Model","text":"<p><code>POST /api/v1/deployments</code></p> <pre><code>{\n  \"job_id\": \"training_job_abc123\"\n}\n</code></pre>"},{"location":"guides/inference/#make-predictions","title":"Make Predictions","text":"<p><code>POST /api/v1/inference/predict</code></p>"},{"location":"guides/inference/#request-body","title":"Request Body","text":"<pre><code>{\n  \"data\": [\n    {\"age\": 25, \"income\": 50000, \"city\": \"New York\"},\n    {\"age\": 35, \"income\": 75000, \"city\": \"Boston\"}\n  ]\n}\n</code></pre>"},{"location":"guides/inference/#response","title":"Response","text":"<pre><code>{\n  \"predictions\": [1, 0],\n  \"probabilities\": [[0.2, 0.8], [0.7, 0.3]]\n}\n</code></pre>"},{"location":"guides/inference/#supported-transformers","title":"Supported Transformers","text":"<p>The inference engine supports all preprocessing transformers used during training. The <code>APPLIER_MAP</code> includes:</p> Category Transformers Encoding OneHotEncoder, LabelEncoder, OrdinalEncoder, TargetEncoder, HashEncoder, DummyEncoder Scaling StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler Imputation SimpleImputer, KNNImputer, IterativeImputer Outliers IQR, ZScore, Winsorize, ManualBounds Transformations PowerTransformer, SimpleTransformation, GeneralTransformation Bucketing GeneralBinning Feature Engineering PolynomialFeatures, FeatureGeneration Feature Selection VarianceThreshold, CorrelationThreshold, UnivariateSelection, ModelBasedSelection Cleaning TextCleaning, ValueReplacement, AliasReplacement, InvalidValueReplacement, Deduplicate Missing Data DropMissingColumns, DropMissingRows, MissingIndicator Type Casting Casting"},{"location":"guides/model_registry/","title":"Model Registry","text":"<p>The Model Registry is a centralized system for managing the lifecycle of your ML models. It tracks model versions, lineage (which training job created each model), and deployment status.</p>"},{"location":"guides/model_registry/#how-it-works","title":"How It Works","text":"<p>The registry is not a separate store\u2014it is a unified view that aggregates completed <code>TrainingJob</code> and <code>HyperparameterTuningJob</code> records from the database. When you train a model through the UI or API, the resulting artifact is automatically tracked here.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Model Registry                           \u2502\n\u2502  (Aggregates TrainingJob + HyperparameterTuningJob tables)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  RandomForest (Dataset: customers.csv)                          \u2502\n\u2502    \u2514\u2500\u2500 v3 [DEPLOYED] - training - accuracy: 0.92                \u2502\n\u2502    \u2514\u2500\u2500 v2            - tuning   - best_score: 0.91              \u2502\n\u2502    \u2514\u2500\u2500 v1            - training - accuracy: 0.85                \u2502\n\u2502                                                                  \u2502\n\u2502  LogisticRegression (Dataset: sales.csv)                        \u2502\n\u2502    \u2514\u2500\u2500 v1            - training - accuracy: 0.78                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/model_registry/#model-versioning","title":"Model Versioning","text":"<p>Every time a training or tuning job completes successfully, a new version is created. The <code>ModelRegistryService</code> automatically calculates the next version number for each <code>(dataset_id, model_type)</code> pair.</p> <ul> <li>Training Jobs: Produce a single model version.</li> <li>Advanced training with tuning Jobs: Produce a model version for the best hyperparameter configuration found.</li> </ul>"},{"location":"guides/model_registry/#using-the-registry-python-api","title":"Using the Registry (Python API)","text":""},{"location":"guides/model_registry/#list-all-models","title":"List All Models","text":"<pre><code>from core.ml_pipeline.model_registry.service import ModelRegistryService\n\nasync def list_models(session):\n    entries = await ModelRegistryService.list_models(session)\n\n    for entry in entries:\n        print(f\"{entry.model_type} ({entry.dataset_name})\")\n        for v in entry.versions:\n            deployed = \"[DEPLOYED]\" if v.is_deployed else \"\"\n            print(f\"  v{v.version} - {v.source} - {v.metrics} {deployed}\")\n</code></pre>"},{"location":"guides/model_registry/#get-versions-for-a-specific-model-type","title":"Get Versions for a Specific Model Type","text":"<pre><code>async def get_versions(session, model_type: str):\n    versions = await ModelRegistryService.get_model_versions(session, model_type)\n    return versions\n</code></pre>"},{"location":"guides/model_registry/#get-next-version-number","title":"Get Next Version Number","text":"<pre><code>async def get_next_version(session):\n    next_ver = await ModelRegistryService.get_next_version(\n        session, \n        dataset_id=\"ds_123\", \n        model_type=\"LogisticRegression\", \n        job_type=\"training\"\n    )\n    print(f\"Next version will be: {next_ver}\")\n</code></pre>"},{"location":"guides/model_registry/#deploying-a-model-for-inference","title":"Deploying a Model for Inference","text":"<p>The <code>DeploymentService</code> takes a model from the registry and makes it the active deployment. Only one model can be active at a time.</p>"},{"location":"guides/model_registry/#deploy-a-model","title":"Deploy a Model","text":"<pre><code>from core.ml_pipeline.deployment.service import DeploymentService\n\nasync def deploy(session, job_id: str):\n    # This deactivates any previous deployment and sets the new one as active\n    deployment = await DeploymentService.deploy_model(session, job_id)\n    print(f\"Deployed {deployment.model_type} from job {deployment.job_id}\")\n</code></pre>"},{"location":"guides/model_registry/#make-predictions-with-the-active-deployment","title":"Make Predictions with the Active Deployment","text":"<p>Once a model is deployed, you can send data to it for inference. The service automatically:</p> <ol> <li>Loads the model artifact (including any fitted transformers from the pipeline).</li> <li>Applies the same preprocessing steps used during training.</li> <li>Runs the model's <code>.predict()</code> method.</li> </ol> <pre><code>async def predict(session, input_data: list[dict]):\n    # Example: [{\"age\": 30, \"income\": 50000}, {\"age\": 25, \"income\": 60000}]\n    predictions = await DeploymentService.predict(session, input_data)\n    return predictions  # e.g., [0, 1]\n</code></pre>"},{"location":"guides/model_registry/#deployment-history","title":"Deployment History","text":"<pre><code>async def list_deployments(session):\n    deployments = await DeploymentService.list_deployments(session)\n    for d in deployments:\n        status = \"ACTIVE\" if d.is_active else \"inactive\"\n        print(f\"{d.model_type} ({d.created_at}) - {status}\")\n</code></pre>"},{"location":"guides/model_registry/#rest-api-endpoints","title":"REST API Endpoints","text":"<p>You can also interact with the registry and deployments via the FastAPI endpoints:</p> Endpoint Method Description <code>/api/registry/stats</code> GET Get registry statistics <code>/api/registry/models</code> GET List all models and versions <code>/api/registry/models/{model_type}/versions</code> GET Get versions for a model type <code>/api/deployments/deploy</code> POST Deploy a model by <code>job_id</code> <code>/api/deployments/predict</code> POST Make predictions with active model <code>/api/deployments/</code> GET List deployment history"},{"location":"guides/recipes/","title":"Common Recipes","text":"<p>Quick reference for common ML preprocessing and modeling tasks in Skyulf.</p>"},{"location":"guides/recipes/#data-cleaning","title":"Data Cleaning","text":""},{"location":"guides/recipes/#handle-missing-values","title":"Handle Missing Values","text":"<pre><code>steps = [\n    # Fill numeric columns with mean\n    {\n        \"name\": \"impute_numeric\",\n        \"transformer\": \"SimpleImputer\",\n        \"params\": {\"strategy\": \"mean\", \"columns\": [\"age\", \"income\"]}\n    },\n    # Fill categorical columns with mode\n    {\n        \"name\": \"impute_categorical\",\n        \"transformer\": \"SimpleImputer\",\n        \"params\": {\"strategy\": \"most_frequent\", \"columns\": [\"city\", \"gender\"]}\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#advanced-imputation-knn","title":"Advanced Imputation (KNN)","text":"<pre><code>steps = [\n    {\n        \"name\": \"knn_impute\",\n        \"transformer\": \"KNNImputer\",\n        \"params\": {\n            \"n_neighbors\": 5,\n            \"columns\": [\"feature1\", \"feature2\"]\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#drop-rowscolumns-with-missing-values","title":"Drop Rows/Columns with Missing Values","text":"<pre><code>steps = [\n    # Drop columns with &gt;50% missing\n    {\n        \"name\": \"drop_cols\",\n        \"transformer\": \"DropMissingColumns\",\n        \"params\": {\"threshold\": 0.5}\n    },\n    # Drop rows with any missing values\n    {\n        \"name\": \"drop_rows\",\n        \"transformer\": \"DropMissingRows\",\n        \"params\": {\"how\": \"any\"}  # or \"all\"\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#remove-duplicates","title":"Remove Duplicates","text":"<pre><code>steps = [\n    {\n        \"name\": \"dedup\",\n        \"transformer\": \"Deduplicate\",\n        \"params\": {\"subset\": [\"id\", \"timestamp\"]}  # Optional: specific columns\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#encoding-categorical-variables","title":"Encoding Categorical Variables","text":""},{"location":"guides/recipes/#one-hot-encoding","title":"One-Hot Encoding","text":"<pre><code>steps = [\n    {\n        \"name\": \"onehot\",\n        \"transformer\": \"OneHotEncoder\",\n        \"params\": {\n            \"columns\": [\"category\", \"region\"],\n            \"handle_unknown\": \"ignore\",\n            \"drop_first\": True  # Avoid multicollinearity\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#label-encoding-ordinal","title":"Label Encoding (Ordinal)","text":"<pre><code>steps = [\n    {\n        \"name\": \"ordinal\",\n        \"transformer\": \"OrdinalEncoder\",\n        \"params\": {\"columns\": [\"size\"]}  # small=0, medium=1, large=2\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#target-encoding","title":"Target Encoding","text":"<pre><code>steps = [\n    {\n        \"name\": \"target_encode\",\n        \"transformer\": \"TargetEncoder\",\n        \"params\": {\n            \"columns\": [\"high_cardinality_col\"],\n            \"smoothing\": 10.0\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#scaling-numeric-features","title":"Scaling Numeric Features","text":""},{"location":"guides/recipes/#standard-scaling-z-score","title":"Standard Scaling (Z-score)","text":"<pre><code>steps = [\n    {\n        \"name\": \"standardize\",\n        \"transformer\": \"StandardScaler\",\n        \"params\": {\"columns\": [\"age\", \"income\", \"score\"]}\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#min-max-scaling-0-1-range","title":"Min-Max Scaling (0-1 range)","text":"<pre><code>steps = [\n    {\n        \"name\": \"minmax\",\n        \"transformer\": \"MinMaxScaler\",\n        \"params\": {\n            \"columns\": [\"feature1\", \"feature2\"],\n            \"feature_range\": (0, 1)\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#robust-scaling-outlier-resistant","title":"Robust Scaling (Outlier-resistant)","text":"<pre><code>steps = [\n    {\n        \"name\": \"robust\",\n        \"transformer\": \"RobustScaler\",\n        \"params\": {\"columns\": [\"feature_with_outliers\"]}\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#handling-outliers","title":"Handling Outliers","text":""},{"location":"guides/recipes/#iqr-method","title":"IQR Method","text":"<pre><code>steps = [\n    {\n        \"name\": \"clip_outliers\",\n        \"transformer\": \"IQR\",\n        \"params\": {\n            \"multiplier\": 1.5,  # IQR multiplier for bounds\n            \"columns\": [\"price\", \"quantity\"]\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#z-score-method","title":"Z-Score Method","text":"<pre><code>steps = [\n    {\n        \"name\": \"zscore_filter\",\n        \"transformer\": \"ZScore\",\n        \"params\": {\n            \"threshold\": 3.0,\n            \"columns\": [\"outlier_prone_feature\"]\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#manual-bounds","title":"Manual Bounds","text":"<pre><code>steps = [\n    {\n        \"name\": \"manual_clip\",\n        \"transformer\": \"ManualBounds\",\n        \"params\": {\n            \"bounds\": {\n                \"age\": {\"lower\": 0, \"upper\": 120},\n                \"temperature\": {\"lower\": -50, \"upper\": 60}\n            }\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#feature-engineering","title":"Feature Engineering","text":""},{"location":"guides/recipes/#create-polynomial-features","title":"Create Polynomial Features","text":"<pre><code>steps = [\n    {\n        \"name\": \"poly\",\n        \"transformer\": \"PolynomialFeatures\",\n        \"params\": {\n            \"degree\": 2,\n            \"include_bias\": False,\n            \"columns\": [\"x1\", \"x2\"]\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#custom-feature-math","title":"Custom Feature Math","text":"<pre><code>steps = [\n    {\n        \"name\": \"ratio_features\",\n        \"transformer\": \"FeatureGeneration\",\n        \"params\": {\n            \"operations\": [\n                {\n                    \"type\": \"ratio\",\n                    \"numerator\": \"price\",\n                    \"denominator\": \"sqft\",\n                    \"output_column\": \"price_per_sqft\"\n                },\n                {\n                    \"type\": \"ratio\",\n                    \"numerator\": \"age\",\n                    \"denominator\": \"income\",\n                    \"output_column\": \"age_income_ratio\"\n                }\n            ]\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#binningbucketing","title":"Binning/Bucketing","text":"<pre><code>steps = [\n    {\n        \"name\": \"age_buckets\",\n        \"transformer\": \"GeneralBinning\",\n        \"params\": {\n            \"columns\": [\"age\"],\n            \"strategy\": \"quantile\",  # or \"uniform\", \"kmeans\"\n            \"n_bins\": 5\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#feature-selection","title":"Feature Selection","text":""},{"location":"guides/recipes/#remove-low-variance-features","title":"Remove Low Variance Features","text":"<pre><code>steps = [\n    {\n        \"name\": \"variance_filter\",\n        \"transformer\": \"VarianceThreshold\",\n        \"params\": {\"threshold\": 0.01}\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#remove-highly-correlated-features","title":"Remove Highly Correlated Features","text":"<pre><code>steps = [\n    {\n        \"name\": \"correlation_filter\",\n        \"transformer\": \"CorrelationThreshold\",\n        \"params\": {\"threshold\": 0.95}\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#select-k-best-features","title":"Select K Best Features","text":"<pre><code>steps = [\n    {\n        \"name\": \"select_best\",\n        \"transformer\": \"UnivariateSelection\",\n        \"params\": {\n            \"k\": 10,\n            \"score_func\": \"f_classif\"  # or \"f_regression\", \"chi2\"\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#power-transformations","title":"Power Transformations","text":""},{"location":"guides/recipes/#yeo-johnson-transform-handles-negatives","title":"Yeo-Johnson Transform (Handles negatives)","text":"<pre><code>steps = [\n    {\n        \"name\": \"power_transform\",\n        \"transformer\": \"PowerTransformer\",\n        \"params\": {\n            \"method\": \"yeo-johnson\",\n            \"columns\": [\"skewed_feature\"]\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#log-transform","title":"Log Transform","text":"<pre><code>steps = [\n    {\n        \"name\": \"log_transform\",\n        \"transformer\": \"SimpleTransformation\",\n        \"params\": {\n            \"operation\": \"log1p\",  # log(1 + x)\n            \"columns\": [\"highly_skewed\"]\n        }\n    }\n]\n</code></pre>"},{"location":"guides/recipes/#model-training","title":"Model Training","text":""},{"location":"guides/recipes/#classification-with-random-forest","title":"Classification with Random Forest","text":"<pre><code>from skyulf.modeling.classification import RandomForestClassifierCalculator\n\nconfig = {\n    \"n_estimators\": 100,\n    \"max_depth\": 10,\n    \"min_samples_split\": 5,\n    \"random_state\": 42\n}\n</code></pre>"},{"location":"guides/recipes/#regression-with-ridge","title":"Regression with Ridge","text":"<pre><code>from skyulf.modeling.regression import RidgeRegressionCalculator\n\nconfig = {\n    \"alpha\": 1.0,\n    \"random_state\": 42\n}\n</code></pre>"},{"location":"guides/recipes/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":""},{"location":"guides/recipes/#grid-search","title":"Grid Search","text":"<pre><code>tuning_config = {\n    \"strategy\": \"grid\",\n    \"metric\": \"accuracy\",\n    \"cv_folds\": 5,\n    \"search_space\": {\n        \"n_estimators\": [50, 100, 200],\n        \"max_depth\": [5, 10, 20]\n    }\n}\n</code></pre>"},{"location":"guides/recipes/#random-search","title":"Random Search","text":"<pre><code>tuning_config = {\n    \"strategy\": \"random\",\n    \"metric\": \"f1\",\n    \"cv_folds\": 3,\n    \"n_trials\": 20,\n    \"search_space\": {\n        \"n_estimators\": [50, 100, 200, 500],\n        \"max_depth\": [5, 10, 15, 20, None],\n        \"min_samples_split\": [2, 5, 10]\n    }\n}\n</code></pre>"},{"location":"guides/recipes/#optuna-bayesian-optimization","title":"Optuna (Bayesian Optimization)","text":"<pre><code>tuning_config = {\n    \"strategy\": \"optuna\",\n    \"metric\": \"roc_auc\",\n    \"cv_folds\": 5,\n    \"n_trials\": 50,\n    \"search_space\": {\n        \"n_estimators\": {\"type\": \"int\", \"low\": 50, \"high\": 500},\n        \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 20},\n        \"learning_rate\": {\"type\": \"float\", \"low\": 0.01, \"high\": 0.3, \"log\": True}\n    }\n}\n</code></pre>"},{"location":"guides/recipes/#complete-pipeline-example","title":"Complete Pipeline Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\n\n# Full preprocessing pipeline\nsteps = [\n    # 1. Clean\n    {\"name\": \"dedup\", \"transformer\": \"Deduplicate\", \"params\": {}},\n    {\"name\": \"drop_missing\", \"transformer\": \"DropMissingColumns\", \"params\": {\"threshold\": 0.7}},\n\n    # 2. Impute\n    {\"name\": \"impute_num\", \"transformer\": \"SimpleImputer\", \"params\": {\"strategy\": \"median\"}},\n\n    # 3. Handle Outliers\n    {\"name\": \"clip\", \"transformer\": \"IQR\", \"params\": {\"multiplier\": 1.5}},\n\n    # 4. Encode\n    {\"name\": \"encode\", \"transformer\": \"OneHotEncoder\", \"params\": {\"columns\": [\"category\"]}},\n\n    # 5. Scale\n    {\"name\": \"scale\", \"transformer\": \"StandardScaler\", \"params\": {}},\n\n    # 6. Feature Selection\n    {\"name\": \"select\", \"transformer\": \"VarianceThreshold\", \"params\": {\"threshold\": 0.01}}\n]\n\nengineer = FeatureEngineer(steps)\nprocessed_df, metrics = engineer.fit_transform(train_df)\n</code></pre>"},{"location":"modeling/","title":"Modeling Overview","text":"<p>The modeling module in Skyulf provides a unified interface for training and applying machine learning models. It is built around the StatefulEstimator pattern, which separates the logic of training a model from applying it for inference.</p>"},{"location":"modeling/#architecture","title":"Architecture","text":""},{"location":"modeling/#statefulestimator","title":"StatefulEstimator","text":"<p>The <code>StatefulEstimator</code> is the main entry point. It manages: 1.  Calculator: The component responsible for training the model (e.g., <code>LogisticRegressionCalculator</code>). 2.  Applier: The component responsible for generating predictions using a trained model (e.g., <code>LogisticRegressionApplier</code>). 3.  In-memory model: The trained model is stored on the estimator instance (<code>estimator.model</code>). For persistence across processes, use <code>SkyulfPipeline.save()</code> / <code>SkyulfPipeline.load()</code>.</p>"},{"location":"modeling/#calculator-vs-applier","title":"Calculator vs. Applier","text":"<ul> <li>Calculator: Takes training data (<code>SplitDataset</code>), fits the model, and returns a serializable model object.</li> <li>Applier: Uses the trained model object to predict on new data.</li> </ul>"},{"location":"modeling/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.modeling.base import StatefulEstimator\nfrom skyulf.modeling.classification import LogisticRegressionCalculator, LogisticRegressionApplier\nfrom skyulf.data.dataset import SplitDataset\n\n# 1. Initialize Estimator\n# We combine a Calculator and an Applier\nestimator = StatefulEstimator(\n    calculator=LogisticRegressionCalculator(),\n    applier=LogisticRegressionApplier(),\n    node_id=\"my_model_node\"\n)\n\n# 3. Train and Predict\n# Create dummy data\nimport pandas as pd\ndf = pd.DataFrame({\n    \"feature\": [1, 2, 3, 4, 5],\n    \"target\": [0, 1, 0, 1, 0]\n})\nmy_split_dataset = SplitDataset(train=df, test=df, validation=df)\n\n# 'dataset' is a SplitDataset containing train/test splits\n# 'target_column' is the name of the label column\npredictions = estimator.fit_predict(\n    dataset=my_split_dataset,\n    target_column=\"target\",\n    config={\"max_iter\": 500}\n)\n\n# The trained model is now available at: estimator.model\n</code></pre>"},{"location":"modeling/classification/","title":"Classification Models","text":"<p>The <code>skyulf.modeling.classification</code> module provides calculators and appliers for classification tasks.</p>"},{"location":"modeling/classification/#logistic-regression","title":"Logistic Regression","text":"<p>A linear model for classification. Wraps <code>sklearn.linear_model.LogisticRegression</code>.</p>"},{"location":"modeling/classification/#usage","title":"Usage","text":"<pre><code>from skyulf.modeling.classification import LogisticRegressionCalculator\n\ncalc = LogisticRegressionCalculator()\n# Override default parameters\nparams = {\n    \"penalty\": \"l2\",\n    \"C\": 1.0,\n    \"solver\": \"lbfgs\"\n}\n</code></pre>"},{"location":"modeling/classification/#default-parameters","title":"Default Parameters","text":"<ul> <li><code>max_iter</code>: 1000</li> <li><code>solver</code>: \"lbfgs\"</li> <li><code>random_state</code>: 42</li> </ul>"},{"location":"modeling/classification/#random-forest-classifier","title":"Random Forest Classifier","text":"<p>An ensemble learning method using multiple decision trees. Wraps <code>sklearn.ensemble.RandomForestClassifier</code>.</p>"},{"location":"modeling/classification/#usage_1","title":"Usage","text":"<pre><code>from skyulf.modeling.classification import RandomForestClassifierCalculator\n\ncalc = RandomForestClassifierCalculator()\n# Override default parameters\nparams = {\n    \"n_estimators\": 100,\n    \"max_depth\": 20\n}\n</code></pre>"},{"location":"modeling/classification/#default-parameters_1","title":"Default Parameters","text":"<ul> <li><code>n_estimators</code>: 50</li> <li><code>max_depth</code>: 10</li> <li><code>min_samples_split</code>: 5</li> <li><code>min_samples_leaf</code>: 2</li> <li><code>n_jobs</code>: -1 (uses all processors)</li> <li><code>random_state</code>: 42</li> </ul>"},{"location":"modeling/regression/","title":"Regression Models","text":"<p>The <code>skyulf.modeling.regression</code> module provides calculators and appliers for regression tasks.</p>"},{"location":"modeling/regression/#ridge-regression","title":"Ridge Regression","text":"<p>Linear least squares with l2 regularization. Wraps <code>sklearn.linear_model.Ridge</code>.</p>"},{"location":"modeling/regression/#usage","title":"Usage","text":"<pre><code>from skyulf.modeling.regression import RidgeRegressionCalculator\n\ncalc = RidgeRegressionCalculator()\n# Override default parameters\nparams = {\n    \"alpha\": 1.0,\n    \"solver\": \"auto\"\n}\n</code></pre>"},{"location":"modeling/regression/#default-parameters","title":"Default Parameters","text":"<ul> <li><code>alpha</code>: 1.0</li> <li><code>random_state</code>: 42</li> </ul>"},{"location":"modeling/regression/#random-forest-regressor","title":"Random Forest Regressor","text":"<p>A random forest regressor. Wraps <code>sklearn.ensemble.RandomForestRegressor</code>.</p>"},{"location":"modeling/regression/#usage_1","title":"Usage","text":"<pre><code>from skyulf.modeling.regression import RandomForestRegressorCalculator\n\ncalc = RandomForestRegressorCalculator()\n# Override default parameters\nparams = {\n    \"n_estimators\": 100,\n    \"criterion\": \"squared_error\"\n}\n</code></pre>"},{"location":"modeling/regression/#default-parameters_1","title":"Default Parameters","text":"<ul> <li><code>n_estimators</code>: 50</li> <li><code>max_depth</code>: 10</li> <li><code>min_samples_split</code>: 5</li> <li><code>min_samples_leaf</code>: 2</li> <li><code>n_jobs</code>: -1</li> <li><code>random_state</code>: 42</li> </ul>"},{"location":"modeling/training/","title":"Model Training","text":"<p>This guide details how to train machine learning models using the <code>StatefulEstimator</code> interface.</p>"},{"location":"modeling/training/#basic-training","title":"Basic Training","text":"<p>The primary method for training is <code>fit_predict</code>. This method: 1.  Trains the model on the training split of the <code>SplitDataset</code>. 2.  Saves the trained model artifact to the <code>ArtifactStore</code>. 3.  Generates predictions for all available splits (train, test, validation). 4.  Returns a dictionary of predictions.</p>"},{"location":"modeling/training/#example","title":"Example","text":"<pre><code>from skyulf.modeling.base import StatefulEstimator\nfrom skyulf.modeling.classification import RandomForestClassifierCalculator, RandomForestClassifierApplier\nfrom skyulf.data.dataset import SplitDataset\nimport pandas as pd\n\n# 1. Prepare Data\ndf = pd.DataFrame({\n    \"feature1\": [1, 2, 3, 4, 5] * 20,\n    \"feature2\": [5, 4, 3, 2, 1] * 20,\n    \"target\": [0, 1, 0, 1, 0] * 20\n})\n# In a real scenario, use a Splitter to create this\ndataset = SplitDataset(train=df, test=df, validation=None)\n\n# 2. Setup Estimator\nestimator = StatefulEstimator(\n    calculator=RandomForestClassifierCalculator(),\n    applier=RandomForestClassifierApplier(),\n    node_id=\"rf_model\"\n)\n\n# 3. Train\n# config passes hyperparameters to the underlying sklearn model\npredictions = estimator.fit_predict(\n    dataset=dataset,\n    target_column=\"target\",\n    config={\"n_estimators\": 100, \"max_depth\": 5}\n)\n\nprint(\"Training completed.\")\nprint(f\"Train Predictions shape: {predictions['train'].shape}\")\n\n# The trained model object is available at: estimator.model\n</code></pre>"},{"location":"modeling/training/#cross-validation","title":"Cross-Validation","text":"<p>You can perform cross-validation on the training set using the <code>cross_validate</code> method. This is useful for assessing model stability and performance without touching the test set.</p> <pre><code># Perform 5-fold Cross-Validation\ncv_results = estimator.cross_validate(\n    dataset=dataset,\n    target_column=\"target\",\n    config={\"n_estimators\": 100},\n    n_folds=5,\n    cv_type=\"k_fold\" # or 'stratified_k_fold'\n)\n\nprint(\"CV Results:\", cv_results)\n# cv_results contains 'metrics' (list of dicts) and 'predictions' (list of Series)\n</code></pre>"},{"location":"modeling/training/#supported-algorithms","title":"Supported Algorithms","text":"<p>The system supports various algorithms via their respective Calculators:</p> <ul> <li>Classification:<ul> <li><code>LogisticRegressionCalculator</code></li> <li><code>RandomForestClassifierCalculator</code></li> </ul> </li> <li>Regression:<ul> <li><code>RidgeRegressionCalculator</code></li> <li><code>RandomForestRegressorCalculator</code></li> </ul> </li> </ul> <p>Each calculator accepts standard hyperparameters in the <code>config</code> dictionary (e.g., <code>n_estimators</code>, <code>C</code>, <code>alpha</code>).</p>"},{"location":"modeling/tuning/","title":"Hyperparameter Tuning","text":"<p>The <code>skyulf.modeling.tuning</code> module allows you to optimize model hyperparameters using various search strategies.</p>"},{"location":"modeling/tuning/#tunercalculator","title":"TunerCalculator","text":"<p>The <code>TunerCalculator</code> wraps another calculator (like <code>LogisticRegressionCalculator</code>) and performs a search over a parameter grid.</p>"},{"location":"modeling/tuning/#supported-strategies","title":"Supported Strategies","text":"<ul> <li><code>grid</code>: Exhaustive search over specified parameter values (<code>GridSearchCV</code>).</li> <li><code>random</code>: Randomized search over parameters (<code>RandomizedSearchCV</code>).</li> <li><code>halving_grid</code>: Successive halving grid search (<code>HalvingGridSearchCV</code>).</li> <li><code>halving_random</code>: Successive halving random search (<code>HalvingRandomSearchCV</code>).</li> <li><code>optuna</code>: Bayesian optimization using Optuna (<code>OptunaSearchCV</code>).</li> </ul>"},{"location":"modeling/tuning/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.modeling.tuning.tuner import TunerCalculator, TunerApplier\nfrom skyulf.modeling.tuning.schemas import TuningConfig\nfrom skyulf.modeling.classification import RandomForestClassifierCalculator\n\n# 1. Define the base model\nbase_calc = RandomForestClassifierCalculator()\n\n# 2. Define the tuning configuration\ntuning_config = TuningConfig(\n    strategy=\"random\",\n    metric=\"accuracy\",\n    cv_folds=5,\n    n_trials=20,  # Number of random combinations to try\n    search_space={\n        \"n_estimators\": [50, 100, 200],\n        \"max_depth\": [5, 10, 20, None],\n        \"min_samples_split\": [2, 5, 10]\n    }\n)\n\n# 3. Initialize the Tuner\n# The tuner acts as a Calculator that returns the best model found\ntuner = TunerCalculator(\n    model_calculator=base_calc\n)\n\nfrom skyulf.modeling.base import StatefulEstimator\nfrom skyulf.data.dataset import SplitDataset\nimport pandas as pd\n\n# Create dummy data for the example\ndf = pd.DataFrame({\n    \"feature1\": [1, 2, 3, 4, 5] * 10,\n    \"feature2\": [5, 4, 3, 2, 1] * 10,\n    \"target\": [0, 1, 0, 1, 0] * 10\n})\ndataset = SplitDataset(train=df, test=df, validation=df)\n\nestimator = StatefulEstimator(\n    calculator=tuner,\n    applier=TunerApplier(),\n    node_id=\"tuned_rf_node\"\n)\n\n# 5. Train\n# Note: fit_predict will return placeholder predictions as this is a tuning job.\n# The best hyperparameters are available on the estimator model (a TuningResult).\nestimator.fit_predict(\n    dataset=dataset,\n    target_column=\"target\",\n    config=tuning_config\n)\n\nprint(\"Best params:\", estimator.model.best_params)\nprint(\"Best score:\", estimator.model.best_score)\n</code></pre>"},{"location":"preprocessing/bucketing/","title":"Bucketing (Binning)","text":"<p>The <code>bucketing</code> module discretizes continuous variables into bins. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/bucketing/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data\ndf = pd.DataFrame({\n    'age': [10, 20, 30, 40, 50, 60, 70, 80],\n    'score': [0.1, 0.5, 0.9, 0.2, 0.8, 0.3, 0.7, 0.4]\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"bin_age\",\n        \"transformer\": \"GeneralBinning\",\n        \"params\": {\n            \"n_bins\": 4,\n            \"strategy\": \"equal_width\",\n            \"columns\": [\"age\"]\n        }\n    },\n    {\n        \"name\": \"bin_score\",\n        \"transformer\": \"CustomBinning\",\n        \"params\": {\n            \"bins\": [0, 0.3, 0.7, 1.0],\n            \"columns\": [\"score\"]\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\nbinned_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/bucketing/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/bucketing/#generalbinning","title":"GeneralBinning","text":"<p>Bins data into intervals using various strategies.</p> <p>Parameters: - <code>n_bins</code> (int): Number of bins. Default 5. - <code>strategy</code> (str): Binning strategy. Options:     - <code>'equal_width'</code>: Uniform width bins (default)     - <code>'equal_frequency'</code>: Equal number of samples per bin     - <code>'kmeans'</code>: Bin edges determined by k-means clustering     - <code>'custom'</code>: Use manually specified bin edges - <code>columns</code> (List[str]): Columns to bin. - <code>column_strategies</code> (Dict): Per-column strategy overrides. - <code>custom_bins</code> (Dict): Custom bin edges for 'custom' strategy. Format: <code>{'column': [edge1, edge2, ...]}</code> - <code>duplicates</code> (str): How to handle duplicate bin edges. Default 'drop'.</p>"},{"location":"preprocessing/bucketing/#custombinning","title":"CustomBinning","text":"<p>Bins data using manually specified edges.</p> <p>Parameters: - <code>bins</code> (List[float]): List of bin edges. - <code>columns</code> (List[str]): Columns to bin.</p>"},{"location":"preprocessing/bucketing/#kbinsdiscretizer","title":"KBinsDiscretizer","text":"<p>Wrapper around sklearn's KBinsDiscretizer.</p> <p>Parameters: - <code>n_bins</code> (int): Number of bins. - <code>encode</code> (str): 'ordinal', 'onehot', or 'onehot-dense'. - <code>strategy</code> (str): 'uniform', 'quantile', or 'kmeans'.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"kbins_discretizer\",\n    \"transformer\": \"KBinsDiscretizer\",\n    \"params\": {\n        \"n_bins\": 5,\n        \"strategy\": \"kmeans\",\n        \"encode\": \"ordinal\"\n    }\n}\n</code></pre>"},{"location":"preprocessing/casting/","title":"Type Casting","text":"<p>The <code>casting</code> module handles data type conversions. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/casting/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data\ndf = pd.DataFrame({\n    'id': [1, 2, 3],\n    'status': ['0', '1', '0']\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"cast_id\",\n        \"transformer\": \"Casting\",\n        \"params\": {\n            \"target_type\": \"string\",\n            \"columns\": [\"id\"]\n        }\n    },\n    {\n        \"name\": \"cast_status\",\n        \"transformer\": \"Casting\",\n        \"params\": {\n            \"target_type\": \"int\",\n            \"columns\": [\"status\"]\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\ncasted_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/casting/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/casting/#casting","title":"Casting","text":"<p>Casts columns to specific data types.</p> <p>Parameters: - <code>target_type</code> (str): 'int', 'float', 'string', 'category', 'datetime'. - <code>columns</code> (List[str]): Columns to cast.</p>"},{"location":"preprocessing/cleaning/","title":"Data Cleaning","text":"<p>The <code>cleaning</code> module provides transformers for cleaning text and replacing values. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/cleaning/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data\ndf = pd.DataFrame({\n    'description': ['Hello World!', '  Test  ', 'Foo'],\n    'status': ['active', 'inactive', '?']\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"clean_text\",\n        \"transformer\": \"TextCleaning\",\n        \"params\": {\n            \"columns\": [\"description\"],\n            \"lowercase\": True,\n            \"remove_punctuation\": True,\n            \"strip_whitespace\": True\n        }\n    },\n    {\n        \"name\": \"replace_values\",\n        \"transformer\": \"ValueReplacement\",\n        \"params\": {\n            \"columns\": [\"status\"],\n            \"mapping\": {\"?\": None}\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\ncleaned_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/cleaning/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/cleaning/#textcleaning","title":"TextCleaning","text":"<p>Performs basic text cleaning operations on string columns.</p> <p>Parameters: - <code>columns</code> (List[str]): Columns to clean. - <code>lowercase</code> (bool): Convert to lowercase. Default <code>True</code>. - <code>remove_punctuation</code> (bool): Remove punctuation. Default <code>True</code>. - <code>remove_numbers</code> (bool): Remove digits. Default <code>False</code>. - <code>strip_whitespace</code> (bool): Strip leading/trailing whitespace. Default <code>True</code>.</p>"},{"location":"preprocessing/cleaning/#valuereplacement","title":"ValueReplacement","text":"<p>Replaces specific values in columns.</p> <p>Parameters: - <code>mapping</code> (Dict): Dictionary mapping old values to new values (e.g., <code>{\"?\": None, \"N/A\": None}</code>). - <code>columns</code> (List[str]): Columns to apply replacement to.</p>"},{"location":"preprocessing/cleaning/#aliasreplacement","title":"AliasReplacement","text":"<p>Replaces aliases with a canonical value.</p> <p>Parameters: - <code>replacements</code> (Dict): Dictionary mapping canonical values to lists of aliases. - <code>columns</code> (List[str]): Columns to apply replacement to.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"fix_usa\",\n    \"transformer\": \"AliasReplacement\",\n    \"params\": {\n        \"replacements\": {\n            \"USA\": [\"United States\", \"U.S.A.\", \"US\"]\n        },\n        \"columns\": [\"country\"]\n    }\n}\n</code></pre>"},{"location":"preprocessing/cleaning/#invalidvaluereplacement","title":"InvalidValueReplacement","text":"<p>Replaces invalid values (e.g., negative ages) with NaN.</p> <p>Parameters: - <code>rules</code> (List[Dict]): List of rules. Each rule has <code>column</code>, <code>operator</code> (&lt;, &gt;, &lt;=, &gt;=, ==, !=), and <code>value</code>.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"fix_invalid_age\",\n    \"transformer\": \"InvalidValueReplacement\",\n    \"params\": {\n        \"rules\": [\n            {\"column\": \"age\", \"operator\": \"&lt;\", \"value\": 0}\n        ]\n    }\n}\n</code></pre>"},{"location":"preprocessing/drop_and_missing/","title":"Drop and Missing Handling","text":"<p>The <code>drop_and_missing</code> module handles dropping rows/columns and creating missing indicators. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/drop_and_missing/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\nimport numpy as np\n\n# Sample Data\ndf = pd.DataFrame({\n    'id': [1, 1, 2],\n    'val': [10, np.nan, 20],\n    'empty_col': [np.nan, np.nan, np.nan]\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"dedup\",\n        \"transformer\": \"Deduplicate\",\n        \"params\": {\"subset\": [\"id\"], \"keep\": \"first\"}\n    },\n    {\n        \"name\": \"drop_cols\",\n        \"transformer\": \"DropMissingColumns\",\n        \"params\": {\"threshold\": 0.9} # Drop if &gt;90% missing\n    },\n    {\n        \"name\": \"missing_ind\",\n        \"transformer\": \"MissingIndicator\",\n        \"params\": {\"columns\": [\"val\"]}\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\ncleaned_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/drop_and_missing/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/drop_and_missing/#deduplicate","title":"Deduplicate","text":"<p>Removes duplicate rows.</p> <p>Parameters: - <code>subset</code> (List[str]): Columns to consider for identifying duplicates. If None, uses all columns. - <code>keep</code> (str): 'first', 'last', or False (drop all). Default 'first'.</p>"},{"location":"preprocessing/drop_and_missing/#dropmissingcolumns","title":"DropMissingColumns","text":"<p>Drops columns with too many missing values.</p> <p>Parameters: - <code>threshold</code> (float): Drop columns with missing fraction &gt; threshold. Default 1.0 (drop only if all missing).</p>"},{"location":"preprocessing/drop_and_missing/#dropmissingrows","title":"DropMissingRows","text":"<p>Drops rows with too many missing values.</p> <p>Parameters: - <code>missing_threshold</code> (float): Drop rows with missing % &gt;= this value. - <code>drop_if_any_missing</code> (bool): If True, drop rows with any missing values. Default False.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"drop_missing_rows\",\n    \"transformer\": \"DropMissingRows\",\n    \"params\": {\n        \"missing_threshold\": 50.0  # Drop if &gt;=50% missing\n    }\n}\n</code></pre> <p>Or drop any row with missing values:</p> <pre><code>{\n    \"name\": \"drop_any_missing\",\n    \"transformer\": \"DropMissingRows\",\n    \"params\": {\n        \"drop_if_any_missing\": True\n    }\n}\n</code></pre>"},{"location":"preprocessing/drop_and_missing/#missingindicator","title":"MissingIndicator","text":"<p>Creates binary indicators for missing values.</p> <p>Parameters: - <code>columns</code> (List[str]): Columns to create indicators for. - <code>suffix</code> (str): Suffix for new columns. Default '_missing'.</p>"},{"location":"preprocessing/encoding/","title":"Encoding","text":"<p>The <code>encoding</code> module handles categorical variable encoding. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/encoding/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data\ndf = pd.DataFrame({\n    'color': ['red', 'blue', 'green'],\n    'size': ['S', 'M', 'L']\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"encode_color\",\n        \"transformer\": \"OneHotEncoder\",\n        \"params\": {\n            \"columns\": [\"color\"],\n            \"handle_unknown\": \"ignore\"\n        }\n    },\n    {\n        \"name\": \"encode_size\",\n        \"transformer\": \"OrdinalEncoder\",\n        \"params\": {\n            \"columns\": [\"size\"]\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\nencoded_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/encoding/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/encoding/#onehotencoder","title":"OneHotEncoder","text":"<p>Creates binary columns for each category.</p> <p>Parameters: - <code>handle_unknown</code> (str): 'error' or 'ignore'. Default 'ignore'. - <code>drop</code> (str): 'first' or None. - <code>columns</code> (List[str]): Columns to encode.</p>"},{"location":"preprocessing/encoding/#ordinalencoder","title":"OrdinalEncoder","text":"<p>Encodes categories as integers.</p> <p>Parameters: - <code>columns</code> (List[str]): Columns to encode.</p>"},{"location":"preprocessing/encoding/#labelencoder","title":"LabelEncoder","text":"<p>Encodes target labels as integers (0 to n_classes-1).</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"label_encode\",\n    \"transformer\": \"LabelEncoder\",\n    \"params\": {}\n}\n</code></pre>"},{"location":"preprocessing/encoding/#targetencoder","title":"TargetEncoder","text":"<p>Encodes categories based on the mean of the target variable.</p> <p>Parameters: - <code>smoothing</code> (float): Smoothing factor. - <code>columns</code> (List[str]): Columns to encode.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"target_encode\",\n    \"transformer\": \"TargetEncoder\",\n    \"params\": {\n        \"columns\": [\"zipcode\"],\n        \"smoothing\": 10.0\n    }\n}\n</code></pre>"},{"location":"preprocessing/encoding/#hashencoder","title":"HashEncoder","text":"<p>Encodes categories using the hashing trick.</p> <p>Parameters: - <code>n_components</code> (int): Number of hash buckets. - <code>columns</code> (List[str]): Columns to encode.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"hash_encode\",\n    \"transformer\": \"HashEncoder\",\n    \"params\": {\n        \"n_components\": 8,\n        \"columns\": [\"category\"]\n    }\n}\n</code></pre>"},{"location":"preprocessing/feature_generation/","title":"Feature Generation","text":"<p>The <code>feature_generation</code> module creates new features from existing ones. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/feature_generation/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data\ndf = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [4, 5, 6]\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"poly_features\",\n        \"transformer\": \"PolynomialFeatures\",\n        \"params\": {\n            \"degree\": 2,\n            \"interaction_only\": True\n        }\n    },\n    {\n        \"name\": \"custom_features\",\n        \"transformer\": \"FeatureGeneration\",\n        \"params\": {\n            \"operations\": [\n                {\n                    \"operation_type\": \"arithmetic\",\n                    \"method\": \"add\",\n                    \"input_columns\": [\"a\", \"b\"],\n                    \"output_column\": \"sum_ab\"\n                }\n            ]\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\ngenerated_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/feature_generation/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/feature_generation/#polynomialfeatures","title":"PolynomialFeatures","text":"<p>Generates polynomial and interaction features.</p> <p>Parameters: - <code>degree</code> (int): The degree of the polynomial features. Default <code>2</code>. - <code>interaction_only</code> (bool): If true, only interaction features are produced. Default <code>False</code>. - <code>include_bias</code> (bool): If true, include a bias column. Default <code>False</code>. - <code>columns</code> (List[str]): Columns to generate features from. Default: all numeric.</p>"},{"location":"preprocessing/feature_generation/#featuregeneration","title":"FeatureGeneration","text":"<p>Creates new features using arithmetic or statistical operations.</p> <p>Parameters: - <code>operations</code> (List[Dict]): List of operations to apply. Each operation has:   - <code>operation_type</code> (str): 'arithmetic', 'statistical', 'datetime', 'text'.   - <code>method</code> (str): The method to use (e.g., 'add', 'subtract', 'multiply', 'divide', 'log', 'sqrt').   - <code>input_columns</code> (List[str]): Input columns.   - <code>output_column</code> (str): Name of output column.   - <code>constants</code> (List[float]): Optional constants for operations. - <code>epsilon</code> (float): Small value added to prevent division by zero. Default <code>1e-8</code>. - <code>allow_overwrite</code> (bool): Allow overwriting existing columns. Default <code>False</code>.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"create_ratio\",\n    \"transformer\": \"FeatureGeneration\",\n    \"params\": {\n        \"operations\": [\n            {\n                \"operation_type\": \"arithmetic\",\n                \"method\": \"divide\",\n                \"input_columns\": [\"price\", \"quantity\"],\n                \"output_column\": \"unit_price\"\n            },\n            {\n                \"operation_type\": \"arithmetic\",\n                \"method\": \"log\",\n                \"input_columns\": [\"income\"],\n                \"output_column\": \"log_income\"\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"preprocessing/feature_selection/","title":"Feature Selection","text":"<p>The <code>feature_selection</code> module reduces dimensionality by selecting the most relevant features. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/feature_selection/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\nimport numpy as np\n\n# Sample Data\ndf = pd.DataFrame({\n    'low_var': [1, 1, 1, 1, 1, 1, 1, 0],\n    'feature1': np.random.rand(8),\n    'feature2': np.random.rand(8),\n    'target': [0, 1, 0, 1, 0, 1, 0, 1]\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"remove_low_var\",\n        \"transformer\": \"VarianceThreshold\",\n        \"params\": {\n            \"threshold\": 0.1 # Remove features with variance &lt; 0.1\n        }\n    },\n    {\n        \"name\": \"split_xy\",\n        \"transformer\": \"feature_target_split\", # Required before supervised selection\n        \"params\": {\"target_column\": \"target\"}\n    },\n    {\n        \"name\": \"select_k_best\",\n        \"transformer\": \"UnivariateSelection\",\n        \"params\": {\n            \"k\": 2,\n            \"score_func\": \"f_classif\"\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\nselected_data, metrics = engineer.fit_transform(df)\n# Note: selected_data will be a tuple (X, y) after split\n</code></pre>"},{"location":"preprocessing/feature_selection/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/feature_selection/#variancethreshold","title":"VarianceThreshold","text":"<p>Removes features with low variance.</p> <p>Parameters: - <code>threshold</code> (float): Features with variance lower than this are removed.</p>"},{"location":"preprocessing/feature_selection/#correlationthreshold","title":"CorrelationThreshold","text":"<p>Removes features highly correlated with others.</p> <p>Parameters: - <code>threshold</code> (float): Correlation coefficient threshold (0 to 1).</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"corr_filter\",\n    \"transformer\": \"CorrelationThreshold\",\n    \"params\": {\n        \"threshold\": 0.95\n    }\n}\n</code></pre>"},{"location":"preprocessing/feature_selection/#univariateselection","title":"UnivariateSelection","text":"<p>Selects features based on univariate statistical tests.</p> <p>Parameters: - <code>k</code> (int): Number of top features to select. - <code>score_func</code> (str): 'f_classif', 'f_regression', 'chi2', etc.</p>"},{"location":"preprocessing/feature_selection/#modelbasedselection","title":"ModelBasedSelection","text":"<p>Selects features using an estimator (e.g., Random Forest importance).</p> <p>Parameters: - <code>estimator</code> (str): 'RandomForest', 'LogisticRegression', etc. - <code>threshold</code> (str or float): 'mean', 'median', or float value.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"model_select\",\n    \"transformer\": \"ModelBasedSelection\",\n    \"params\": {\n        \"estimator\": \"RandomForest\",\n        \"threshold\": \"mean\"\n    }\n}\n</code></pre>"},{"location":"preprocessing/imputation/","title":"Imputation","text":"<p>The <code>imputation</code> module provides strategies for filling missing values. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/imputation/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\nimport numpy as np\n\n# Sample Data\ndf = pd.DataFrame({\n    'age': [25, np.nan, 30],\n    'salary': [50000, 60000, np.nan]\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"impute_age\",\n        \"transformer\": \"SimpleImputer\",\n        \"params\": {\n            \"strategy\": \"mean\",\n            \"columns\": [\"age\"]\n        }\n    },\n    {\n        \"name\": \"impute_salary\",\n        \"transformer\": \"KNNImputer\",\n        \"params\": {\n            \"n_neighbors\": 2,\n            \"columns\": [\"salary\"]\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\nimputed_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/imputation/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/imputation/#simpleimputer","title":"SimpleImputer","text":"<p>Imputes missing values using univariate statistics.</p> <p>Parameters: - <code>strategy</code> (str): 'mean', 'median', 'most_frequent', or 'constant'. - <code>fill_value</code> (Any): Value to use when strategy is 'constant'. - <code>columns</code> (List[str]): Columns to impute.</p>"},{"location":"preprocessing/imputation/#knnimputer","title":"KNNImputer","text":"<p>Imputes missing values using k-Nearest Neighbors.</p> <p>Parameters: - <code>n_neighbors</code> (int): Number of neighbors. Default 5. - <code>weights</code> (str): 'uniform' or 'distance'. Default 'uniform'. - <code>columns</code> (List[str]): Columns to impute.</p>"},{"location":"preprocessing/imputation/#iterativeimputer","title":"IterativeImputer","text":"<p>Multivariate imputation by chained equations (MICE).</p> <p>Parameters: - <code>max_iter</code> (int): Maximum number of imputation rounds. - <code>random_state</code> (int): Seed for reproducibility. - <code>columns</code> (List[str]): Columns to impute.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"impute_mice\",\n    \"transformer\": \"IterativeImputer\",\n    \"params\": {\n        \"max_iter\": 10,\n        \"columns\": [\"age\", \"salary\"]\n    }\n}\n</code></pre>"},{"location":"preprocessing/inspection/","title":"Inspection","text":"<p>The <code>inspection</code> module provides read-only nodes that analyze data without modifying it. These are useful for debugging pipelines and capturing intermediate statistics.</p>"},{"location":"preprocessing/inspection/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data\ndf = pd.DataFrame({\n    'age': [25, 30, 35, None],\n    'salary': [50000, 60000, 70000, 80000]\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"profile_data\",\n        \"transformer\": \"DatasetProfile\",\n        \"params\": {}\n    },\n    {\n        \"name\": \"snapshot\",\n        \"transformer\": \"DataSnapshot\",\n        \"params\": {\"n_rows\": 3}\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\nresult_df, metrics = engineer.fit_transform(df)\n\n# The data is unchanged, but metrics contain profiling info\nprint(metrics)\n</code></pre>"},{"location":"preprocessing/inspection/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/inspection/#datasetprofile","title":"DatasetProfile","text":"<p>Computes basic statistics about the dataset without modifying it.</p> <p>Parameters: None required.</p> <p>Output Metrics: - <code>rows</code> (int): Number of rows. - <code>columns</code> (int): Number of columns. - <code>dtypes</code> (Dict): Column data types. - <code>missing</code> (Dict): Missing value counts per column. - <code>numeric_stats</code> (Dict): Descriptive statistics for numeric columns.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"profile\",\n    \"transformer\": \"DatasetProfile\",\n    \"params\": {}\n}\n</code></pre>"},{"location":"preprocessing/inspection/#datasnapshot","title":"DataSnapshot","text":"<p>Captures a snapshot of the first N rows for debugging.</p> <p>Parameters: - <code>n_rows</code> (int): Number of rows to capture. Default <code>5</code>.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"preview\",\n    \"transformer\": \"DataSnapshot\",\n    \"params\": {\"n_rows\": 10}\n}\n</code></pre> <p>Note: Inspection nodes are pass-through\u2014they do not modify the DataFrame. Their output is captured in the pipeline metrics.</p>"},{"location":"preprocessing/outliers/","title":"Outlier Handling","text":"<p>The <code>outliers</code> module provides methods for detecting and handling outliers. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/outliers/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data\ndf = pd.DataFrame({\n    'price': [10, 12, 11, 1000, 13], # 1000 is an outlier\n    'age': [20, 21, 22, 150, 23]     # 150 is an outlier\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"remove_price_outliers\",\n        \"transformer\": \"IQR\",\n        \"params\": {\n            \"multiplier\": 1.5,\n            \"columns\": [\"price\"]\n        }\n    },\n    {\n        \"name\": \"clip_age\",\n        \"transformer\": \"ManualBounds\",\n        \"params\": {\n            \"bounds\": {\"age\": {\"lower\": 0, \"upper\": 100}} # Clip age to 0-100\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\nprocessed_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/outliers/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/outliers/#iqr-interquartile-range","title":"IQR (Interquartile Range)","text":"<p>Detects outliers using the IQR method (Q1 - kIQR, Q3 + kIQR). Rows with values outside bounds are removed.</p> <p>Parameters: - <code>multiplier</code> (float): Multiplier for IQR (k). Default 1.5. - <code>columns</code> (List[str]): Columns to check.</p>"},{"location":"preprocessing/outliers/#zscore","title":"ZScore","text":"<p>Detects outliers using Z-Score (mean +/- kstd). Rows with Z-score beyond threshold are removed*.</p> <p>Parameters: - <code>threshold</code> (float): Z-score threshold. Default 3.0. - <code>columns</code> (List[str]): Columns to check.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"zscore_filter\",\n    \"transformer\": \"ZScore\",\n    \"params\": {\n        \"threshold\": 3.0,\n        \"columns\": [\"feature1\"]\n    }\n}\n</code></pre>"},{"location":"preprocessing/outliers/#winsorize","title":"Winsorize","text":"<p>Limits extreme values to specified percentiles (clips values to bounds instead of removing rows).</p> <p>Parameters: - <code>lower_percentile</code> (float): Lower percentile (0-100). Default <code>5.0</code>. - <code>upper_percentile</code> (float): Upper percentile (0-100). Default <code>95.0</code>. - <code>columns</code> (List[str]): Columns to winsorize.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"winsorize_income\",\n    \"transformer\": \"Winsorize\",\n    \"params\": {\n        \"lower_percentile\": 5.0,\n        \"upper_percentile\": 95.0,\n        \"columns\": [\"income\"]\n    }\n}\n</code></pre>"},{"location":"preprocessing/outliers/#manualbounds","title":"ManualBounds","text":"<p>Clips values to manually specified bounds. Rows outside bounds are removed.</p> <p>Parameters: - <code>bounds</code> (Dict): Dictionary mapping columns to bounds. Each bound has <code>lower</code> and/or <code>upper</code>.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"clip_age\",\n    \"transformer\": \"ManualBounds\",\n    \"params\": {\n        \"bounds\": {\n            \"age\": {\"lower\": 0, \"upper\": 100},\n            \"score\": {\"lower\": 0}  # Only lower bound\n        }\n    }\n}\n</code></pre>"},{"location":"preprocessing/outliers/#ellipticenvelope","title":"EllipticEnvelope","text":"<p>Detects outliers using robust covariance estimation (multivariate). Outlier rows are removed.</p> <p>Parameters: - <code>contamination</code> (float): Expected proportion of outliers (0-0.5). Default <code>0.01</code>. - <code>columns</code> (List[str]): Columns to check. Default: all numeric.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"elliptic_envelope\",\n    \"transformer\": \"EllipticEnvelope\",\n    \"params\": {\n        \"contamination\": 0.05,\n        \"columns\": [\"feature1\", \"feature2\"]\n    }\n}\n</code></pre>"},{"location":"preprocessing/resampling/","title":"Resampling","text":"<p>The <code>resampling</code> module handles class imbalance using oversampling and undersampling techniques from <code>imbalanced-learn</code>. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p> <p>Note: Resampling requires all features to be numeric. Use encoding transformers (OneHotEncoder, OrdinalEncoder) before resampling.</p>"},{"location":"preprocessing/resampling/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data (Imbalanced)\ndf = pd.DataFrame({\n    'feature': range(20),\n    'target': [0]*14 + [1]*6 # 14 zeros, 6 ones\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"split_xy\",\n        \"transformer\": \"feature_target_split\",\n        \"params\": {\"target_column\": \"target\"}\n    },\n    {\n        \"name\": \"oversample\",\n        \"transformer\": \"Oversampling\",\n        \"params\": {\n            \"method\": \"smote\",\n            \"sampling_strategy\": \"auto\",\n            \"k_neighbors\": 5\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\nresampled_data, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/resampling/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/resampling/#oversampling","title":"Oversampling","text":"<p>Oversamples the minority class using SMOTE variants.</p> <p>Parameters: - <code>method</code> (str): Oversampling method. One of:   - <code>'smote'</code> (default) - Standard SMOTE   - <code>'adasyn'</code> - Adaptive Synthetic Sampling   - <code>'borderline_smote'</code> - Borderline-SMOTE   - <code>'svm_smote'</code> - SVM-SMOTE   - <code>'kmeans_smote'</code> - KMeans-SMOTE   - <code>'smote_tomek'</code> - SMOTE + Tomek Links cleaning - <code>sampling_strategy</code> (str|float|dict): Sampling strategy. Default <code>'auto'</code>. - <code>random_state</code> (int): Random seed. Default <code>42</code>. - <code>k_neighbors</code> (int): Number of nearest neighbors for SMOTE. Default <code>5</code>.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"smote_oversample\",\n    \"transformer\": \"Oversampling\",\n    \"params\": {\n        \"method\": \"borderline_smote\",\n        \"sampling_strategy\": \"minority\",\n        \"k_neighbors\": 5,\n        \"random_state\": 42\n    }\n}\n</code></pre>"},{"location":"preprocessing/resampling/#undersampling","title":"Undersampling","text":"<p>Undersamples the majority class.</p> <p>Parameters: - <code>method</code> (str): Undersampling method. One of:   - <code>'random'</code> (default) - Random undersampling   - <code>'nearmiss'</code> - NearMiss algorithm   - <code>'tomek'</code> - Tomek Links removal   - <code>'enn'</code> - Edited Nearest Neighbours - <code>sampling_strategy</code> (str|float|dict): Sampling strategy. Default <code>'auto'</code>. - <code>random_state</code> (int): Random seed. Default <code>42</code>.</p>"},{"location":"preprocessing/scaling/","title":"Scaling","text":"<p>The <code>scaling</code> module normalizes numeric features. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/scaling/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data\ndf = pd.DataFrame({\n    'age': [20, 30, 40],\n    'salary': [30000, 50000, 90000]\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"scale_age\",\n        \"transformer\": \"StandardScaler\",\n        \"params\": {\n            \"columns\": [\"age\"] # Optional: specify columns\n        }\n    },\n    {\n        \"name\": \"scale_salary\",\n        \"transformer\": \"MinMaxScaler\",\n        \"params\": {\n            \"feature_range\": (0, 1),\n            \"columns\": [\"salary\"]\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\nscaled_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/scaling/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/scaling/#standardscaler","title":"StandardScaler","text":"<p>Standardizes features by removing the mean and scaling to unit variance.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"standard_scale\",\n    \"transformer\": \"StandardScaler\",\n    \"params\": {}\n}\n</code></pre>"},{"location":"preprocessing/scaling/#minmaxscaler","title":"MinMaxScaler","text":"<p>Scales features to a given range (usually 0-1).</p> <p>Parameters: - <code>feature_range</code> (Tuple[float, float]): (min, max). Default <code>(0, 1)</code>.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"minmax_scale\",\n    \"transformer\": \"MinMaxScaler\",\n    \"params\": {\n        \"feature_range\": (0, 1)\n    }\n}\n</code></pre>"},{"location":"preprocessing/scaling/#robustscaler","title":"RobustScaler","text":"<p>Scales features using statistics that are robust to outliers (IQR).</p> <p>Parameters: - <code>quantile_range</code> (Tuple[float, float]): (q_min, q_max). Default <code>(25.0, 75.0)</code>.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"robust_scale\",\n    \"transformer\": \"RobustScaler\",\n    \"params\": {\n        \"quantile_range\": (25.0, 75.0)\n    }\n}\n</code></pre>"},{"location":"preprocessing/scaling/#maxabsscaler","title":"MaxAbsScaler","text":"<p>Scales each feature by its maximum absolute value.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"maxabs_scale\",\n    \"transformer\": \"MaxAbsScaler\",\n    \"params\": {}\n}\n</code></pre>"},{"location":"preprocessing/split/","title":"Splitting","text":"<p>The <code>split</code> module handles dataset splitting. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/split/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\n\n# Sample Data\ndf = pd.DataFrame({\n    'feature': range(20),\n    'target': [0, 1] * 10\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"split_xy\",\n        \"transformer\": \"feature_target_split\",\n        \"params\": {\"target_column\": \"target\"}\n    },\n    {\n        \"name\": \"split_train_test\",\n        \"transformer\": \"TrainTestSplitter\",\n        \"params\": {\n            \"test_size\": 0.2,\n            \"stratify\": \"target\" # Stratify by y (target)\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\nsplit_data, metrics = engineer.fit_transform(df)\n# split_data will be a SplitDataset object (train, test, validation)\n</code></pre>"},{"location":"preprocessing/split/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/split/#traintestsplitter","title":"TrainTestSplitter","text":"<p>Splits the dataset into training, testing, and optionally validation sets.</p> <p>Parameters: - <code>test_size</code> (float): Proportion of the dataset for test split. Default 0.2. - <code>validation_size</code> (float): Proportion of the dataset for validation split. Default 0.0 (no validation set). - <code>random_state</code> (int): Seed for reproducibility. Default 42. - <code>shuffle</code> (bool): Whether to shuffle data before splitting. Default True. - <code>stratify</code> (bool): Whether to stratify the split by target. Default False. - <code>target_column</code> (str): Column to use for stratification (required if stratify=True).</p>"},{"location":"preprocessing/split/#feature_target_split","title":"feature_target_split","text":"<p>Separates features (X) from the target variable (y).</p> <p>Parameters: - <code>target_column</code> (str): Name of the target column.</p>"},{"location":"preprocessing/transformations/","title":"Transformations","text":"<p>The <code>transformations</code> module provides mathematical transformations. These transformers are typically used within a <code>FeatureEngineer</code> pipeline.</p>"},{"location":"preprocessing/transformations/#usage-example","title":"Usage Example","text":"<pre><code>from skyulf.preprocessing.pipeline import FeatureEngineer\nimport pandas as pd\nimport numpy as np\n\n# Sample Data\ndf = pd.DataFrame({\n    'skewed_data': np.random.exponential(size=100),\n    'large_values': np.random.randint(1, 10000, size=100)\n})\n\n# Define Steps\nsteps = [\n    {\n        \"name\": \"power_transform\",\n        \"transformer\": \"PowerTransformer\",\n        \"params\": {\n            \"method\": \"yeo-johnson\",\n            \"columns\": [\"skewed_data\"]\n        }\n    },\n    {\n        \"name\": \"log_transform\",\n        \"transformer\": \"SimpleTransformation\",\n        \"params\": {\n            \"transformations\": [\n                {\"column\": \"large_values\", \"method\": \"log\"}\n            ]\n        }\n    }\n]\n\n# Execute\nengineer = FeatureEngineer(steps)\ntransformed_df, metrics = engineer.fit_transform(df)\n</code></pre>"},{"location":"preprocessing/transformations/#available-transformers","title":"Available Transformers","text":""},{"location":"preprocessing/transformations/#powertransformer","title":"PowerTransformer","text":"<p>Applies a power transform (Yeo-Johnson or Box-Cox) to make data more Gaussian-like.</p> <p>Parameters: - <code>method</code> (str): 'yeo-johnson' or 'box-cox'. - <code>standardize</code> (bool): Apply zero-mean, unit-variance normalization.</p>"},{"location":"preprocessing/transformations/#simpletransformation","title":"SimpleTransformation","text":"<p>Applies simple mathematical functions to columns.</p> <p>Parameters: - <code>transformations</code> (List[Dict]): List of transformations. Each has:   - <code>column</code> (str): Column to transform.   - <code>method</code> (str): One of:     - <code>'log'</code> - Natural log (uses log1p for safety with zeros)     - <code>'square_root'</code> - Square root     - <code>'cube_root'</code> - Cube root     - <code>'square'</code> - Square     - <code>'reciprocal'</code> - 1/x     - <code>'exponential'</code> - e^x (clipped to prevent overflow)</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"log_transform\",\n    \"transformer\": \"SimpleTransformation\",\n    \"params\": {\n        \"transformations\": [\n            {\"column\": \"income\", \"method\": \"log\"},\n            {\"column\": \"count\", \"method\": \"square_root\"}\n        ]\n    }\n}\n</code></pre>"},{"location":"preprocessing/transformations/#generaltransformation","title":"GeneralTransformation","text":"<p>Applies a custom lambda function (use with caution, requires code execution).</p> <p>Parameters: - <code>func</code> (str): Python code string for the function.</p> <p>Python Config:</p> <pre><code>{\n    \"name\": \"custom_lambda\",\n    \"transformer\": \"GeneralTransformation\",\n    \"params\": {\n        \"func\": \"lambda x: x + 1\"\n    }\n}\n</code></pre>"}]}